<article>
  <front>
    <journal-meta>
      <journal-title-group>
        <journal-title>British Journal of Nursing</journal-title>
      </journal-title-group>
    </journal-meta>
    <article-meta>
      <title-group>
        <article-title>Have OSCEs come of age in nursing education?</article-title>
      </title-group>
      <contrib-group>
        <aff id="0">
          <label>0</label>
          <institution>Marian Traynor, Director of Education; Despina Galanouli Research Fellow (Education), School of Nursing and Midwifery, Queen's University Belfast, Medical Biology Centre</institution>
          ,
          <addr-line>Belfast</addr-line>
        </aff>
      </contrib-group>
      <abstract>
        <p>This article is intended to contribute to the current debate as to whether the objective structured clinical examination (OSCE) should become a standard assessment tool for undergraduate nursing education as they currently are for medicine. The authors describe how one UK university developed an OSCE for a nursing undergraduate programme with the aim of emphasising the need for nursing students to be competent in clinical skills and offering a means of standardising the assessment of these skills. There has been an increasing number of research studies carried out in this area at international level and this article's main contribution to the literature is the description of the Angoff standard-setting procedure that was used to calibrate the OSCE at this University and which makes it the first nursing OSCE in the UK to incorporate a scientific standardsetting procedure.</p>
      </abstract>
      <kwd-group>
        <kwd>Nursing education ■ Nursing assessment ■ Clinical competence ■ Calibration</kwd>
      </kwd-group>
      <volume>24</volume>
      <issue>7</issue>
      <fpage>388</fpage>
      <lpage>392</lpage>
      <pub-date>
        <year>2015</year>
      </pub-date>
    </article-meta>
  </front>
  <body>
    <sec id="1">
      <title>-</title>
      <p>T by Harden et al (1975), and is now widely accepted
he objective structured clinical examination (OSCE)
was first developed for medical education in Scotland
as a fit-for-purpose instrument for measuring clinical
reasoning skills with a high degree of technical accuracy. It
is a practical examination of what the candidate does with
the emphasis on assessing components of competence in
a structured way, in order to arrive at a technically sound
measure of clinical reasoning. Norman et al (2002) refer
to the work of O’Connor and McGraw (1994) and Fowell
and Bligh (1998) to suggest that the OSCE is widely
considered to provide a reliable and valid assessment of
clinical skills and is superior to traditional methods based
on the physical examination of real patients, the evaluation
of oral presentations of cases and written work. The OSCE
has therefore been adopted by many universities for the
assessment of healthcare competencies (Marshall and Harris,
2000; Wessel et al, 2003) and is generally accepted as a valid
assessment tool (Schuwirth and van der Vleuten, 2003) and
also as a formative teaching tool (Alinier, 2003).</p>
      <p>McWilliam and Botwinski (2012) argue that a wider
range of skills can be tested through an OSCE. In a study to
identify the strengths and weaknesses of using an OSCE in a
nursing programme in New England, USA, with 60 full-time
students, they concluded that:
‘A well-designed and implemented OSCE
can provide students with opportunities to
demonstrate interpersonal and interview skills,
problem-solving abilities, teaching, assessment
skills, and application of basic clinical knowledge.’
(McWilliam and Botwinski, 2012: 38–9)</p>
      <p>Rentschler et al (2007) also refer to students’ ability to ‘think
on their feet’, when presented with a simulated real-life health
issue, as another skill educators can observe during an OSCE.</p>
      <p>The importance the OSCE has assumed in nursing
education is also evidenced in the increasing number of
international research studies aiming to develop and improve
the OSCE process. For example, in Australia, such research
has resulted in the development of an implementation
framework for using OSCEs in nursing (Henderson et
al, 2013) and also in the establishment of best practice
guidelines for use of OSCEs in order to improve student
learning (Nulty et al, 2011). In Sweden, OSCEs have
been used as a component of a multi-method assessment
strategy in combination with real-life (in vivo) conditions
(e.g. bedside observation examinations (Mårtensson and
Löfmark, 2013), and in the USA OSCEs have proved
effective for multidisciplinary learners (Corcoran et al,
2013). Numerous other studies on the OSCE experience of
students and its impact on them and other stakeholders have
provided evidence that this type of examination has been
judged worthwhile (Brosnan et al, 2006; Rentchler et al,
2007; McWilliam and Botwinski, 2010; Nulty et al, 2011).</p>
      <p>Assessment of clinical competence
The study reported in this paper took place at a UK university
that implemented a dual-process approach to the assessment of
clinical competence in 2002. This approach entailed working
in close collaboration with clinical practitioners to develop a
robust assessment methodology for clinical skills. The resulting
assessment tool combined the validity associated with a
performance-based assessment, i.e. a measurement of what
the student can do in the practice placement, and the
enhanced reliability associated with an OSCE, a
competencybased assessment of what the student can do in controlled
representations of professional practice (Boursicot et al, 2011;
Nulty et al, 2011).The model of clinical competence described
by Miller (1990) with regard to mentorship and assessment was
influential in developing this approach.</p>
      <p>Developing the OSCE
Aim
The OSCE was developed for an undergraduate nursing
programme with the aim of emphasising the need for
nursing students to be competent in clinical skills and
offering a means of standardising the assessment of these
skills. It also aimed to identify students’ strengths and
weaknesses in skills performance and to enable them to
identify future learning needs. More importantly, the OSCE
identifies those who achieve a pass standard for clinical skills
and who can progress to the next year of their programme.</p>
      <p>Since originally described by Harden and Gleeson in 1979,
the use of the OSCE has become widespread in both
undergraduate and postgraduate medical education and
also in a number of schools of nursing and midwifery. It
is important to note that the OSCE was not implemented
in this school in order to replace the assessment carried
out by the clinical mentor in practice, but rather it was
intended to complement assessment in the clinical setting.</p>
      <p>This flexibility of the OSCE to be used both as a formative
and summative assessment tool has been described as one
of its advantages (Nulty et al, 2011; Liddle, 2014). It is
emphasised to the students that the acquisition of clinical
skills is paramount to the development of a competent safe
practitioner and the OSCE is one means to facilitate the
learning, teaching and assessment of these skills.</p>
      <p>The OSCE in the undergraduate nursing curriculum of
this university therefore aims to:
■ Contribute to the learning and teaching of clinical skills
■ Assess the clinical competency of undergraduate nursing</p>
      <p>students in a range of clinical skills
■ Standardise the assessment process
■ Encourage students to develop essential clinical skills in the</p>
      <p>practice setting
■ Identify potentially weak students who may require</p>
      <p>additional support in clinical practice
■ Identify students who can progress to the next year of their</p>
      <p>studies.</p>
      <p>Standard setting
Standards are not only difficult to define; they are also difficult
to set. According to Tarrant et al (2009) pass standards for any
examinations should be set relative to the difficulty of the test
using any one of a number of established absolute
standardsetting methods such as the Ebel procedure or Angoff method
that define a cut-off score, thereby identifying candidates
who are competent and eligible for progression There is no
perfect standard setting method and the decision on which
method to use is based on the most important criteria for that
particular examination (Cizek and Bunch, 2007; Bejar, 2008;</p>
      <p>Dijkstra et al, 2012). Berk (1986) suggests that there are over 30
tdL standard-setting procedures for an OSCE and one of the most
re
ach robust is the ‘Angoff Procedure’ (Angoff, 1971). The Angoff
lta
eH standard-setting procedure normally involves three stages and a
A
51M minimum of 30 judges in order to minimise error. In stage one,
2© judges consider the test items and make a judgement about a
0
‘minimally acceptable’ student’s probability of answering each
item correctly at the end of a period of training.This is carried
out in private, by the judges. During stage two, a sample of
students’ actual scores is given to the judges and the judges are
given the opportunity to reconsider their initial judgements.</p>
      <p>Again this stage is carried out in private. The third and final
stage involves judges clarifying their reasons for the standard
they set in stage two and all judges are given the opportunity to
revisit the standards set during the second stage.</p>
      <p>The cut score is taken as the average of the judgements at
the end of stage three and can be defined as the minimum
standard of competence to be achieved. By comparing the
score the student receives, with the cut score, inferences
about the competence of each student can be made. The
cut score is therefore of great significance to the assessment
tool. If, for example, in the case of an assessment tool
designed for nursing students, the cut score is set too low,
many incompetent professionals could conceivably attain
professional status. Alternatively, if the cut score is set too
high, many capable students will fail the assessment and
therefore be unable to register with the profession.</p>
      <p>Calibration of the pass score
The OSCE has been used in nursing assessment in other
UK universities, but there is no published literature to
suggest the procedure chosen to set a defensible, reliable
standard setting has been addressed. The school in this
study chose a modified Angoff (1971) standard-setting
procedure to calibrate the OSCE making it, effectively, the
first nursing OSCE in the UK to incorporate a scientific
standard-setting procedure.</p>
      <p>Two panels of experts were established for the standard
setting: one consisting of clinical mentors (n=10) who
were experienced assessors and one consisting of nurse
lecturers (n=20) currently involved in assessing student
clinical performance. To encourage full participation from
clinical mentors it was made clear to them that the standard
setting process was about professional judgement and did not
involve assessing students.Ten mentors agreed to be involved.</p>
      <p>The participants (judges) were given an outline of the
OSCE structure and an explanation of the different stations.</p>
      <p>At each station candidates are assigned a specific clinical
task to perform, for example, they may be asked to measure
blood pressure, or provide post-operative care. They may
be asked to do this on a simulated patient or on a manikin.</p>
      <p>This form of assessment does not use real patients but
while validity may be compromised, it is not sacrificed,
and the design does enhance reliability. Each of the 30
participants (10 mentors and 20 lecturers) was given copies
of what would happen at each station and a checklist of the
competencies to be assessed at each station. After looking
through this information, each participant was given a copy
of each of the standard-setting checklists. The standard
setting was completed using a modified Angoff approach.</p>
      <p>The Angoff-determined passing score is the mean of the
passing scores calculated from the second-stage judgement
proformas. The passing score was found to be 648 (the
maximum score on the OSCE was 750). The passing
score therefore was the judges’ standard (648) minus two
standard errors of measurement (SEM). If no SEM is added
to the cut score the probabilities of making two errors are
approximately balanced. These are when a passing student
is failed or a failing student is passed. Adding one SEM
makes the likelihood of failing a passing student about five
times more likely than passing a failing student. Adding two
SEM makes the likelihood of failing a passing student about
twenty times that of passing a failing student.</p>
      <p>Following adjustment for test fallibility, a passing score of
600 (80%) was established for the first year nursing OSCE (the
same process was employed for the second year nursing OSCE
with a pass score of 70%).The second year OSCE stations were
testing higher-order clinical skills.</p>
      <p>Implementation
The university has two intakes of nursing students each
year, which means that 400 students undertake an OSCE
during the first year of the undergraduate programme.</p>
      <p>Typically, in the first year OSCE there are seven individual
stations lasting 5 minutes each and they include measuring
blood pressure, recording vital signs and urinalysis. In the
second year there are ten stations lasting 10  minutes each
and include post-operative care, recording and interpreting
Glasgow Coma Scale scores.</p>
      <p>Preparation of students
Research suggests that student preparation for the OSCE, as
well as subsequent feedback on their performance, is crucial
for a successful OSCE process. For example, findings of a
qualitative study by Cazzell and Rodriguez (2011) on the
experiences of nursing students with the OSCE suggest that
some students experienced high levels of stress and a loss of
control, which they related to lack of preparation. Students
have also reported that they failed to make the connection
between the OSCE experience and future clinical practice
owing to the lack of immediate feedback (Liddle, 2014;
Cazzell and Rodriguez, 2011).</p>
      <p>Student preparation therefore plays an important role
when planning an OSCE, and the school of nursing and
midwifery where the nursing OSCE was developed takes
the measures described below to ensure student familiarity
with the assessment procedure, in an attempt to reduce
potential stress and anxiety:
■ All students on the undergraduate nursing programme are
made aware of the format of the OSCE and that it is part
of the assessment process for the clinical modules
n Objective structured clinical examinations (OSCEs) offer the potential for
highly valid and reliable assessments of clinical practice
n The high reliability of OSCEs is warranted by a standard setting procedure
such as Angoff
n OSCEs may be used formatively (to improve learning) or summatively to
determine competence
n OSCE-development benefits from a partnership between clinical mentors and
nurse lecturers
■ Lecturing staff meet with the students to offer advice and
answer questions related to the administration of the OSCE
examination
■ Information on the OSCE is posted on the student
resources pages of the virtual learning environment (VLE);
this includes a short video of a typical OSCE. Students
are therefore made aware of the range of clinical skills that
could be tested in the OSCE. Students are made aware of
the fact that they can schedule sessions in the clinical skills
education centre to practice and refine specific skills
■ Students are advised that they must present for the</p>
      <p>examination in uniform with a valid student identity card
■ The date and time of the examination are posted on the</p>
      <p>student resources pages of the University’s VLE.</p>
      <p>Preparation of OSCE assessors
From the beginning of the nursing OSCE-development
process, the school recognised the importance of involving
clinical mentors in the assessment process. A partnership
approach was adopted and both clinical mentors and nurse
lecturers were invited to act as examiners for the OSCE. It is
important to note that, while other professional bodies, such as
the General Medical Council (GMC), set out clear guidelines
on the roles and responsibilities of assessors in OSCEs (GMC,
2009), no such guidelines exist within the Nursing and
Midwifery Council (NMC). It is therefore the responsibility
of each higher education institution to develop their own
standards and guidelines. However, the NMC does recommend
the OSCE for the assessment of medicines administration
(Liddle, 2014) and, notably, the latest NMC guidelines on
overseas competency testing for UK registration have included
the OSCE as a practical assessment (NMC, 2014).</p>
      <p>The school promotes consistency in the operation of its
OSCEs by inviting all OSCE examiners to attend a briefing
session before the examination. Examiners are fully briefed on
the nature of the examination, i.e. the number of stations, the
pass score and the compensatory aspect of the examination
(the pass mark is aggregated across the stations and therefore a
student may do less well in one station but can compensate in
another).They are also advised of their roles and responsibilities
and are shown the marking criteria for each station, with a
detailed question and answer session for any queries they have.</p>
      <p>This school recognises that the organisational aspect of the
OSCE is very demanding and has therefore assigned specific
administration support to academic staff in the planning and
execution of the OSCE.The school has invested in an optical
computerised reader to assist in the scanning of the OSCE
papers and calibration of results. The School administrator
attends regular meetings to agree the stations, to timetable the
examiners and to timetable the simulated patients who are
at the different stations in the OSCE. The clinical allocations
manager in the school is advised of the OSCE dates and
circulates the dates to the clinical areas so that mentors can
put their names forward to act as examiners. Mentors who
participate in the OSCE are provided with a certificate and
this is recognised as evidence to support mentorship updates.</p>
      <p>Currently the school has in excess of 200 mentors on the
OSCE examiners’ database. Mentors cite the OSCE as not
only providing them with insight into the undergraduate
but also credit the
with encouraging
students to concentrate on achieving competence in clinical
skills while on practice placement.
The role of the nurse is changing and
will continue to
change alongside the needs and expectations of patients.</p>
      <p>more than ever there is a need to demonstrate to the
public that higher education institutions with a responsibility
for undergraduate nurse education are prepared to invest in
implementing procedures that ensure nursing students are
tested using the best assessment methods available.</p>
      <p>Since their original development</p>
      <p>OSCEs have become
established as one of the main methods of assessing clinical
competence in medical education (Gormley, 2011).This is not
replicated in nursing education where the uptake is sporadic
and, according to Rushforth (2007), in some HEIs the OSCE
process has been subject to
major adaptations which
have undermined important elements of the original model.</p>
      <p>Although the OSCE is not without its critics, the potential for
high reliability (Swanson 1987; Swanson and van der Vleuten
2013) and the emerging evidence on its increased validity
(Petrusa 2002, Downing 2003), cannot be ignored, making it a
valuable assessment method.
challenge, therefore, is to
persuade those
responsibility for validating nursing curricula within the UK
to adopt the OSCE in combination with other methods,
as a standard assessment tool for undergraduate education.</p>
      <p>It is important to note that the OSCE does not provide a
complete profile of a student’s level of competency but there
is enough evidence to support the adaptation of the OSCE
as one method to assess competence. This will also require
those higher education institutions who have a responsibility
for undergraduate nurse education to invest in the OSCE
as means of not only achieving excellence in education but
also ensuring public confidence in the profession. There are
cost implications, of course, as good assessment methods are
resource intensive. However, as van derVleuten (1996) argued,
investing in assessment is investing in teaching and learning;
good assessment will facilitate good learning. Ultimately,
therefore, good assessment
more likely to
produce nurse graduates who are not only academically
qualified but also have the required proficiencies at the point
of registration; something which the NMC require and the</p>
      <p>Conflict of interest: none</p>
    </sec>
  </body>
  <back>
    <ref-list>
      <ref>
        <mixed-citation>
          <source>Postgrad Med J</source>
          <volume>74</volume>
          (
          <issue>867</issue>
          ):
          <fpage>18</fpage>
          -24
          <string-name>
            <surname>General Medical Council</surname>
          </string-name>
          (
          <year>2009</year>
          )
          <article-title>Assessment in undergraduate medical education: advice supplementary to Tomorrow's Doctors</article-title>
          . General Medical Council, London. http://tinyurl.
          <source>com/qgptkw2 (accessed 26 March</source>
          <year>2015</year>
          )
          <string-name>
            <surname>Gormley</surname>
            <given-names>G</given-names>
          </string-name>
          (
          <year>2011</year>
          )
          <article-title>Summative OSCEs in undergraduate medical education</article-title>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <source>Ulster Med J</source>
          <volume>80</volume>
          (
          <issue>3</issue>
          ):
          <fpage>127</fpage>
          -32
          <string-name>
            <surname>Harden</surname>
            <given-names>RM</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Stevenson</surname>
            <given-names>M</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Downie</surname>
            <given-names>WW</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Wilson</surname>
            <given-names>GM</given-names>
          </string-name>
          (
          <year>1975</year>
          )
          <article-title>Assessment of clinical competence using objective structured examination</article-title>
          . BMJ 1:
          <fpage>447</fpage>
          . doi: http://dx.doi.org/10.1136/bmj.1.5955.447
          <string-name>
            <surname>Harden</surname>
            <given-names>RM</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Gleeson</surname>
            <given-names>F</given-names>
          </string-name>
          (
          <year>1979</year>
          )
          <article-title>Assessment of clinical competence using an objective structured clinical examination</article-title>
          .
          <source>Med Educ</source>
          <volume>13</volume>
          :
          <fpage>39</fpage>
          -
          <lpage>54</lpage>
          . doi: 10.1111/ j.1365-
          <fpage>2923</fpage>
          .
          <year>1979</year>
          .tb00918.x
          <string-name>
            <surname>Henderson</surname>
            <given-names>A</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Nulty</surname>
            <given-names>DD</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Mitchell</surname>
            <given-names>ML</given-names>
          </string-name>
          et al (
          <year>2013</year>
          )
          <article-title>An implementation framework for using OSCEs in nursing curricula</article-title>
          .
          <source>Nurse Educ Today</source>
          <volume>33</volume>
          (
          <issue>12</issue>
          ):
          <fpage>1459</fpage>
          -
          <lpage>61</lpage>
          . doi: 10.1016/j.nedt.
          <year>2013</year>
          .04.008
          <string-name>
            <surname>Marshall</surname>
            <given-names>G</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Harris</surname>
            <given-names>P</given-names>
          </string-name>
          (
          <year>2000</year>
          )
          <article-title>A study of the role of an objective structured clinical examination (OSCE) in assessing clinical competence in third year student radiographers</article-title>
          .
          <source>Radiography</source>
          <volume>6</volume>
          (
          <issue>2</issue>
          ):
          <fpage>117</fpage>
          -22
          <string-name>
            <surname>Mårtensson</surname>
            <given-names>G</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Löfmark</surname>
            <given-names>A</given-names>
          </string-name>
          (
          <year>2013</year>
          )
          <article-title>Implementation and student evaluation of clinical final examination in nursing education</article-title>
          .
          <source>Nurse Educ Today</source>
          <volume>33</volume>
          (
          <issue>12</issue>
          ):
          <fpage>1563</fpage>
          -
          <lpage>8</lpage>
          . doi: 10.1016/j.nedt.
          <year>2013</year>
          .01.003
          <string-name>
            <surname>McWilliam</surname>
            <given-names>P</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Botwinski</surname>
            <given-names>C</given-names>
          </string-name>
          (
          <year>2010</year>
          )
          <article-title>Developing a successful nursing Objective Structured Clinical Examination</article-title>
          .
          <source>J Nurs Educ</source>
          <volume>49</volume>
          (
          <issue>1</issue>
          ):
          <fpage>36</fpage>
          -
          <lpage>41</lpage>
          . doi: 10.3928/
          <fpage>01484834</fpage>
          -
          <lpage>20090915</lpage>
          -01
          <string-name>
            <surname>McWilliam</surname>
            <given-names>PL</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Botwinski</surname>
            <given-names>CA</given-names>
          </string-name>
          (
          <year>2012</year>
          )
          <article-title>Identifying strengths and weaknesses in the utilization of Objective Structured Clinical Examination (OSCE) in a nursing program</article-title>
          .
          <source>Nurs Educ Perspect</source>
          <volume>33</volume>
          (
          <issue>1</issue>
          ):
          <fpage>35</fpage>
          -9
          <string-name>
            <surname>Miller</surname>
            <given-names>GE</given-names>
          </string-name>
          (
          <year>1990</year>
          )
          <article-title>The assessment of clinical skills/competence/performance</article-title>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <source>Acad Med</source>
          <volume>65</volume>
          (
          <issue>9</issue>
          Suppl):
          <fpage>S63</fpage>
          -7
          <string-name>
            <surname>Norman</surname>
            <given-names>IJ</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Watson</surname>
            <given-names>R</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Murrells</surname>
            <given-names>T</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Calman</surname>
            <given-names>L</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Redfern</surname>
            <given-names>S</given-names>
          </string-name>
          (
          <year>2002</year>
          )
          <article-title>The validity and reliability of methods to assess the competence to practise of pre-registration nursing and midwifery students</article-title>
          .
          <source>Int J Nurs Stud</source>
          <volume>39</volume>
          (
          <issue>2</issue>
          ):
          <fpage>133</fpage>
          -
          <lpage>45</lpage>
          Nulty DD,
          <string-name>
            <surname>Mitchell</surname>
            <given-names>ML</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Jeffrey</surname>
            <given-names>CA</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Henderson</surname>
            <given-names>A</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Groves</surname>
            <given-names>M</given-names>
          </string-name>
          (
          <year>2011</year>
          )
          <article-title>Best Practice Guidelines for use of OSCEs: Maximising value for student learning</article-title>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <source>Nurse Educ Today</source>
          <volume>31</volume>
          (
          <issue>2</issue>
          ):
          <fpage>145</fpage>
          -
          <lpage>51</lpage>
          . doi: 10.1016/j.nedt.
          <year>2010</year>
          .05.006 Nursing and
          <string-name>
            <given-names>Midwifery</given-names>
            <surname>Council</surname>
          </string-name>
          (
          <year>2014</year>
          )
          <article-title>Registering as a nurse or midwife in the UK: Information for nurses and midwives who trained outside of the EU or EEA countries</article-title>
          . http://tinyurl.
          <source>com/pul2ry6 (accessed 25 March</source>
          <year>2015</year>
          )
          <string-name>
            <surname>Liddle</surname>
            <given-names>C</given-names>
          </string-name>
          (
          <year>2014</year>
          )
          <article-title>The objective structured clinical examination</article-title>
          .
          <source>Nursing Times.</source>
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <article-title>110(Online issue)</article-title>
          . http://tinyurl.
          <source>com/ptbbwbv (accessed 25 March</source>
          <year>2015</year>
          ) O'
          <string-name>
            <surname>Connor</surname>
            <given-names>HM</given-names>
          </string-name>
          ,
          <string-name>
            <surname>McGraw</surname>
            <given-names>RC</given-names>
          </string-name>
          (
          <year>1997</year>
          )
          <article-title>Clinical skills training: developing objective assessment instruments</article-title>
          .
          <source>Med Educ</source>
          <volume>31</volume>
          (
          <issue>5</issue>
          ):
          <fpage>359</fpage>
          -63
          <string-name>
            <surname>Petrusa</surname>
            <given-names>ER</given-names>
          </string-name>
          (
          <year>2002</year>
          )
          <article-title>Clinical performance assessments</article-title>
          . In Norman GR,
          <string-name>
            <surname>van der Vleuten</surname>
            <given-names>CPM</given-names>
          </string-name>
          , Newble DI, eds.
          <article-title>International handbook for research in medical education</article-title>
          . Kluwer Academic Publishers, Dordrecht:
          <fpage>248</fpage>
          -320
          <string-name>
            <surname>Rentschler</surname>
            <given-names>DD</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Eaton</surname>
            <given-names>J</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Cappiello</surname>
            <given-names>J</given-names>
          </string-name>
          ,
          <string-name>
            <surname>McNally</surname>
            <given-names>SF</given-names>
          </string-name>
          ,
          <string-name>
            <surname>McWilliam</surname>
            <given-names>P</given-names>
          </string-name>
          (
          <year>2007</year>
          )
          <article-title>Evaluation of undergraduate students using Objective Structured Clinical Evaluation</article-title>
          .
          <source>J Nurs Educ</source>
          <volume>46</volume>
          (
          <issue>3</issue>
          ):
          <fpage>135</fpage>
          -9
          <string-name>
            <surname>Rushforth</surname>
            <given-names>HE</given-names>
          </string-name>
          (
          <year>2007</year>
          )
          <article-title>Objective structured clinical examination (OSCE): review of literature and implications for nursing education</article-title>
          .
          <source>Nurse Educ Today</source>
          <volume>27</volume>
          (
          <issue>5</issue>
          ):
          <fpage>481</fpage>
          -
          <lpage>90</lpage>
          . doi: 10.1016/j.nedt.
          <year>2006</year>
          .08.009
          <string-name>
            <surname>Schuwirth</surname>
            <given-names>LWT</given-names>
          </string-name>
          ,
          <string-name>
            <surname>van der Vleuten</surname>
            <given-names>CPM</given-names>
          </string-name>
          (
          <year>2003</year>
          )
          <article-title>The use of clinical simulations in assessment</article-title>
          .
          <source>Med Educ 37 Suppl</source>
          <volume>1</volume>
          :
          <fpage>65</fpage>
          -71
          <string-name>
            <surname>Swanson</surname>
            <given-names>D</given-names>
          </string-name>
          (
          <year>1987</year>
          )
          <article-title>A measurement framework for performance based tests</article-title>
          .
          <source>In: Hart IR</source>
          , Harden RM, eds.
          <article-title>Further developments in assessing clinical competence</article-title>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <given-names>Heal</given-names>
            <surname>Publications</surname>
          </string-name>
          , Montreal:
          <fpage>13</fpage>
          -45
          <string-name>
            <surname>Swanson</surname>
            <given-names>DB</given-names>
          </string-name>
          ,
          <string-name>
            <surname>van der Vleuten</surname>
            <given-names>CPM</given-names>
          </string-name>
          (
          <year>2013</year>
          )
          <article-title>Assessment of clinical skills with standardized patients: state of the art revisited</article-title>
          .
          <source>Teach Learn Med 25 Suppl</source>
          <volume>1</volume>
          :
          <fpage>S17</fpage>
          -
          <lpage>25</lpage>
          . doi: 10.1080/10401334.2013.842916
          <string-name>
            <surname>Tarrant</surname>
            <given-names>M</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Ware</surname>
            <given-names>J</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Mohammed</surname>
            <given-names>AM</given-names>
          </string-name>
          (
          <year>2009</year>
          )
          <article-title>An assessment of functioning and non-functioning distractors in multiple-choice questions: a descriptive analysis</article-title>
          .
          <source>BMC Med Educ</source>
          <volume>9</volume>
          :
          <fpage>40</fpage>
          . doi: 10.1186/
          <fpage>1472</fpage>
          -
          <lpage>6920</lpage>
          -
          <fpage>9</fpage>
          -40 van der
          <string-name>
            <surname>Vleuten</surname>
            <given-names>CP</given-names>
          </string-name>
          (
          <year>1996</year>
          )
          <article-title>The assessment of professional competence: Developments, research and practical implications</article-title>
          .
          <source>Adv Health Sci EducTheory Pract</source>
          <volume>1</volume>
          (
          <issue>1</issue>
          ):
          <fpage>41</fpage>
          -
          <lpage>67</lpage>
          . doi: 10.1007/BF00596229 van der
          <string-name>
            <surname>Vleuten</surname>
            <given-names>CPM</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Schuwirth</surname>
            <given-names>LWT</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Scheele</surname>
            <given-names>F</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Driessen</surname>
            <given-names>EW</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Hodges</surname>
            <given-names>B</given-names>
          </string-name>
          (
          <year>2010</year>
          )
          <article-title>The assessment of professional competence: building blocks for theory development</article-title>
          .
          <source>Best Pract Res Clin Obstet Gynaecol</source>
          <volume>24</volume>
          (
          <issue>6</issue>
          ):
          <fpage>703</fpage>
          -
          <lpage>19</lpage>
          . doi: 10.1016/j.bpobgyn.
          <year>2010</year>
          .04.001
          <string-name>
            <surname>Wessel</surname>
            <given-names>J</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Williams</surname>
            <given-names>R</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Finch</surname>
            <given-names>E</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Gémus</surname>
            <given-names>M</given-names>
          </string-name>
          (
          <year>2003</year>
          )
          <article-title>Reliability and validity of an objective structured clinical examination for physical therapy students</article-title>
          .
          <source>J Allied Health</source>
          <volume>32</volume>
          (
          <issue>4</issue>
          ):
          <fpage>266</fpage>
          -
          <lpage>9</lpage>
        </mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>
