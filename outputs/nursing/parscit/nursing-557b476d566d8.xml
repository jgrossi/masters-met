<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.9029685">
Have OSCEs come of age
in nursing education?
Marian Traynor and Despina Galanouli
Key words: Nursing education ■ Nursing assessment ■ Clinical
</bodyText>
<equation confidence="0.3644795">
competence ■ Calibration
T
</equation>
<bodyText confidence="0.894854166666667">
he objective structured clinical examination (OSCE)
was first developed for medical education in Scotland
by Harden et al (1975), and is now widely accepted
as a fit-for-purpose instrument for measuring clinical
reasoning skills with a high degree of technical accuracy. It
is a practical examination of what the candidate does with
the emphasis on assessing components of competence in
a structured way, in order to arrive at a technically sound
measure of clinical reasoning. Norman et al (2002) refer
to the work of O’Connor and McGraw (1994) and Fowell
and Bligh (1998) to suggest that the OSCE is widely
considered to provide a reliable and valid assessment of
clinical skills and is superior to traditional methods based
on the physical examination of real patients, the evaluation
of oral presentations of cases and written work. The OSCE
has therefore been adopted by many universities for the
assessment of healthcare competencies (Marshall and Harris,
2000; Wessel et al, 2003) and is generally accepted as a valid
assessment tool (Schuwirth and van der Vleuten, 2003) and
also as a formative teaching tool (Alinier, 2003).
Marian Traynor, Director of Education; Despina Galanouli Research
Fellow (Education), School of Nursing and Midwifery, Queen’s
University Belfast, Medical Biology Centre, Belfast
Accepted for publication: March 2015
</bodyText>
<page confidence="0.860002">
388
</page>
<bodyText confidence="0.977849095238095">
‘A well-designed and implemented OSCE
can provide students with opportunities to
demonstrate interpersonal and interview skills,
problem-solving abilities, teaching, assessment
skills, and application of basic clinical knowledge.’
(McWilliam and Botwinski, 2012: 38–9)
Rentschler et al (2007) also refer to students’ ability to ‘think
on their feet’, when presented with a simulated real-life health
issue, as another skill educators can observe during an OSCE.
The importance the OSCE has assumed in nursing
education is also evidenced in the increasing number of
international research studies aiming to develop and improve
the OSCE process. For example, in Australia, such research
has resulted in the development of an implementation
framework for using OSCEs in nursing (Henderson et
al, 2013) and also in the establishment of best practice
guidelines for use of OSCEs in order to improve student
learning (Nulty et al, 2011). In Sweden, OSCEs have
been used as a component of a multi-method assessment
strategy in combination with real-life (in vivo) conditions
(e.g. bedside observation examinations (Mårtensson and
Löfmark, 2013), and in the USA OSCEs have proved
effective for multidisciplinary learners (Corcoran et al,
2013). Numerous other studies on the OSCE experience of
students and its impact on them and other stakeholders have
provided evidence that this type of examination has been
judged worthwhile (Brosnan et al, 2006; Rentchler et al,
2007; McWilliam and Botwinski, 2010; Nulty et al, 2011).
Assessment of clinical competence
The study reported in this paper took place at a UK university
that implemented a dual-process approach to the assessment of
clinical competence in 2002. This approach entailed working
in close collaboration with clinical practitioners to develop a
robust assessment methodology for clinical skills. The resulting
assessment tool combined the validity associated with a
performance-based assessment, i.e. a measurement of what
the student can do in the practice placement, and the
enhanced reliability associated with an OSCE, a competencybased assessment of what the student can do in controlled
representations of professional practice (Boursicot et al, 2011;
Nulty et al, 2011). The model of clinical competence described
British Journal of Nursing, 2015, Vol 24, No 7
© 2015 MA Healthcare Ltd
</bodyText>
<sectionHeader confidence="0.905719" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999650222222222">
This article is intended to contribute to the current debate as to
whether the objective structured clinical examination (OSCE)
should become a standard assessment tool for undergraduate
nursing education as they currently are for medicine. The authors
describe how one UK university developed an OSCE for a nursing
undergraduate programme with the aim of emphasising the need
for nursing students to be competent in clinical skills and offering a
means of standardising the assessment of these skills. There has been
an increasing number of research studies carried out in this area at
international level and this article’s main contribution to the literature
is the description of the Angoff standard-setting procedure that was
used to calibrate the OSCE at this University and which makes it the
first nursing OSCE in the UK to incorporate a scientific standardsetting procedure.
McWilliam and Botwinski (2012) argue that a wider
range of skills can be tested through an OSCE. In a study to
identify the strengths and weaknesses of using an OSCE in a
nursing programme in New England, USA, with 60 full-time
students, they concluded that:
</bodyText>
<sectionHeader confidence="0.946588" genericHeader="keywords">
EDUCATION
</sectionHeader>
<bodyText confidence="0.995497">
by Miller (1990) with regard to mentorship and assessment was
influential in developing this approach.
</bodyText>
<subsectionHeader confidence="0.677208">
Developing the OSCE
Aim
</subsectionHeader>
<bodyText confidence="0.996984230769231">
The OSCE was developed for an undergraduate nursing
programme with the aim of emphasising the need for
nursing students to be competent in clinical skills and
offering a means of standardising the assessment of these
skills. It also aimed to identify students’ strengths and
weaknesses in skills performance and to enable them to
identify future learning needs. More importantly, the OSCE
identifies those who achieve a pass standard for clinical skills
and who can progress to the next year of their programme.
Since originally described by Harden and Gleeson in 1979,
the use of the OSCE has become widespread in both
undergraduate and postgraduate medical education and
also in a number of schools of nursing and midwifery. It
is important to note that the OSCE was not implemented
in this school in order to replace the assessment carried
out by the clinical mentor in practice, but rather it was
intended to complement assessment in the clinical setting.
This flexibility of the OSCE to be used both as a formative
and summative assessment tool has been described as one
of its advantages (Nulty et al, 2011; Liddle, 2014). It is
emphasised to the students that the acquisition of clinical
skills is paramount to the development of a competent safe
practitioner and the OSCE is one means to facilitate the
learning, teaching and assessment of these skills.
The OSCE in the undergraduate nursing curriculum of
this university therefore aims to:
</bodyText>
<listItem confidence="0.872950916666667">
■■ Contribute to the learning and teaching of clinical skills
■■ Assess the clinical competency of undergraduate nursing
students in a range of clinical skills
■■ Standardise the assessment process
■■ Encourage students to develop essential clinical skills in the
practice setting
■■ Identify potentially weak students who may require
additional support in clinical practice
■■ Identify students who can progress to the next year of their
studies.
© 2015 MA Healthcare Ltd
Standard setting
</listItem>
<bodyText confidence="0.99919854054054">
Standards are not only difficult to define; they are also difficult
to set. According to Tarrant et al (2009) pass standards for any
examinations should be set relative to the difficulty of the test
using any one of a number of established absolute standardsetting methods such as the Ebel procedure or Angoff method
that define a cut-off score, thereby identifying candidates
who are competent and eligible for progression There is no
perfect standard setting method and the decision on which
method to use is based on the most important criteria for that
particular examination (Cizek and Bunch, 2007; Bejar, 2008;
Dijkstra et al, 2012). Berk (1986) suggests that there are over 30
standard-setting procedures for an OSCE and one of the most
robust is the ‘Angoff Procedure’ (Angoff, 1971). The Angoff
standard-setting procedure normally involves three stages and a
minimum of 30 judges in order to minimise error. In stage one,
judges consider the test items and make a judgement about a
British Journal of Nursing, 2015, Vol 24, No 7
‘minimally acceptable’ student’s probability of answering each
item correctly at the end of a period of training. This is carried
out in private, by the judges. During stage two, a sample of
students’ actual scores is given to the judges and the judges are
given the opportunity to reconsider their initial judgements.
Again this stage is carried out in private. The third and final
stage involves judges clarifying their reasons for the standard
they set in stage two and all judges are given the opportunity to
revisit the standards set during the second stage.
The cut score is taken as the average of the judgements at
the end of stage three and can be defined as the minimum
standard of competence to be achieved. By comparing the
score the student receives, with the cut score, inferences
about the competence of each student can be made. The
cut score is therefore of great significance to the assessment
tool. If, for example, in the case of an assessment tool
designed for nursing students, the cut score is set too low,
many incompetent professionals could conceivably attain
professional status. Alternatively, if the cut score is set too
high, many capable students will fail the assessment and
therefore be unable to register with the profession.
Calibration of the pass score
The OSCE has been used in nursing assessment in other
UK universities, but there is no published literature to
suggest the procedure chosen to set a defensible, reliable
standard setting has been addressed. The school in this
study chose a modified Angoff (1971) standard-setting
procedure to calibrate the OSCE making it, effectively, the
first nursing OSCE in the UK to incorporate a scientific
standard-setting procedure.
Two panels of experts were established for the standard
setting: one consisting of clinical mentors (n=10) who
were experienced assessors and one consisting of nurse
lecturers (n=20) currently involved in assessing student
clinical performance. To encourage full participation from
clinical mentors it was made clear to them that the standard
setting process was about professional judgement and did not
involve assessing students. Ten mentors agreed to be involved.
The participants (judges) were given an outline of the
OSCE structure and an explanation of the different stations.
At each station candidates are assigned a specific clinical
task to perform, for example, they may be asked to measure
blood pressure, or provide post-operative care. They may
be asked to do this on a simulated patient or on a manikin.
This form of assessment does not use real patients but
while validity may be compromised, it is not sacrificed,
and the design does enhance reliability. Each of the 30
participants (10 mentors and 20 lecturers) was given copies
of what would happen at each station and a checklist of the
competencies to be assessed at each station. After looking
through this information, each participant was given a copy
of each of the standard-setting checklists. The standard
setting was completed using a modified Angoff approach.
The Angoff-determined passing score is the mean of the
passing scores calculated from the second-stage judgement
proformas. The passing score was found to be 648 (the
maximum score on the OSCE was 750). The passing
score therefore was the judges’ standard (648) minus two
</bodyText>
<page confidence="0.993395">
389
</page>
<subsubsectionHeader confidence="0.491718">
Implementation
</subsubsectionHeader>
<bodyText confidence="0.988785625">
The university has two intakes of nursing students each
year, which means that 400 students undertake an OSCE
during the first year of the undergraduate programme.
Typically, in the first year OSCE there are seven individual
stations lasting 5 minutes each and they include measuring
blood pressure, recording vital signs and urinalysis. In the
second year there are ten stations lasting 10 minutes each
and include post-operative care, recording and interpreting
</bodyText>
<table confidence="0.512059666666667">
Glasgow Coma Scale scores.
Preparation of students
Research suggests that student preparation for the OSCE, as
</table>
<bodyText confidence="0.993430368421053">
well as subsequent feedback on their performance, is crucial
for a successful OSCE process. For example, findings of a
qualitative study by Cazzell and Rodriguez (2011) on the
experiences of nursing students with the OSCE suggest that
some students experienced high levels of stress and a loss of
control, which they related to lack of preparation. Students
have also reported that they failed to make the connection
between the OSCE experience and future clinical practice
owing to the lack of immediate feedback (Liddle, 2014;
Cazzell and Rodriguez, 2011).
Student preparation therefore plays an important role
when planning an OSCE, and the school of nursing and
midwifery where the nursing OSCE was developed takes
the measures described below to ensure student familiarity
with the assessment procedure, in an attempt to reduce
potential stress and anxiety:
■■ All students on the undergraduate nursing programme are
made aware of the format of the OSCE and that it is part
of the assessment process for the clinical modules
</bodyText>
<figure confidence="0.571644444444444">
KEY POINTS
n
Objective
structured clinical examinations (OSCEs) offer the potential for
highly valid and reliable assessments of clinical practice
n
The high reliability of OSCEs is warranted by a standard setting procedure
such as Angoff
n
OSCEs
may be used formatively (to improve learning) or summatively to
determine competence
n
OSCE-development
nurse lecturers
390
benefits from a partnership between clinical mentors and
■■ Lecturing
</figure>
<bodyText confidence="0.995123917808219">
staff meet with the students to offer advice and
answer questions related to the administration of the OSCE
examination
■■ Information on the OSCE is posted on the student
resources pages of the virtual learning environment (VLE);
this includes a short video of a typical OSCE. Students
are therefore made aware of the range of clinical skills that
could be tested in the OSCE. Students are made aware of
the fact that they can schedule sessions in the clinical skills
education centre to practice and refine specific skills
■■ Students are advised that they must present for the
examination in uniform with a valid student identity card
■■ The date and time of the examination are posted on the
student resources pages of the University’s VLE.
Preparation of OSCE assessors
From the beginning of the nursing OSCE-development
process, the school recognised the importance of involving
clinical mentors in the assessment process. A partnership
approach was adopted and both clinical mentors and nurse
lecturers were invited to act as examiners for the OSCE. It is
important to note that, while other professional bodies, such as
the General Medical Council (GMC), set out clear guidelines
on the roles and responsibilities of assessors in OSCEs (GMC,
2009), no such guidelines exist within the Nursing and
Midwifery Council (NMC). It is therefore the responsibility
of each higher education institution to develop their own
standards and guidelines. However, the NMC does recommend
the OSCE for the assessment of medicines administration
(Liddle, 2014) and, notably, the latest NMC guidelines on
overseas competency testing for UK registration have included
the OSCE as a practical assessment (NMC, 2014).
The school promotes consistency in the operation of its
OSCEs by inviting all OSCE examiners to attend a briefing
session before the examination. Examiners are fully briefed on
the nature of the examination, i.e. the number of stations, the
pass score and the compensatory aspect of the examination
(the pass mark is aggregated across the stations and therefore a
student may do less well in one station but can compensate in
another).They are also advised of their roles and responsibilities
and are shown the marking criteria for each station, with a
detailed question and answer session for any queries they have.
This school recognises that the organisational aspect of the
OSCE is very demanding and has therefore assigned specific
administration support to academic staff in the planning and
execution of the OSCE. The school has invested in an optical
computerised reader to assist in the scanning of the OSCE
papers and calibration of results. The School administrator
attends regular meetings to agree the stations, to timetable the
examiners and to timetable the simulated patients who are
at the different stations in the OSCE. The clinical allocations
manager in the school is advised of the OSCE dates and
circulates the dates to the clinical areas so that mentors can
put their names forward to act as examiners. Mentors who
participate in the OSCE are provided with a certificate and
this is recognised as evidence to support mentorship updates.
Currently the school has in excess of 200 mentors on the
OSCE examiners’ database. Mentors cite the OSCE as not
only providing them with insight into the undergraduate
British Journal of Nursing, 2015, Vol 24, No 7
© 2015 MA Healthcare Ltd
standard errors of measurement (SEM). If no SEM is added
to the cut score the probabilities of making two errors are
approximately balanced. These are when a passing student
is failed or a failing student is passed. Adding one SEM
makes the likelihood of failing a passing student about five
times more likely than passing a failing student. Adding two
SEM makes the likelihood of failing a passing student about
twenty times that of passing a failing student.
Following adjustment for test fallibility, a passing score of
600 (80%) was established for the first year nursing OSCE (the
same process was employed for the second year nursing OSCE
with a pass score of 70%).The second year OSCE stations were
testing higher-order clinical skills.
</bodyText>
<sectionHeader confidence="0.903794" genericHeader="introduction">
EDUCATION
</sectionHeader>
<bodyText confidence="0.991419441860465">
curriculum but also credit the OSCE with encouraging
students to concentrate on achieving competence in clinical
skills while on practice placement.
Conclusion
The role of the nurse is changing and will continue to
change alongside the needs and expectations of patients.
Now more than ever there is a need to demonstrate to the
public that higher education institutions with a responsibility
for undergraduate nurse education are prepared to invest in
implementing procedures that ensure nursing students are
tested using the best assessment methods available.
Since their original development OSCEs have become
established as one of the main methods of assessing clinical
competence in medical education (Gormley, 2011). This is not
replicated in nursing education where the uptake is sporadic
and, according to Rushforth (2007), in some HEIs the OSCE
process has been subject to major adaptations which may
have undermined important elements of the original model.
Although the OSCE is not without its critics, the potential for
high reliability (Swanson 1987; Swanson and van der Vleuten
2013) and the emerging evidence on its increased validity
(Petrusa 2002, Downing 2003), cannot be ignored, making it a
valuable assessment method.
The challenge, therefore, is to persuade those with
responsibility for validating nursing curricula within the UK
to adopt the OSCE in combination with other methods,
as a standard assessment tool for undergraduate education.
It is important to note that the OSCE does not provide a
complete profile of a student’s level of competency but there
is enough evidence to support the adaptation of the OSCE
as one method to assess competence. This will also require
those higher education institutions who have a responsibility
for undergraduate nurse education to invest in the OSCE
as means of not only achieving excellence in education but
also ensuring public confidence in the profession. There are
cost implications, of course, as good assessment methods are
resource intensive. However, as van der Vleuten (1996) argued,
investing in assessment is investing in teaching and learning;
good assessment will facilitate good learning. Ultimately,
therefore, good assessment methods are more likely to
produce nurse graduates who are not only academically
qualified but also have the required proficiencies at the point
of registration; something which the NMC require and the
</bodyText>
<table confidence="0.638424384615385">
BJN
public demand.
© 2015 MA Healthcare Ltd
Conflict of interest: none
Alinier G (2003) Nursing students’ and lecturers’ perspectives of objective
structured clinical examination incorporating simulation. Nurse Educ Today
23(6): 419–26
Angoff WH (1971) Scales, Norms and Equivalent Scores. In: Thorndike RL,
ed. Educational Measurement. 2nd edn. American Council of Education,
Washington DC: 508-600
Bejar I (2008) Standard Setting: What Is It? Why Is It Important? R&amp;D
Connections 7: 1-6. http://tinyurl.com/nq5bkna (accessed 25 March 2015)
Berk RA (1986) A consumer’s guide to setting standards on criterionreferenced tests. Review of Educational Research. 56: 137–72
</table>
<reference confidence="0.960764744680851">
Boursicot KAM, Roberts TE, Burdick WP (2011) Structured assessments of
clinical competence. In: Swanwick T, ed. Understanding Medical Education,
Evidence,Theory and Practice. Wiley-Blackwell, Chichester: 246–258
Brosnan M, Evans W, Brosnan E, Brown G (2006) Implementing objective
structured clinical skills evaluation (OSCE) in nurse registration programmes
in a centre in Ireland: a utilisation focused evaluation. Nurse Educ Today 26(2):
British Journal of Nursing, 2015, Vol 24, No 7
115–22. doi: 10.1016/j.nedt.2005.08.003
Cazzell M, Rodriguez A (2011) Qualitative analysis of student beliefs and
attitudes after an objective structured clinical evaluation: implications for
affective domain learning in undergraduate nursing education. J Nurs Educ
50(12): 711–4. doi: 10.3928/01484834-20111017-04
Cizek GJ, Bunch MB (2007) Standard Setting: A Guide to Establishing and
Evaluating Performance Standards on Tests. Sage Publications, Thousand Oaks
Corcoran AM, Lysaght S, Lamarra D, Ersek M (2013) Pilot test of a three-station
palliative care observed structured clinical examination for multidisciplinary
trainees. J Nurs Educ 52(5): 294–8. doi: 10.3928/01484834-20130328-02
Dijkstra J, Galbraith R, Hodges BD et al (2012) Expert validation of fit-forpurpose guidelines for designing programmes of assessment. BMC Med
Educ 12: 20. doi: 10.1186/1472-6920-12-20
Downing SM (2003) Validity: on the meaningful interpretation of assessment
data. Med Educ 37:830-7. doi: 10.1046/j.1365-2923.2003.01594.x
Fowell SL, Bligh JG (1998) Recent developments in assessing medical students.
Postgrad Med J 74(867): 18–24
General Medical Council (2009) Assessment in undergraduate medical
education: advice supplementary to Tomorrow’s Doctors. General Medical
Council, London. http://tinyurl.com/qgptkw2 (accessed 26 March 2015)
Gormley G (2011) Summative OSCEs in undergraduate medical education.
Ulster Med J 80(3): 127–32
Harden RM, Stevenson M, Downie WW, Wilson GM (1975) Assessment of
clinical competence using objective structured examination. BMJ 1:447. doi:
http://dx.doi.org/10.1136/bmj.1.5955.447
Harden RM, Gleeson F (1979) Assessment of clinical competence using an
objective structured clinical examination. Med Educ 13: 39-54. doi: 10.1111/
j.1365-2923.1979.tb00918.x
Henderson A, Nulty DD, Mitchell ML et al (2013) An implementation
framework for using OSCEs in nursing curricula. Nurse Educ Today 33(12):
1459–61. doi: 10.1016/j.nedt.2013.04.008
Marshall G, Harris P (2000) A study of the role of an objective structured
clinical examination (OSCE) in assessing clinical competence in third year
student radiographers. Radiography 6(2): 117-22
Mårtensson G, Löfmark A (2013) Implementation and student evaluation of
clinical final examination in nursing education. Nurse Educ Today 33(12):
1563–8. doi: 10.1016/j.nedt.2013.01.003
McWilliam P, Botwinski C (2010) Developing a successful nursing
Objective Structured Clinical Examination. J Nurs Educ 49(1): 36–41. doi:
10.3928/01484834-20090915-01
McWilliam PL, Botwinski CA (2012) Identifying strengths and weaknesses in
the utilization of Objective Structured Clinical Examination (OSCE) in a
nursing program. Nurs Educ Perspect 33(1): 35–9
Miller GE (1990) The assessment of clinical skills/competence/performance.
Acad Med 65(9 Suppl): S63–7
Norman IJ,Watson R, Murrells T, Calman L, Redfern S (2002) The validity and
reliability of methods to assess the competence to practise of pre-registration
nursing and midwifery students. Int J Nurs Stud 39(2): 133–45
Nulty DD, Mitchell ML, Jeffrey CA, Henderson A, Groves M (2011) Best
Practice Guidelines for use of OSCEs: Maximising value for student learning.
Nurse Educ Today 31(2): 145–51. doi: 10.1016/j.nedt.2010.05.006
Nursing and Midwifery Council (2014) Registering as a nurse or midwife in
the UK: Information for nurses and midwives who trained outside of the
EU or EEA countries. http://tinyurl.com/pul2ry6 (accessed 25 March 2015)
Liddle C (2014) The objective structured clinical examination. Nursing Times.
110(Online issue). http://tinyurl.com/ptbbwbv (accessed 25 March 2015)
O’Connor HM, McGraw RC (1997) Clinical skills training: developing
objective assessment instruments. Med Educ 31(5): 359–63
Petrusa ER (2002) Clinical performance assessments. In Norman GR, van der
Vleuten CPM, Newble DI, eds. International handbook for research in medical
education. Kluwer Academic Publishers, Dordrecht: 248-320
Rentschler DD, Eaton J, Cappiello J, McNally SF, McWilliam P (2007)
Evaluation of undergraduate students using Objective Structured Clinical
Evaluation. J Nurs Educ 46(3): 135–9
Rushforth HE (2007) Objective structured clinical examination (OSCE):
review of literature and implications for nursing education. Nurse Educ Today
27(5): 481–90. doi: 10.1016/j.nedt.2006.08.009
Schuwirth LWT, van der Vleuten CPM (2003) The use of clinical simulations
in assessment. Med Educ 37 Suppl 1: 65–71
Swanson D (1987) A measurement framework for performance based tests. In:
Hart IR, Harden RM, eds. Further developments in assessing clinical competence.
Heal Publications, Montreal: 13-45
Swanson DB, van der Vleuten CPM (2013) Assessment of clinical skills with
standardized patients: state of the art revisited. Teach Learn Med 25 Suppl 1:
S17–25. doi: 10.1080/10401334.2013.842916
Tarrant M, Ware J, Mohammed AM (2009) An assessment of functioning
and non-functioning distractors in multiple-choice questions: a descriptive
analysis. BMC Med Educ 9: 40. doi: 10.1186/1472-6920-9-40
van der Vleuten CP (1996) The assessment of professional competence:
Developments, research and practical implications. Adv Health Sci Educ Theory
Pract 1(1): 41–67. doi: 10.1007/BF00596229
van der Vleuten CPM, Schuwirth LWT, Scheele F, Driessen EW, Hodges
B (2010) The assessment of professional competence: building blocks for
theory development. Best Pract Res Clin Obstet Gynaecol 24(6): 703–19. doi:
10.1016/j.bpobgyn.2010.04.001
Wessel J, Williams R, Finch E, Gémus M (2003) Reliability and validity of an
objective structured clinical examination for physical therapy students. J Allied
Health 32(4): 266–9
</reference>
<page confidence="0.977208">
391
</page>
<reference confidence="0.662885">
Copyright of British Journal of Nursing is the property of Mark Allen Publishing Ltd and its
content may not be copied or emailed to multiple sites or posted to a listserv without the
copyright holder&apos;s express written permission. However, users may print, download, or email
articles for individual use.
</reference>
</variant>
</algorithm>

<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Boursicot KAM</author>
<author>Roberts TE</author>
<author>Burdick WP</author>
</authors>
<title>Structured assessments of clinical competence. In: Swanwick T, ed. Understanding Medical Education, Evidence,Theory and Practice. Wiley-Blackwell,</title>
<date>2011</date>
<journal>J Nurs Educ</journal>
<booktitle>Nurse Educ Today 26(2): British Journal of Nursing, 2015, Vol 24, No 7 115–22. doi: 10.1016/j.nedt.2005.08.003 Cazzell M, Rodriguez A</booktitle>
<volume>50</volume>
<issue>12</issue>
<pages>246--258</pages>
<editor>Corcoran AM, Lysaght S, Lamarra D, Ersek M</editor>
<location>Chichester:</location>
<marker>KAM, TE, WP, 2011</marker>
<rawString>Boursicot KAM, Roberts TE, Burdick WP (2011) Structured assessments of clinical competence. In: Swanwick T, ed. Understanding Medical Education, Evidence,Theory and Practice. Wiley-Blackwell, Chichester: 246–258 Brosnan M, Evans W, Brosnan E, Brown G (2006) Implementing objective structured clinical skills evaluation (OSCE) in nurse registration programmes in a centre in Ireland: a utilisation focused evaluation. Nurse Educ Today 26(2): British Journal of Nursing, 2015, Vol 24, No 7 115–22. doi: 10.1016/j.nedt.2005.08.003 Cazzell M, Rodriguez A (2011) Qualitative analysis of student beliefs and attitudes after an objective structured clinical evaluation: implications for affective domain learning in undergraduate nursing education. J Nurs Educ 50(12): 711–4. doi: 10.3928/01484834-20111017-04 Cizek GJ, Bunch MB (2007) Standard Setting: A Guide to Establishing and Evaluating Performance Standards on Tests. Sage Publications, Thousand Oaks Corcoran AM, Lysaght S, Lamarra D, Ersek M (2013) Pilot test of a three-station palliative care observed structured clinical examination for multidisciplinary trainees. J Nurs Educ 52(5): 294–8. doi: 10.3928/01484834-20130328-02 Dijkstra J, Galbraith R, Hodges BD et al (2012) Expert validation of fit-forpurpose guidelines for designing programmes of assessment. BMC Med Educ 12: 20. doi: 10.1186/1472-6920-12-20 Downing SM (2003) Validity: on the meaningful interpretation of assessment data. Med Educ 37:830-7. doi: 10.1046/j.1365-2923.2003.01594.x Fowell SL, Bligh JG (1998) Recent developments in assessing medical students.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Postgrad Med J</author>
</authors>
<title>74(867): 18–24 General Medical Council (2009) Assessment in undergraduate medical education: advice supplementary to Tomorrow’s Doctors.</title>
<date>2015</date>
<journal>General Medical Council, London. http://tinyurl.com/qgptkw2 (accessed</journal>
<booktitle>Gormley G</booktitle>
<volume>26</volume>
<marker>J, 2015</marker>
<rawString>Postgrad Med J 74(867): 18–24 General Medical Council (2009) Assessment in undergraduate medical education: advice supplementary to Tomorrow’s Doctors. General Medical Council, London. http://tinyurl.com/qgptkw2 (accessed 26 March 2015) Gormley G (2011) Summative OSCEs in undergraduate medical education.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Harden RM</author>
<author>M Stevenson</author>
<author>Downie WW</author>
<author>Wilson GM</author>
</authors>
<title>Assessment of clinical competence using objective structured examination. BMJ 1:447. doi: http://dx.doi.org/10.1136/bmj.1.5955.447</title>
<date>1975</date>
<journal>Med Educ</journal>
<booktitle>Nurse Educ Today 33(12): 1459–61. doi: 10.1016/j.nedt.2013.04.008</booktitle>
<volume>13</volume>
<pages>39--54</pages>
<location>McWilliam PL, Botwinski CA</location>
<marker>RM, Stevenson, WW, GM, 1975</marker>
<rawString>Ulster Med J 80(3): 127–32 Harden RM, Stevenson M, Downie WW, Wilson GM (1975) Assessment of clinical competence using objective structured examination. BMJ 1:447. doi: http://dx.doi.org/10.1136/bmj.1.5955.447 Harden RM, Gleeson F (1979) Assessment of clinical competence using an objective structured clinical examination. Med Educ 13: 39-54. doi: 10.1111/ j.1365-2923.1979.tb00918.x Henderson A, Nulty DD, Mitchell ML et al (2013) An implementation framework for using OSCEs in nursing curricula. Nurse Educ Today 33(12): 1459–61. doi: 10.1016/j.nedt.2013.04.008 Marshall G, Harris P (2000) A study of the role of an objective structured clinical examination (OSCE) in assessing clinical competence in third year student radiographers. Radiography 6(2): 117-22 Mårtensson G, Löfmark A (2013) Implementation and student evaluation of clinical final examination in nursing education. Nurse Educ Today 33(12): 1563–8. doi: 10.1016/j.nedt.2013.01.003 McWilliam P, Botwinski C (2010) Developing a successful nursing Objective Structured Clinical Examination. J Nurs Educ 49(1): 36–41. doi: 10.3928/01484834-20090915-01 McWilliam PL, Botwinski CA (2012) Identifying strengths and weaknesses in the utilization of Objective Structured Clinical Examination (OSCE) in a nursing program. Nurs Educ Perspect 33(1): 35–9 Miller GE (1990) The assessment of clinical skills/competence/performance.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norman IJ</author>
<author>R Watson</author>
<author>T Murrells</author>
<author>L Calman</author>
<author>S Redfern</author>
</authors>
<title>The validity and reliability of methods to assess the competence to practise of pre-registration nursing and midwifery students.</title>
<date>2002</date>
<journal>Int J Nurs Stud</journal>
<booktitle>Acad Med 65(9 Suppl): S63–7</booktitle>
<volume>39</volume>
<issue>2</issue>
<pages>133--45</pages>
<marker>IJ, Watson, Murrells, Calman, Redfern, 2002</marker>
<rawString>Acad Med 65(9 Suppl): S63–7 Norman IJ,Watson R, Murrells T, Calman L, Redfern S (2002) The validity and reliability of methods to assess the competence to practise of pre-registration nursing and midwifery students. Int J Nurs Stud 39(2): 133–45 Nulty DD, Mitchell ML, Jeffrey CA, Henderson A, Groves M (2011) Best Practice Guidelines for use of OSCEs: Maximising value for student learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nurse</author>
</authors>
<title>Educ Today 31(2): 145–51. doi: 10.1016/j.nedt.2010.05.006 Nursing and Midwifery Council (2014) Registering as a nurse or midwife in the UK: Information for nurses and midwives who trained outside of the EU or EEA countries.</title>
<date>2015</date>
<note>http://tinyurl.com/pul2ry6 (accessed 25</note>
<marker>Nurse, 2015</marker>
<rawString>Nurse Educ Today 31(2): 145–51. doi: 10.1016/j.nedt.2010.05.006 Nursing and Midwifery Council (2014) Registering as a nurse or midwife in the UK: Information for nurses and midwives who trained outside of the EU or EEA countries. http://tinyurl.com/pul2ry6 (accessed 25 March 2015) Liddle C (2014) The objective structured clinical examination. Nursing Times.</rawString>
</citation>
<citation valid="false">
<title>Clinical skills training: developing objective assessment instruments. Med Educ 31(5): 359–63 Petrusa ER (2002) Clinical performance assessments. In</title>
<date>2015</date>
<journal>J Nurs Educ</journal>
<booktitle>Nurse Educ Today 27(5): 481–90. doi: 10.1016/j.nedt.2006.08.009 Schuwirth LWT, van der Vleuten CPM (2003) The use of clinical simulations in assessment. Med Educ 37 Suppl 1: 65–71 Swanson D</booktitle>
<volume>25</volume>
<pages>248--320</pages>
<editor>O’Connor HM, McGraw RC</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht:</location>
<marker>2015</marker>
<rawString>110(Online issue). http://tinyurl.com/ptbbwbv (accessed 25 March 2015) O’Connor HM, McGraw RC (1997) Clinical skills training: developing objective assessment instruments. Med Educ 31(5): 359–63 Petrusa ER (2002) Clinical performance assessments. In Norman GR, van der Vleuten CPM, Newble DI, eds. International handbook for research in medical education. Kluwer Academic Publishers, Dordrecht: 248-320 Rentschler DD, Eaton J, Cappiello J, McNally SF, McWilliam P (2007) Evaluation of undergraduate students using Objective Structured Clinical Evaluation. J Nurs Educ 46(3): 135–9 Rushforth HE (2007) Objective structured clinical examination (OSCE): review of literature and implications for nursing education. Nurse Educ Today 27(5): 481–90. doi: 10.1016/j.nedt.2006.08.009 Schuwirth LWT, van der Vleuten CPM (2003) The use of clinical simulations in assessment. Med Educ 37 Suppl 1: 65–71 Swanson D (1987) A measurement framework for performance based tests. In: Hart IR, Harden RM, eds. Further developments in assessing clinical competence.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Heal Publications</author>
</authors>
<title>Assessment of clinical skills with standardized patients: state of the art revisited. Teach Learn Med 25 Suppl 1: S17–25. doi: 10.1080/10401334.2013.842916</title>
<date></date>
<journal>Best Pract Res Clin Obstet Gynaecol</journal>
<booktitle>Adv Health Sci Educ Theory Pract 1(1): 41–67. doi: 10.1007/BF00596229</booktitle>
<volume>24</volume>
<issue>6</issue>
<pages>703--19</pages>
<location>Montreal:</location>
<marker>Publications, </marker>
<rawString>Heal Publications, Montreal: 13-45 Swanson DB, van der Vleuten CPM (2013) Assessment of clinical skills with standardized patients: state of the art revisited. Teach Learn Med 25 Suppl 1: S17–25. doi: 10.1080/10401334.2013.842916 Tarrant M, Ware J, Mohammed AM (2009) An assessment of functioning and non-functioning distractors in multiple-choice questions: a descriptive analysis. BMC Med Educ 9: 40. doi: 10.1186/1472-6920-9-40 van der Vleuten CP (1996) The assessment of professional competence: Developments, research and practical implications. Adv Health Sci Educ Theory Pract 1(1): 41–67. doi: 10.1007/BF00596229 van der Vleuten CPM, Schuwirth LWT, Scheele F, Driessen EW, Hodges B (2010) The assessment of professional competence: building blocks for theory development. Best Pract Res Clin Obstet Gynaecol 24(6): 703–19. doi: 10.1016/j.bpobgyn.2010.04.001 Wessel J, Williams R, Finch E, Gémus M (2003) Reliability and validity of an objective structured clinical examination for physical therapy students. J Allied Health 32(4): 266–9 Copyright of British Journal of Nursing is the property of Mark Allen Publishing Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder&apos;s express written permission. However, users may print, download, or email articles for individual use.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>