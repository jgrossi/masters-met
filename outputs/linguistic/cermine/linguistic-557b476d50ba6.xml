<article>
  <front>
    <journal-meta>
      <journal-title-group>
        <journal-title>Brain &amp; Language</journal-title>
      </journal-title-group>
    </journal-meta>
    <article-meta>
      <title-group>
        <article-title>Brain &amp; Language</article-title>
      </title-group>
      <article-id pub-id-type="doi">10.1016/j.bandl.2012.01.005</article-id>
      <contrib-group>
        <contrib contrib-type="author">
          <string-name>Michael Grosvald</string-name>
          <email>m.grosvald@uci.edu</email>
          <xref ref-type="aff" rid="2">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Eva Gutierrez</string-name>
          <xref ref-type="aff" rid="0">0</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Sarah Hafer</string-name>
          <xref ref-type="aff" rid="0">0</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>David Corina</string-name>
          <xref ref-type="aff" rid="0">0</xref>
          <xref ref-type="aff" rid="1">1</xref>
        </contrib>
        <aff id="0">
          <label>0</label>
          <institution>Center for Mind and Brain, University of California at Davis</institution>
          ,
          <country country="US">United States</country>
        </aff>
        <aff id="1">
          <label>1</label>
          <institution>Departments of Linguistics and Psychology, University of California at Davis</institution>
          ,
          <country country="US">United States</country>
        </aff>
        <aff id="2">
          <label>2</label>
          <institution>Department of Neurology, University of California at Irvine</institution>
          ,
          <country country="US">United States</country>
        </aff>
      </contrib-group>
      <abstract>
        <p>a b s t r a c t A fundamental advance in our understanding of human language would come from a detailed account of how non-linguistic and linguistic manual actions are differentiated in real time by language users. To explore this issue, we targeted the N400, an ERP component known to be sensitive to semantic context. Deaf signers saw 120 American Sign Language sentences, each consisting of a ''frame'' (a sentence without the last word; e.g. BOY SLEEP IN HIS) followed by a ''last item'' belonging to one of four categories: a high-close-probability sign (a ''semantically reasonable'' completion to the sentence; e.g. BED), a lowclose-probability sign (a real sign that is nonetheless a ''semantically odd'' completion to the sentence; e.g. LEMON), a pseudo-sign (phonologically legal but non-lexical form), or a non-linguistic grooming gesture (e.g. the performer scratching her face). We found significant N400-like responses in the incongruent and pseudo-sign contexts, while the gestures elicited a large positivity. 2012 Elsevier Inc. All rights reserved.</p>
      </abstract>
      <kwd-group>
        <kwd>Sign language</kwd>
        <kwd>ASL</kwd>
        <kwd>ERP</kwd>
        <kwd>N400</kwd>
        <kwd>Deaf</kwd>
        <kwd>Pseudo-word</kwd>
        <kwd>Grooming gesture</kwd>
      </kwd-group>
      <volume>121</volume>
      <issue>2012</issue>
      <fpage>12</fpage>
      <lpage>24</lpage>
      <pub-date>
        <year>2012</year>
      </pub-date>
    </article-meta>
  </front>
  <body>
    <sec id="1">
      <title>1. Introduction</title>
      <p>While it is now widely accepted that signed languages used in
deaf communities around the world represent full-fledged
instantiations of human languages—languages which are expressed in the
visual–manual modality rather than the aural–oral modality—the
question of how a sign is recognized and integrated into a sentential
context in real time has received far less attention (see Corina &amp;
Knapp, 2006; Emmorey, 2002; for some discussions). Sign language
recognition may be more complicated than spoken language
recognition by virtue of the fact that the primary articulators, the hands
and arms, are also used in a wide range of other common everyday
behaviors that include non-linguistic actions such a reaching and
grasping, waving, and scratching oneself, as well gesticulations that
accompany speech (i.e. co-speech gestures) or serve non-sign
language deictic functions, such as pointing.</p>
      <p>The formal relationship between signed languages and human
gestural actions is of considerable interest to a range of disciplines.
Linguists, psychologists and cognitive scientists have proposed a
critical role for manual gesture in the development and evolution
of human languages (Arbib, 2005, 2008; Gentilucci &amp; Corballis,
2006; Rizzolatti &amp; Arbib, 1998; Tomasello, 2005; Wilcox, 2004).
Recently, linguists have documented compelling evidence that the
development of nascent sign languages derives from idiosyncratic
gestural and pantomimic systems used by isolated communities,
which in some cases may be limited to individual families who have
a need to communicate with a deaf child (Frishberg, 1987;
GoldinMeadow, 2003; Kegl, Senghas, &amp; Coppola, 1999; Meir, Sandler,
Padden, &amp; Aronoff, 2010; Morford &amp; Kegl, 2000; Senghas, 2005). Even
within mature sign languages of Deaf communities, linguistic
accounts of sign language structure have also argued that lexical and
discourse components of American Sign Language (ASL) and other
signed languages may be best understood as being gesturally based
(Liddell, 2003). Thus diachronic and synchronic evidence from
language research support the contention that signed languages might
make use of perceptual systems similar to those through which
humans understand or parse human actions and gestures more
generally (Corballis, 2009). In contrast, given its linguistic status, sign
language perception may require the attunement of specialized
systems for recognizing sign forms.</p>
      <p>A comprehensive theory of sign language recognition will be
enhanced by providing an account of when and how the processing
of sign forms diverges from the processing of human actions in
general. Recent behavioral and neuro-imaging studies have
reported differences in deaf subjects’ responses to single signs
compared to non-linguistic gestures (Corina, Grosvald, &amp; Lachaud,
2011; Corina et al., 2007; Emmorey, Xu, Gannon, Goldin-Meadow,
&amp; Braun, 2010; MacSweeney et al., 2004), but no studies to our
knowledge have examined the recognition of signs and gestures
under sentence processing constraints. Consider for example the
signer, who, in mid-sentence, fulfills the urge to scratch his face,
or perhaps swat away a flying insect. What is the fate of this
non-linguistic articulation? Does the sign perceiver attempt to
incorporate these manual behaviors into accruing sentential
representations, or are these actions easily tagged as non-linguistic and
thus rejected by the parser? The goal of the present paper was to
use real-time electrophysiological measures to assess empirically
the time course of sentence processing in cases where subjects
encountered non-linguistic manual forms (here ‘‘self-grooming’’
behaviors, e.g. scratching the face, rubbing one’s eye, adjusting
the sleeves of a shirt, etc.). We sought to compare the processing
of these non-linguistic gestural forms within a sentential context
to cases in which deaf signers encountered violations of semantic
expectancy that have been observed to elicit a well-defined
electrophysiological component, the N400.</p>
      <p>The N400 component (Holcomb &amp; Neville, 1991; Kutas &amp;
Hillyard, 1980) has been frequently investigated in previous ERP
research on written, spoken and signed language (e.g. Capek et al.,
2009; Kutas, Neville, &amp; Holcomb, 1987). The N400 is a broad
negative deflection generally seen at central and parietal scalp sites
that peaks about 400 ms after the visual or auditory presentation
of a word. Although all content words elicit an N400 component,
the ERP response is larger for words that are semantically
anomalous or less expected (Hagoort &amp; Brown, 1994; Kutas &amp; Hillyard,
1984); thus the N400 is often interpreted as an index of ease or
difficulty in semantic conceptual integration (Brown &amp; Hagoort, 1993;
Hagoort &amp; Van Berkum, 2007). For example, for listeners
encountering the two sentences ‘‘I like my coffee with milk and sugar’’
and ‘‘I like my coffee with milk and mud,’’ the N400 response to
the last word in the second item is expected to be larger.</p>
      <p>An N400 or N400-like component can also be found in response
to orthographically/phonologically legal but non-occurring
‘‘pseudo-words’’ (e.g. ‘‘blork’’), and it has sometimes been reported that
pseudo-words elicit a stronger N400 response than semantically
incongruent real words (Bentin, 1987; Bentin, McCarthy, &amp; Wood,
1985; Hagoort &amp; Kutas, 1995), consistent with the idea that the
magnitude of N400 response is related to the difficulty of the
ongoing process of semantic-contextual integration. However,
orthographically illegal ‘‘non-words’’ (e.g. ‘‘rbsnk’’) do not generally
elicit an N400, and a positive component is sometimes seen instead
(Hagoort &amp; Kutas, 1995; Ziegler, Besson, Jacobs, Nazir, &amp; Carr,
1997). This may reflect the operation of some kind of filtering
mechanism during online processing, through which language
users are able to quickly reject forms that lie beyond a certain point
of acceptability, or plausibility, during the ongoing processing of
the incoming language stream.1</p>
      <p>The N400 (or N400-like responses) can also be observed in
numerous contexts involving non-linguistic but meaningful stimuli, such as
pictures (Ganis &amp; Kutas, 2003; Ganis, Kutas, &amp; Sereno, 1996; Nigam,
Hoffman, &amp; Simons, 1992; Pratarelli, 1994), faces (Barrett &amp; Rugg,
1989; Bobes, Valdés-Sosa, &amp; Olivares, 1994), environmental noises
(Chao, Nielsen-Bohlman, &amp; Knight, 1995; Van Petten &amp; Rheinfelder,
1995), movie clips (Sitnikova, Holcomb, Kiyonaga, &amp; Kuperberg,
2008; Sitnikova, Kuperberg, &amp; Holcomb, 2003) and co-speech
gestures (Kelly, Kravitz, &amp; Hopkins, 2004; Wu &amp; Coulson, 2005).</p>
      <p>Linguistically anomalous stimuli are not always associated with
an N400 response. For example, the left anterior negativity (LAN;
Friederici, 2002; Neville, Nicol, Barss, Forster, &amp; Garrett, 1991) and
P600 (Osterhout &amp; Holcomb, 1992) are well-known ERP
components that have been found in syntactic violation contexts in</p>
      <sec id="1-1">
        <title>1 This possibility is bolstered by recent work of Albert Kim and colleagues, who</title>
        <p>have found that relative to real word controls, N400 amplitude decreases and P600
amplitude increases, parametrically, as orthographic irregularity increases (Kim &amp;
Pitkänen, submitted for publication).
spoken and written language, and more recent work has shown that
these components can be elicited in the visual–manual modality as
well. For example, in a recent study Capek et al. (2009) compared
ERP responses to semantically and syntactically well-formed and
ill-formed sentences. While semantic violations elicited an N400 that
was largest over central and posterior sites, syntactic violations
elicited an anterior negativity followed by a widely distributed
P600. These findings are consistent with the idea that within written,
spoken and signed languages, semantic and syntactic processes are
mediated by non-identical brain systems (Capek et al., 2009).</p>
        <p>The present study makes use of dynamic video stimuli showing
ASL sentences completed by four classes of ending
item—semantically congruent signs, semantically incongruent signs, phonologically
legal but non-occurring pseudo-signs, and non-linguistic grooming
gestures. Based upon previous studies, we expected a gradation of
N400-like responses across conditions, with N400 effects of smaller
magnitude for semantically incongruent endings and of larger
magnitude (i.e. more negative) for phonologically legal pseudo-signs.</p>
        <p>The ERP response for the non-linguistic gesture condition is a
priori more difficult to predict. Previous neuro-imaging studies of
deaf signers have reported differences in patterns of activation
associated with the perception of signs compared to non-linguistic
gestures (Corina et al., 2007; Emmorey et al., 2010; MacSweeney
et al., 2004), but the methodologies used in those studies lacked
the temporal resolution to determine at what stage of processing
these differences may occur. While N400-like responses have been
elicited to co-speech gestural mismatches (Kelly et al., 2004; Wu &amp;
Coulson, 2005), in our study, gestures occur in place of semantically
appropriate sentence-ending items, rather than as a possible
accompaniment. It should also be borne in mind that the
relationship of signs and grooming gestures is probably not quite akin to
that between standard lexical items in spoken language and the
orthographically/phonotactically illegal pseudo-words used in
earlier ERP studies. Unlike grooming gestures, which are part of
everyday life, illegal non-words like ‘‘dkfpst’’ are probably alien to most
people’s routine experience. A better spoken-language analogue of
our grooming action condition might be something like ‘‘I like my
coffee with milk and [clearing of throat],’’ though we know of no
spoken-language studies which have incorporated such a
condition. The non-linguistic grooming gestures used in the present
study may be another example of forms that language users (in
this case, signers) are able to quickly reject as non-linguistic during
language processing. If this is the case, then one might also expect
that such forms will not elicit an N400 but rather a positive-going
component (cf. Hagoort &amp; Kutas, 1995).</p>
        <p>In summary, to the extent that semantic processing at the
sentence level is similar for signed and spoken language, despite the
obvious difference in modality, the ERP responses associated with
our four sentence ending condition should be predictable. First, the
incongruent signs should elicit a negative-going component
relative to the baseline (congruent sign) condition, consistent with
the classic N400 response seen for English and other spoken
languages, as well as some previous ERP studies of ASL (Kutas et al.,
1987; Neville et al., 1997). Second, the pseudo-signs should also
elicit a negative-going wave, and this response can be expected
to be of larger magnitude (i.e. be more negative) than that seen
for the incongruent signs. Third, while the likely response to the
grooming gesture condition is more difficult to predict, we may
expect to see a positive-going component relative to the baseline.</p>
      </sec>
    </sec>
    <sec id="2">
      <title>2. Methodology</title>
      <sec id="2-1">
        <title>2.1. Participants The 16 participants (12 female and 4 male; age range = [19, 45], mean = 25.4 and SD = 8.3) were deaf users of ASL; all were students</title>
        <p>or staff at Gallaudet University in Washington DC and received a
small payment for participating. Three were left-handed. There
were 11 native signers (i.e. born to signing parents; the remaining
five non-natives’ mean self-reported age of acquisition of ASL was
9.0 (SD = 6.3, range = [2, 16]). All subjects were uninformed as to
the purpose of the study and gave informed consent in accordance
with established Institutional Review Board procedures at
Gallaudet University.2</p>
      </sec>
      <sec id="2-2">
        <title>2.2. Stimuli and procedure</title>
        <p>During the experiment, the subject was seated in a comfortable
chair facing the computer screen approximately 85 cm away, at
which distance the 4-inch-wide video stimuli subtended an angle
of about 7 degrees. The stimuli were delivered using a program
created by the first author using Presentation software
(Neurobehavioral Systems).</p>
        <p>For each trial, the subject viewed an ASL sentence ‘‘frame’’
consisting of an entire sentence minus a last item (e.g. BOY SLEEP IN
HIS; see Fig. 1 for an illustration), followed by an ‘‘ending item’’
completing the sentence. The ending item could be one of four
types (shown, respectively, to the upper left, upper right, lower left
and lower right of the question mark in Fig. 1): a semantically
congruent sign (e.g. BED for the sentence frame just given), a
semantically incongruent sign (e.g. LEMON), a phonotactically legal but
non-occurring ‘‘pseudo-sign’’ (e.g. BARK.A, this notation indicating
that this pseudo-sign was formed by articulating the real sign
BARK with an A handshape3), or a grooming gesture such as eye
rubbing or head scratching. All stimulus items (sentence frames
and ending items) were performed by a female native signer of
ASL, who also verified that each sentence frame plus last sign item
was grammatically acceptable in ASL. The ending items for both sign
conditions (semantically congruent and incongruent) were all nouns.</p>
        <p>As is the case with spoken languages, signed languages,
including ASL, have regional variants that could potentially affect
comprehension. In the present situation, this would be relevant if
subjects encountering extant but unfamiliar sign forms as ending
items produced an ERP response similar to that for pseudo-signs.
Our experience, however, suggests that in the majority of cases,
fluent users of ASL have previously encountered regional variants.
This can be compared to the way a New England English speaker,
while not using the word the ‘‘sack’’ as part of his or her own
dialect (instead using bag), would most likely comprehend a sentence
such as ‘‘The grocer placed the vegetables in the sack’’ without
difficulty. In principle, one might expect such forms to produce
increased processing difficulties, similar to encountering low
frequency forms. However, we are cognizant of the regional
variants in ASL and in planning this study, aimed to use signs which
were unambiguous and would reflect the most frequent forms.</p>
        <p>As a check, we asked two native signers (not participants in the
main study) to scrutinize all semantically congruent and
semanti</p>
        <sec id="2-2-1">
          <title>2 A group of 10 hearing non-signers was also run on the same experiment as a</title>
          <p>control measure. All were undergraduate students at the University of California at
Davis with no substantial knowledge of sign language. Like the deaf group, these
subjects were uninformed as to the purpose of the study and gave informed consent
in accordance with established Institutional Review Board procedures at UC Davis.
However, unlike the deaf group, no significant effects or interactions related to Ending
item were found, and no further analysis related to this group will be presented here.</p>
        </sec>
        <sec id="2-2-2">
          <title>3 The pseudo-signs could be one-handed or two-handed. The two-handed variants</title>
          <p>include cases where a handshape is articulated on a base hand, as well as cases in
which the two hands move symmetrically; both of these occur in real two-handed
signs. Both the one- and two-handed pseudo-sign items appear as compositional
forms that are non-occurring in ASL, and identification of the handshape alone is not
sufficient to determine whether the sign is true sign or a pseudo-sign. The consensus
view among the signers in our group is that our pseudo-signs are more akin to legal
but non-occurring items, (e.g. ‘‘glack’’), rather than being consistently seen as
recognizable but altered lexical items (e.g. ‘‘glassu’’).</p>
          <p>Fig. 1. Still shots taken from one of the sentence frame stimuli, along with its four
possible endings (see text). Note that the actual stimuli were dynamic, not static.
cally incongruent ending signs, to mark whether they knew of any
regional variants or alternative pronunciations, and to list any such
forms. Note that without a proper sociolinguistic analysis, it is
difficult to ascertain whether such differences are indeed
sociolinguistic variants, so we were most liberal in asking for any known
alternative form. Out of 240 critical items, 49 were deemed to have
a potential regional variant (e.g., PIZZA and FOOTBALL) or
alternative pronunciations (e.g. the sign EARRINGS, which can be signed
with an F handshape, or a b0 handshape, aka ‘‘baby-O’’). Of these
potentially problematic items, we then asked whether if seen in
isolation, any would be unrecognized. Only 7 of those 49 sign
forms were deemed as potentially unrecognized. Most
importantly, none of these variants were the forms used in the actual
experiment. For example, EASTER was a sign used as a semantically
incongruent item. Both signers agreed that the form used in our
study was the most common form (two E-handshapes held near
the shoulders with a twisting motion). One of the two signers knew
of an alternative form in which the E handshape rises off of the
palm of the B-handshape base hand. The second signer noted she
would not have known what that item meant if she had seen it
in isolation. Most importantly, such forms were not used in our
study. Thus we are confident that in choosing our sign ending
items (during which we took into account intuitions and feedback
from native signers, including one of our co-authors on this paper),
we have selected highly frequent and recognizable forms likely to
be recognized by sign users, even if they do not use the same forms
themselves in each and every case (as in the bag/sack example in
English).</p>
          <p>Each sentence frame stimulus was filmed by having the signer
begin with her hands in her lap, raise her hands to sign the
sentence frame, then place her hands back in her lap. Each ending item
was also filmed in this way, beginning and ending with the signer’s
hands in her lap. Each frame was filmed just once, with no ending
item in mind, rather than creating a separate instance of each
frame for each of the four possible ending items. This was done
to provide a consistent lead-up to each ending item, eliminating
the possibility that differing coarticulatory effects or other
confounds might lead to diverging processing on the part of the signer
prior to the onset of each ending item. During video editing, the
stimuli were trimmed slightly so that the full movement of the
signer’s hands to and from her lap at the end of each sentence
frame and beginning of each ending item was not seen when the
stimuli were viewed during the running of the actual experiment.</p>
          <p>Over the course of the entire experiment, the subject saw a
sequence of 120 of these sign sentences, with 30 instances of each of
the four types of ending items. The ordering of the sentences was
randomized for each subject and the type of ending item
(semantically congruent, semantically incongruent, pseudo-sign or
gesture) shown for each sentence frame also varied among subjects.
A complete list of all 120 sentence frames with each of their
possible ending items is given in Appendix A. For most trials, no
behavioral responses were required, but in order to encourage subjects
to attend to the meaning of the sentences, an occasional
comprehension check was given (see Section 2.3).</p>
          <p>The sentence frames and ending items were separated by a
200ms blank screen, so that the slight visual discontinuity between the
two stimuli (which were filmed separately, as described earlier)
would be less jarring. In the Presentation program which delivered
the stimuli, the default color of the computer screen was set to
match the background behind the signer in the video stimuli for
color and intensity. Blink intervals of randomly varying length
between 3200 and 3700 ms were given after each trial. Longer blink
intervals lasting an additional 4 s were provided after every eight
trials. Fixation crosses appeared at key moments to remind
subjects to maintain a consistent gaze toward the center of the screen,
and to indicate when the next trial was about to begin. After every
30 trials (three times total during the experiment), open-ended
break sessions were given so that subjects could rest longer if they
desired. Before the experiment began, subjects were given a brief
practice session, six trials long, to acquaint them with the format
of the experiment. The six ASL sentences used in the practice
session were different from the sentences used in the actual
experiment.</p>
        </sec>
      </sec>
      <sec id="2-3">
        <title>2.3. Behavioral task</title>
        <p>In order to provide an objective measure that could be used
after-the-fact to verify that each subject had been paying attention
to the sentences, occasional comprehension checks appearing at
random intervals were programmed into the experiment; these
appeared after each five to eight sentences. At each comprehension
check, the subject was required to choose which of two words,
presented on-screen, was most closely related to the meaning of the
just-shown ASL sentence. For example, after the ASL sentence
beginning with the frame ‘‘BOY SLEEP IN HIS,’’ the two candidate
words were ‘‘fight’’ and ‘‘sleep,’’ with the latter being the correct
answer in this case. Two such words were chosen for each sentence
frame, and were always dependent only on the sentence frame,
never on any of the four possible ending items for that sentence
frame. The two quiz words always appeared side-by-side with
the left vs. right position on-screen being chosen at random on
each trial for the correct and incorrect word choices. The quiz
words were presented in English, but only frequent words were
used as quiz items, so that users of ASL would be unlikely to be
unfamiliar with them. All subjects’ scores were deemed sufficiently
high (mean: 97.3%, SD: 4.6%, range: [84.0%, 100%]) that no subjects
were excluded because of poor performance on the quizzes.</p>
      </sec>
      <sec id="2-4">
        <title>2.4. Electroencephalogram (EEG) recording and data analysis</title>
        <p>EEG data were recorded continuously from 32 scalp locations at
frontal, parietal, occipital, temporal and central sites, using AgCl
electrodes attached to an elastic cap (BioSemi). Vertical and
horizontal eye movements were monitored by means of two electrodes
placed above and below the left eye and two others located
adjacent to the left and right eye. All electrodes were referenced to
the average of the left and right mastoids. The EEG was digitized
online at 256 Hz, and filtered offline below 30 Hz and above
0.01 Hz. Scalp electrode impedance threshold values were set at
20 kX.</p>
        <p>Initial analysis of the EEG data was performed using the ERPLAB
plugin (Lopez-Calderon &amp; Luck, in press) for EEGLAB (Delorme &amp;
Makeig, 2004). Epochs began 200 ms before stimulus onset and
ended 1000 ms after. Inspection of subjects’ EEG data was
performed by eye to check rejections suggested by a script run in
ERPLAB whose artifact rejection thresholds were set at ±120 lV. For all
16 subjects, in each of the four sentence ending conditions at least
20 of the original 30 trials remained after the rejection procedure
just described. The statistical analyses reported below were carried
out using the SPSS statistical package.</p>
        <p>To assess the significance of the observed effects, a column
analysis was conducted (cf. Kim &amp; Osterhout, 2005) for which a
separate ANOVA was run on each of four subsets of the scalp sites,
as illustrated in Fig. 2. For the midline scalp sites, colored green in
the figure, the two factors in the ANOVA were Electrode (one level
for each of the four electrodes) and sentence Ending (semantically
congruent sign, semantically incongruent sign, pseudo-sign and
grooming gesture). The other three ANOVAs, corresponding to
the inner (colored blue in the figure), outer (purple), and outermost
(orange) sites, included a third factor of hemisphere (left or right);
in addition, for these three ANOVAs the factor Electrode had one
level for each pair of electrodes, from most anterior to most
posterior. For the N400 analysis, the dependent measure was mean
amplitude of EEG response within the window from 400 to
600 ms after stimulus onset. Because the latency of the effects
related to gesture was somewhat greater, a second column analysis
was run for the window from 600 to 800 ms after stimulus onset.
For the purposes of these column analyses, data from the two
frontmost sites (FP1 and FP2) and from two posterior sites (PO3
and PO4) were not used. In all cases, Greenhouse–Geisser
(Greenhouse &amp; Geisser, 1959) adjustments for non-sphericity were
performed where appropriate and are reflected in the reported results.</p>
        <p>In the following section, the outcomes for the ANOVAs
performed for each time window will be given in the order midline,
inner, outer, and outermost. This establishes the significance of
the results, which we first introduce with illustrations and
descriptions of the waveforms and the associated topographic maps.</p>
      </sec>
    </sec>
    <sec id="3">
      <title>3. Results</title>
      <sec id="3-1">
        <title>3.1. Waveforms and topographic maps</title>
        <p>Pictured in Figs. 3 and 4 are grand-average waveforms at
selected electrode sites for the four sentence Ending conditions.
Visual inspection of the waveforms reveals that all conditions
evoked exogenous potentials often associated with written words
(Hagoort &amp; Kutas, 1995); these include a posteriorly distributed
positivity (P1) peaking at about 100 ms post-stimulus onset,
followed by a posteriorly distributed negativity (N1) peaking at about
180 ms after target onset. Starting at approximately 300 ms after
stimulus onset, we begin to see a differentiation for different
sentence ending conditions which we will quantify in detail in the
statistical analysis. Relative to the baseline (semantically congruent
sign) condition, both the semantically incongruent and
pseudosign conditions can be seen to have elicited negative-going waves.
In addition, the pseudo-sign negativity is generally greater in
magnitude than the negativity elicited in the semantically incongruent
condition. In contrast, beginning at approximately 400 ms we
observe a large positive-going wave relative to the baseline for the
grooming gesture condition. These effects appear to be long lasting,
extending beyond the end of the 1000 ms time window.</p>
        <p>Fig. 2. Electrode groupings for the ANOVAs.</p>
        <p>Fig. 5 presents topographic maps of key contrasts, and
reinforces the patterns seen Figs. 3 and 4. The maps show mean
amplitude difference between the indicated conditions during the two
time windows we analyzed. Again, we see that the pseudo-signs
elicit a negativity that is overall more pronounced than the one
elicited by the semantically incongruent signs, both in terms of
magnitude and distribution. In contrast, the response to the
gesture condition starts to diverge clearly from the others in the
earlier time window, starting at posterior sites and then spreading
more generally.</p>
        <p>We now continue with a presentation of the statistical results.
For simplicity of presentation, only main effects and interactions
related to sentence Ending are discussed in the text, but complete
ANOVA results are given in Table 1. Our analyses confirm the very
consistent patterning of mean ERP response with respect to
sentence Ending that was seen in Figs. 3–5 and noted in the foregoing
discussion. Relative to the baseline (semantically congruent sign)
condition, the incongruent signs and the pseudo-signs elicited a
negative-going wave, while the grooming gestures elicited a large
positivity; also, the negative-going component elicited by the
pseudo-signs was overall larger than the one elicited by the
incongruent signs. This general pattern was observed for almost all scalp
sites; the relatively minor exceptions are noted below.</p>
      </sec>
      <sec id="3-2">
        <title>3.2. Window from 400 to 600 ms</title>
        <p>For the earlier of the two time windows, the midline ANOVA
found a main effect of Ending (p &lt; 0.001) and an interaction of
Ending by Electrode (p &lt; 0.001). Mean amplitude for Ending showed
the predicted pattern among condition means, with respective
means for pseudo-signs, incongruent signs, congruent signs and
grooming gestures equal to 4.95, 4.20, 3.36 and 1.16 lV.
The gesture condition mean differed significantly from the rest
(p &lt; 0.05 for gesture vs. baseline, p’s &lt; 0.001 for the other two
comparisons); the other differences did not reach significance. It was at
the frontmost sites that the baseline differed the most from the
incongruent and pseudo-sign conditions (p &lt; 0.05 and p = 0.062,
respectively). At the vertex electrode CZ, a departure from the
predicted progression of condition means was found; incongruent
signs showed the most negative amplitude here, which however
was not significantly different from the amplitudes for
pseudosigns or congruent signs. At the other three midline electrode sites,
the familiar pattern among the four Ending conditions was seen.</p>
        <p>The inner-electrode ANOVA found a main effect of Ending
(p &lt; 0.001) and a marginal Ending by Electrode interaction
(p = 0.094). Mean amplitudes for the four conditions progressed
in the predicted way; for pseudo-signs, incongruent signs,
congruent signs and grooming gestures, respective means were 4.54,
3.57, 2.56 and 0.302. Pairwise differences between means
were all significant, except for incongruent signs vs. congruent
signs (marginally significant at p = 0.088) and pseudo-signs vs.
incongruent signs (p = 0.24). The same ordering from most
negative to most positive means for the four conditions was seen at
all three levels of electrode. The most anterior sites showed the
greatest differences between the baseline condition mean and
the incongruent and pseudo-sign means; at these sites, both
pairwise differences were significant.</p>
        <p>For the outer electrodes ANOVA, the results included a main
effect of Ending (p &lt; 0.001). The respective overall mean amplitudes
for the pseudo-sign, incongruent, congruent and gesture
Fig. 3. Grand-average waveforms at (from top to bottom) frontal, central, parietal
and occipital sites. Units on the vertical axis are microvolts; those on the horizontal
axis are milliseconds. Negative is plotted downward.</p>
        <p>Fig. 4. Grand-average waveforms at the OZ site. Units on the vertical axis are
microvolts; successive tick marks on the horizontal axis are 200 ms apart. Negative
is plotted downward.
conditions were 2.93, 1.97, 1.20 and 0.345, consistent with
the predicted pattern. The highly significant main effect of Ending
here is due to the fact that the pairwise differences between
conditions were either significant or showed near-significant trends
(cases of the latter were: for pseudo-sign vs. incongruent sign,
p = 0.11; for incongruent sign vs. congruent sign, p = 0.14, for
gesture vs. congruent sign, p = 0.062). The absence of a significant
Ending by Electrode interaction is due to the fact that the predicted
pattern of pseudo-sign &lt; incongruent sign &lt; congruent sign &lt;
gesture was seen for all four levels of Electrode.</p>
        <p>Finally, the ANOVA for the outermost set of electrodes found a
significant main effect of sentence Ending (p &lt; 0.001) and an
interaction of Ending by Electrode (p &lt; 0.05). The main effect of Ending
reflects the pattern already seen repeatedly; for the outermost
sites, mean EEG amplitudes were 2.45, 1.34, 1.03 and
0.083 lV, respectively, for the pseudo-signs, incongruent signs,
congruent signs and grooming gestures. Follow-up comparisons
showed that all of the pairwise differences among conditions were
significant, except for the congruent sign vs. incongruent sign
comparison (p = 0.39), and the incongruent sign vs. pseudo-sign
comparison, which was however marginally significant (p = 0.078).
The Ending by Electrode interaction reflects two exceptions to
the predicted pattern for the four Ending conditions. First, at the
two most posterior sites, the amplitude for the incongruent sign
condition was slightly greater than for the congruent sign
condition (this difference, however, did not approach significance;
p = 0.62). Second, at the anterior electrode sites the gesture
condition was not associated with significantly greater positivity than
the other three conditions.</p>
      </sec>
      <sec id="3-3">
        <title>3.3. Window from 600 to 800 ms</title>
        <p>For the later time window, the midline-electrode ANOVA found
a main effect of Ending (p &lt; 0.001) and an interaction of Ending by
Electrode (p &lt; 0.001). Mean amplitude for the four Ending
conditions diverged slightly here from the familiar pattern, with
respective means for pseudo-signs, incongruent signs, congruent signs
and grooming gestures equal to 2.43, 2.47, 1.33 and 3.97.
The comparison of pseudo-sign vs. incongruent sign did not
approach significance however (p = 0.96), nor did the comparison of
pseudo-sign vs. congruent sign (p = 0.31). The congruent vs.
incongruent sign comparison was marginally significant at p = 0.063.
The gesture means were again very different from the rest (all
three p’s &lt; 0.001).</p>
        <p>The inner-electrode ANOVA found a main effect of Ending
(p &lt; 0.001) and an Ending by Electrode interaction (p &lt; 0.001).
Mean amplitudes for the four conditions progressed in the
predicted way, with the gesture means again very different from the
rest (all three p’s &lt; 0.001). The baseline vs. incongruent comparison
also reached significance (p &lt; 0.05), and the baseline vs.
pseudosign comparison showed a near-significant trend (p = 0.13). The
same ordering from most negative to most positive means for
the four conditions was seen at all three levels of electrode
(respective mean values for pseudo-sign, incongruent sign, congruent
sign, grooming gesture = 1.86, 1.60, 0.43, 5.05), with gesture
means being most divergent from the other three condition means
at posterior sites.</p>
        <p>For the outer electrodes ANOVA, the results included a main
effect of Ending (p &lt; 0.001) and an interaction of Ending by Electrode
(p &lt; 0.05). The familiar pattern of pseudo-sign &lt; incongruent
sign &lt; congruent sign &lt; grooming gesture was seen for all four
levels of Electrode (mean values in the usual order = 1.60, 0.85,
0.21, 3.35), but only the only differences reaching significance
were for gesture vs. the other conditions (all three p’s &lt; 0.001).
The baseline vs. pseudo-sign difference showed a near-significant
trend (p = 0.12). Again, gesture means showed the greatest
differences from the other three condition means at posterior electrode
sites.</p>
        <p>Finally, the ANOVA for the outermost set of electrodes found a
highly significant effect of sentence Ending (p &lt; 0.001), as well as
***
*
*
***
***
*
**
Fig. 5. Grand-average topographic maps (viewed from above, with anterior
oriented upward) for key contrasts between sentence Ending conditions over the
two indicated time intervals. Units on the scale are microvolts.
an interaction of Ending by Electrode (p &lt; 0.001). The predicted
pattern among condition means was seen again here (mean values
in same order as before: 1.63, 0.66, 0.39, 2.12). For this later
window, however, only the grooming gesture mean was
significantly different from the rest (all three p’s &lt; 0.001), but the
baseline vs. pseudo-sign comparison was marginally significant
(p = 0.085). The Ending by Electrode interaction is due to the fact
that the differences between the gesture mean and the means
associated with the other three conditions were greatest at
posterior sites.</p>
      </sec>
      <sec id="3-4">
        <title>3.4. Earlier time windows</title>
        <p>ANOVAs like those just described were also carried out for the
following time windows: from 0 to 100 ms post-stimulus onset,
100–200 ms, 200–300 ms, and 300–400 ms. For each of these
ANOVAs there was an absence of significant effects or interactions
related to sentence Ending. An additional post hoc test was carried
out in order to investigate specifically the negative-going trend for
the pseudo-sign condition relative to baseline at the OZ site in the
200–400 ms window, seen most clearly in Fig. 4. However, this
contrast was found to be only marginally significant in that time
window (p = 0.085).</p>
      </sec>
    </sec>
    <sec id="4">
      <title>4. General discussion</title>
      <p>This study investigated sign language users’ ERP responses
when confronted with ASL sentences with four kinds of endings:
semantically congruent signs, semantically incongruent signs,
phonologically legal pseudo-signs and non-linguistic grooming
gestures. We hypothesized that the neurophysiological response
associated with these ending types would differ in a manner
consistent with findings in analogous studies of spoken and written
language. Specifically, we predicted that the incongruent signs
would elicit negative-going waves relative to the baseline
(congruent sign) condition, and that the pseudo-signs would elicit a
negativity of larger magnitude than the incongruent sings. In addition,
we suggested that the non-linguistic gestures might elicit a
positive-going wave, relative to baseline. While this last prediction
was more speculative than the others, the expected pattern was
in fact observed very consistently in our analysis of the data.
Together, these findings lead to a number of important observations.</p>
      <p>First, the outcome of our incongruent sign vs. congruent sign
comparison replicates earlier findings which have also indicated
that the N400 generalizes across modalities, including the visual–
manual modality of signed language (e.g. Capek et al., 2009; Kutas
et al., 1987). These findings are broadly consistent with other
studies using a variety of methodologies including positron emission
tomography (PET; Corina, San Jose-Robertson, Guillemin, High, &amp;
Braun, 2003), functional magnetic resonance imaging (fMRI;
Neville et al., 1998), and cortical stimulation mapping (Corina et al.,
1999), highlighting key neural processing similarities between
signed and spoken language, in spite of the obvious physical
differences in the linguistic signal.</p>
      <p>For instance, Neville et al. (1997) also found that deaf signers
exhibited an N400 response to semantically incongruent ASL
sentences, relative to congruent sentences. Like the effects in the
present study, this response was broadly distributed and had an
onset and peak that the researchers noted was somewhat later
than would be expected for written language, but consistent with
earlier studies on auditory language (Holcomb &amp; Neville, 1990,
1991). Neville et al. suggested that this delay might be due to the
fact that the recognition point of different signs will tend to vary
more than for printed language, in which all information is made
available at the same time. Capek et al. (2009) also found a
relatively late N400 response to semantic incongruity in sign
sentences; this bilateral and posteriorly prominent effect had an
onset of about 300 ms post-stimulus onset and peaked at about
600 ms post-stimulus onset, very much like the negativities we
have described in the present study.</p>
      <p>These effects are somewhat different from those that have been
described in studies incorporating incongruent co-speech gestures
and other sorts of non-linguistic imagery like drawings,
photographs and videos. In Wu and Coulson’s (2005) study of
contextually incongruent gestures, a component described by the
researchers as a ‘‘gesture N450’’ was observed. Wu and Coulson
noted the similarity of this effect to the N450 reported by Barrett
and Rugg (1990) for second items in unrelated picture pairs
relative to related picture pairs (e.g. wrench/fork vs. knife/fork), stating
(p. 659) that consistent with their own findings, ‘‘most such
‘picture’ ERP studies report a broadly distributed negativity largest
at frontal electrode sites and not evident at occipital sites (Barrett
&amp; Rugg, 1990; Holcomb &amp; McPherson, 1994; McPherson &amp;
Holcomb, 1999; Sitnikova et al., 2003; West &amp; Holcomb, 2002).’’ In
contrast, the negativity reported in the present study was quite
evident at occipital sites, as can be seen clearly in Figs. 3 and 4.</p>
      <p>A second notable finding in our study concerns deaf subjects’
ERP response in the phonologically legal pseudo-sign condition,
which was also consistent with an N400 response but was
generally larger (more negative) than the negativity seen for
semantically incongruent but fully lexical signs. This provides further
evidence for broad processing similarities for different linguistic
modalities, in the light of similar findings for pseudo-words in
earlier studies (Bentin, 1987; Bentin et al., 1985; Hagoort &amp; Kutas,
1995). It is interesting, however, that phonologically legal
pseudo-signs did not more strongly differentiate from the semantically
incongruent signs in the present study. This may be an indication
that our pseudo-signs (or some of their sub-lexical components)
are activating lexical representations to a substantial degree (cf.
Friedrich, Eulitz, &amp; Lahiri, 2006), and that at the same time, these
representations are incongruent with the sentential contexts in
which they have been presented. The pseudo-sign and incongruent
sign conditions shared another similarity in that the effects they
elicited were very prominent at occipital sites, which differs from
what has traditionally been observed in studies of word
processing. Whether this is a reflection of the modality of expression or
other experimental factors must await further study, though
results of Corina et al. (2007), discussed below, may offer some
insights about useful directions such research might take.</p>
      <p>A third set of findings, concerning the outcome related to our
non-linguistic grooming actions, is especially provocative. In
contrast to the three other kinds of sentence-final items, all of which
could be considered linguistic (i.e. as actual lexical items in two
cases, and phonologically legal lexical gaps in the third), the
non-linguistic grooming actions elicited a large positivity. As noted earlier,
phonologically illegal words in ERP studies have in some cases
elicited a positive-going component rather than an N400 (Holcomb &amp;
Neville, 1990; Ziegler et al., 1997). Holcomb and Neville (1990)
examined differences between pseudo-words and non-words in
the visual and auditory modalities in the context of a lexical decision
experiment. Pseudo-words accorded with phonotactic constraints
of English; visually presented non-words were composed of
consonant strings and auditory non-words were words played backwards.
The researchers reported that within an early time window (150–
300 ms), auditory non-words (but not visual non-words) elicited a
more negative response than pseudo-words, but only at anterior
and right hemisphere sites. In a later time window (300–500 ms),
the response to non-words was more positive for both modalities,
and like the positivity seen in the present study, this positivity was
long-lasting, continuing past the 1000 ms time-point.</p>
      <p>Ziegler et al. (1997) examined the effects of task constraints on
the processing of visually presented words, pseudo-words and
non-words. In a letter search task, following a post-stimulus
N1P2 complex, the researchers reported a negative component,
peaking around 350 ms, which was larger for words and pseudo-words
than for non-words. A late positive component (LPC) was then
generated that appeared to be slightly larger for non-words than for
words or pseudo-words. In a second experiment, in which subjects’
responses to the three types of stimuli were delayed, the ERP
response in the 300–500 ms window was more positive to
nonwords than to words and pseudo-words; responses for words
and pseudo-words did not significantly differ. In a final experiment
which required a semantic categorization of the target, a negative
component with a peak around 400 ms was elicited in response to
words and pseudo-words. In contrast, a striking late positive
component was observed in response to non-words; this lasted from
300 ms to the end of the recording period.</p>
      <p>Thus, across multiple studies we see that illegal non-words,
relative to pseudo-words and real words, appear to elicit a large late
positivity. This positivity has sometimes been interpreted as a
P300 response (e.g. Holcomb &amp; Neville, 1990). In the present
experiment, the centro-parietal distribution of the positive component
elicited in the gesture condition also corresponds to the typical
distribution of the P300 component. At least three interpretations of
this effect may be relevant here. First, a P300 response is
well-attested in studies making use of stimuli perceived by subjects to
be in a low-probability category (e.g. Johnson &amp; Donchin, 1980).
In our experiment, grooming gestures occurred 1/4 of the time,
rendering these non-linguistic events low-probability with respect
to the other three (linguistic) sentence ending conditions. Second,
ERP differences between non-words and words have been
attributed to the fact that these non-linguistic items have little or
nothing in common with lexical entries and therefore do not generate
lexical activity (cf. Rugg &amp; Nagy, 1987). Third, the ERP differences
observed between pseudo-words and non-words have also been
suggested to reflect a pre-lexical filtering process that quickly
rejects non-linguistic items based upon aberrant physical
characteristics (Holcomb &amp; Neville, 1990). For example, Ziegler et al. (1997)
suggest such a categorization may be based on a spelling check in
the case of non-word consonant-string stimuli.</p>
      <p>This last interpretation accords well with a possibility we noted
in the Introduction, that such an ERP response may be due to the
operation of a filtering/rejection mechanism, allowing language
users to efficiently reject items in the incoming linguistic signal
that do not fall within some limits of linguistic acceptability. The
gesture stimuli in the present study, in lacking the semantic
appropriateness of semantically congruent signs, the lexicality of
incongruent signs, and even the phonological legality of pseudo-signs,
apparently fail to reach some ‘‘acceptability threshold,’’ causing
them to be rejected and thus dealt with during processing in a
qualitatively different way. This hypothesis also permits us to
predict an answer to an interesting related question: what kind of ERP
response would be elicited by phonologically illegal non-signs in
sentence contexts like those explored in the present study? The
positivities seen in studies incorporating non-words; the
crossmodality parallels we have already noted in ERP studies of spoken,
written and signed language; as well as the positive waveforms
seen in response to the non-linguistic gesture stimuli in the
present study; all support the prediction that phonologically illegal
non-signs would elicit a positive-going waveform. However,
confirmation of this must await future research.</p>
      <p>We have already alluded to the growing number of studies
which have used ERP methodology to examine the contributions
of co-speech manual gestures to the interpretation of both
linguistic and non-linguistic stimuli (Holle &amp; Gunter, 2007; Kelly et al.,
2004; Ozyürek, Willems, Kita, &amp; Hagoort, 2007; Wu &amp; Coulson,
2005, 2007a, 2007b). Many of these studies have used iconic
manual gestures that depict a salient visual–spatial property of
concrete objects, such as their size and shape or an associated
manner of movement (but see also Cornejo et al., 2009).
Collectively these studies suggest that co-speech manual gestures
influence semantic representations, and that discrepancies between
gestural forms and the semantic contexts in which they occur lead
to greater processing costs on the part of language perceivers. This
in turn results in increased negativities in the time window often
associated with the classic N400 effect, observed in response to
word meanings that violate the wider semantic context (Kutas &amp;
Hillyard, 1980).</p>
      <p>For example, Kelly et al. (2004) observed modulation of ERP
responses for speech tokens that were either accompanied by
matching, complementary or mismatched hand gestures. An N400-like
component was observed for mismatched gesture-speech tokens
relative to the other conditions. Wu and Coulson (2005) examined
ERPs for subjects who watched cartoons followed by a gestural
depiction that either matched or mismatched the events shown
in the cartoons. Gestures elicited an N400-like component (a
socalled ‘‘gesture N450’’) that was larger for incongruent than
congruent items. Ozyürek et al. (2007) recorded EEG while subjects
listened to sentences with a critical verb (e.g. ‘‘knock’’) accompanied
by a related co-speech gesture (e.g. KNOCK). Verbal/gestural
semantic content either matched or mismatched the earlier part
of the sentence. The researchers noted that following the N1–P2
complex, the ERP response to mismatch conditions started to
deviate from the response to the correct condition in the latency
window of the P2 component, around 225–275 ms post stimulus
onset, while at around 350 ms, the mismatch conditions deviated
from the congruent condition. This was followed by a similar effect
with a peak latency somewhat later than the one usually seen for
the N400. These data were taken as evidence that that the brain
integrates both speech and gestural information simultaneously.
However, it is interesting to note that double violations (speech
and gesture) did not produce additive effects, suggesting parallel
integration of speech and gesture in this context.</p>
      <p>In contrast, the grooming gesture condition in the present study
was not associated with any N400-like effects. We suggest that this
is due to the fact that subjects were unlikely to perceive these
actions as being akin to co-speech (or co-sign) gestures, but instead
as something qualitatively different. This detection process
evidently occurred quite rapidly during online processing of these
stimuli, comparable to the speed at which semantic processing
was carried out for the linguistic stimuli. The findings of a PET
study by Corina et al. (2007) may shed some additional light on
this outcome. In that study, deaf signers were found to have
engaged different brain regions when processing ASL signs and
selfgrooming gestures, in contrast with the hearing non-signers who
also took part in the study. Specifically, deaf signers engaged
lefthemisphere perisylvian language areas when processing ASL sign
forms, but recruited middle-occipital temporal–ventral regions
when processing self-grooming actions. The latter areas are known
to be involved in the detection of human bodies, faces, and
movements. The present findings add temporal precision the results of
that study, enabling us to determine when such information is
rejected as non-linguistic during the course of ASL sentence
processing.</p>
      <p>The findings of Corina et al. (2007) may also speak to the fact,
noted earlier, that the N400 effects observed in the present study
were more prominent at occipital sites than the N400 effects
typically seen in analogous speech studies. The effects seen in the
present study’s gesture condition were strongest in posterior areas
as well, though in both cases, one must be cautious in making a
connection between the scalp topography of ERP effects and
location of their source.4</p>
      <p>Finally, an alternative interpretation of the positivity seen in the
grooming gesture condition in the present study is that it is due to
subjects’ interpreting these non-linguistic final items as missing
information, i.e. as the absence of a final item, rather than a final
item which is present yet enigmatic. While this cannot be entirely
ruled out, it should be noted that the pseudo-signs could also
potentially be considered ‘‘semantically void’’ items, but the
responses to the pseudo-signs (which have a discernible linguistic
structure consistent with that of real ASL lexical items) and the
gestures (which do not) were qualitatively different. Relative to
the baseline condition, no significant positivity was seen in any
time window for the pseudo-signs, and no significant negativity
was seen in any time window for the grooming gestures.</p>
    </sec>
    <sec id="5">
      <title>5. Conclusion</title>
      <p>To the best of our knowledge, this is the first ERP study of sign
language users that investigates sentential processing in such a
wide a range of lexical/semantic contexts. Consistent with previous
research on both spoken and signed language, we found that ASL
sentences ending with semantically incongruent signs were
associated with significant N400-like responses relative to the baseline
condition, in which sentences ended with semantically congruent
signs. Furthermore, we found that phonologically legal
pseudosign sentence endings elicited an N400-like effect that was
somewhat stronger than the response to the semantically incongruent
signs; this is consistent with existing work on spoken language,
but represents a new finding for signed language. In contrast to
the incongruent sign and pseudo-sign conditions, grooming
actions elicited a very large positive-going wave; this is also a new
finding, and complements earlier work on spoken language. The
fact that our results largely parallel those seen in analogous ERP
studies of spoken language constitutes strong evidence that
highlevel linguistic processing shows remarkable consistency across
modalities. Moreover, our results offer important new information
about the relationship between sign and action processing,
particularly the topography and timing of the processes that are
involved.</p>
      <p>We thank the staff and students at Gallaudet University for
helping make this study possible, and Kearnan Welch and Deborah
Williams for their assistance in data collection. We also thank two
anonymous reviewers for valuable feedback concerning the
presentation of our results. This work was supported in part by Grant
NIH-NIDCD 2ROI-DC03099-11, awarded to David Corina.
Appendix A. List of stimulus items</p>
      <p>Table A1 lists all 120 sentence frames and three of the
corresponding endings for each sentence: the semantically congruent
signs, semantically incongruent signs and pseudo-signs. In the
fourth class of endings, the gesture stimuli, the sign performer
was seen making brief grooming actions such as head scratching,
eye rubbing or passing her fingers through her hair. To create</p>
      <sec id="5-1">
        <title>4 It should also be noted that the effects seen in the present study were for gestures</title>
        <p>in a sentence context, while in Corina et al. (2007) the sign and gesture stimuli were
seen in isolation.
120 unique gesture stimuli, the actions were performed with
differences in the number and configuration of hands or fingers used,
the location of the body involved, and so on. Also shown in the
rightmost two columns of the table are the ‘‘correct’’ and
‘‘incorrect’’ word choices for the occasional quiz items. A number sign
(#) preceding an item means that item was fingerspelled. Many
of these sentences were adapted from the English-language stimuli
used in Johnson and Hamm (2000).</p>
      </sec>
      <sec id="5-2">
        <title>7 PRO3 BECOME-ILL SICK CAN’T GO-TO 8 MY HOUSE LIGHTS BLACKOUT-POW CL:set-up-around 9 DOG ANGRY CHASE 10 #PATIENT SIT-HABITUAL ANALYZE-PRO1, PRO3</title>
        <p>11 BOY STOLE
12 KIDS PRO3 CL:go-out-in-group WATCH
13 DAUGHTER MY LIKE READ
14 PLUMBER HIS JOB FIX
15 PRIEST WORK THERE
16 STUDENTS WRITE-TOOK
17 POLICEMAN PRO CATCH
18 MOTHER HAVE 3
19 DARK ROOM PRO FOR-FOR DEVELOP
20 HUNTER SHOOT-AT KILL
21 AIRPLANE SEAT FULL CL:mass 300
22 PIRATE PRO3 HUNT WHERE
23 GOLFER STROKE CL:ball-fly-across WRONG CL:ball-into-water
24 SPRING THIS YEAR MINE TAX HEAVY
25 KING-PRO FALL-IN-LOVE
26 TELESCOPE I TELESCOPE-FOCUS SEE
27 #APOLLO ROCKET-MAN GO-TO TOUCH
28 FARMER NOW CL:milk (verb)
29 SCIENTIST INVENT++ HARD
30 MURDERER PRO3 CAUGHT PUT-INTO
31 FATHER COMMAND SON GO CLEAN
32 NEW YORK TIMES NEWSPAPER ITS TENDENCY I READ MANY
33 JUDGE PRO3 ARGUMENT LISTEN LISTEN THINK-IT-OVER READY</p>
        <p>MAKE
34 MY BIRTHDAY SOON COME MY MOM PRO3 BAKE
35 #ZOO ITS-TENDENCY HAVE MANY VARIOUS
36 MEN PRO3 CL:group-up GO-OUT CHUG
37 I JOIN ARMY I SHOPPING CLOTHES NEED SHIRT PANTS
38 TEA DRINK TASTE BITTER NEED
39 PANTS CL:pull-on CL:fit-loose NEED
40 I CALL HOTEL RESERVE
41 WOMAN CL:lie-down SUNNING THERE
42 MUSEUM I LOOK-AT HALLWAY LOOK-AT WOW BEAUTIFUL
43 #OFFICE MAX I ENTER SHOP-AROUND PRO #FAX PRO COMPUTER</p>
        <p>PRO
44 MORNING BOY PRO CL:get-on-bike RIDE-BIKE CL:deliver
45 MY LIVING ROOM EMPTY NONE
46 MONEY FATHER GAVE-ME I PUT-IN WHERE
47 POPCORN I MAKE FINISH I CL:pour-over
48 I THIRST-FOR WATER NEED
49 ME ENTER BDRM I SPOT MOUSE CL:sneak-under
50 FROG CL:tongue-stick-out-retract GULP
51 GRASS CL:thereabout I WALK CL:walk-around WET OH
52 GIRLFRIEND GO FURNITURE STORE BUY BRING
53 BIRD EAT SLEEP WHERE
54 WOMAN PRO3 TOP ATHLETE PARTICIPATE
55 COOK PRO3 EXPERT THEIRS COOKING
56 I ENTER HOUSE I HEAR TICK-TICK AHA! PRO3
57 VW BUS I BUY DRIVE WRONG BROKEDOWN
58 OUTSIDE GIRL CL:lie-down OBSERVE
59 LAWYER CL:sit-down-with-someone DISCUSS WITH PRO
60 I INFORM SISTER PLEASE PACK BRING
61 MAIL/LTR I GOT OPEN-ENVELOPE I GOT
62 MAN PRO3 I SEE PRO3 FIX FINISH CONSTRUCT
63 TARA PRO WANT MAKE PIE NEED BUY
Table A1 (continued)
64 THIS WKND I GO HIKE I LOOK-AT WOW BEAUTIFUL MOUNTAIN
65 HOMEWORK I WRITE CL:glass-breaking I LOOK-AT WINDOW
66 PLANT CL:branching I LOOK PRO3 OH SEED
67 MOM CL:walk-by CL:pick-up BABY
68 BUILDING PRO3 CRUMBLE OH SEEM BOMB
69 SCHOOL THERE BOY TROUBLE MUST GO-TO PRINCIPAL
70 COLLEGE I ENTER AUDIENCE CL:sit-down WATCH GOOD LECTURE
71 GIRL PRO3 THIRSTY WANT DRINK WATER
72 I HUNGRY WANT EAT I GO-TO RESTAURANT
73 CAR CL:car-stalls RAN-OUT GAS
74 #STEAK MAN EAT TASTE-FUNNY NEED SEASON SALT PEPPER
75 MOVIE DIRECTOR MAKE MOVIE WANT SEEK INTERVIEW GOOD ACTOR(S)
76 DR PRO3 SELF PLASTIC SURGEON HIS SPECIALTY BREASTS
77 STAMP I SICK-OF ANNUAL INCREASE PRICE
78 BOY SICK I LOOK CHECK OH HURT THROAT
79 COURT I GO-TO CL:sit-down FACE JUDGE
80 DOWNTOWN LAUNDROMAT I GO ARRIVE I ENTER OH I NEED COINS
81 SCHOOL CLOSED NEXT WEEK STUDENTS GO HOME (lowered HOLIDAY
eyebrows)
82 RESTAURANT PRO3 FIRST TIME I ENTER ME FEAST WHOA FOOD</p>
        <p>DELICIOUS
83 PRO WOMAN SHORT THIN PRO3 EXPERT PRO3 CL:drink-shot WHISKEY
84 OUTSIDE THERE WEATHER BAD THERE TORNADO PRO3 HIT++ SCHOOL</p>
        <p>DESTROY++
85 BOY PRO3 NOT-WANT GO SCHOOL PRO3 MOM SAY GO SCHOOL TEST</p>
        <p>MUST TAKE
86 A-LONG-TIME-AGO STREET CL:flat-surface BUMPY-SURFACE BRICK
87 GIRL PRO CL:foot-limping SHOE CL:shoe-taken-off OH STONE
88 NEW HOME I MOVE-IN OH EMPTY NEED FURNITURE
89 DOWNTOWN MOVIE THERE I WATCH ANNOYED PEOPLE PRO3 AUDIENCE</p>
        <p>RUDE CHAT++ IN
90 LAST_NIGHT I FEEL-LIKE SIT WATCH FIRE WRONG GONE WOOD
91 THIS MORNING CHILDREN I GO-TO PARK I LOOK-AROUND OH SWING/SLIDE</p>
        <p>NONE
92 FATHER GONE 2 MONTHS CAME HOME I LOOK-STUNNED GROW BEARD
93 KNOW-THAT SMOKING MANY YEARS CAUSE (nod) CANCER
94 RIVER I STAND LOOK-ACROSS OH MUST BOAT
95 BICYCLE TIRE CL:flat NEED PUMP
96 CAR CL:gone-by-fast SPEED WRONG CL:pull-over GET TICKET
97 THIS SUMMER VACATION I WANT TRAVEL OVER-THERE EUROPE
98 X-MAS GIFT I WRAP I LOOK-AROUND NONE TAPE
99 DAUGHTER PRO BECOME-SICK I TAP COME I GIVE MEDICINE
100 WATER DRINK TOO-WARM PRO CL:cup MUST PUT-IN ICE
101 RESTAURANT PRO FANCY ENTER WANT MUST DRESS-UP DRESS TIE</p>
        <p>COAT
102 BATHROOM I ENTER MIRROR I LOOK-AT PUZZLED DIRTY FACE
103 I TYPE++ ALL-NIGHT WRONG EYES CL:eyes-fuzzy FROM COMPUTER
104 CHILDREN PRO3 WANT ACTIVITY DIFF PRO3 WANT PAINTING PRO3 BASEBALL</p>
        <p>WANT BASKETBALL PRO3 WANT
105 DAUGHTER PRO GO-TO DR TODAY NEED PRO3 WRONG I SICK</p>
        <p>CANCEL
106 RUN EVERY-MORNING RELISH I NOW MY FEET HURT BUY NEW SHOES
107 GRAD PARTY COMING-UP-SOON I NEED SHOP THINGS FOOD CAKE CHAMPAGNE
108 #HILTON SELF FANCY (nod) HOTEL
109 MY SON PRO I INFORM HIM TONITE EAT FINISH PRO3 MUST GO DISHES</p>
        <p>WASH
110 TONITE I DO-ERRANDS MUST I COOK, VACUUM, CLEAN FLOOR
111 I LOOK-AROUND NOTICE++ CHILDREN NOWADAYS INCREASINGLY GLASSES</p>
        <p>NEED
112 EVERY-FRI NITE MY FAMILY LIKES WATCH BASEBALL GAME
113 BUGS CL:hovering CL:biting-me I LOOK OH MOSQUITO
114 GIRL PRO HER TOOTH BROKE GO SEE DENTIST
115 RESTAURANT THERE ITALIAN ITS FOOD DELICIOUS PIZZA RAVIOLI SPAGHETTI
116 KNOW-THAT JULY MONTH ITS-TENDENCY HOT MONTH
117 APPLICATION YOU FILL-OUT FINISH SIGN (nod) NAME
118 BUY BOOK NOT-NECESSARY SIMPLY GO-TO LIBRARY
119 EVERY-MORNING MY FAMILY EAT EGGS #HASH BROWN BACON
120 SQUIRREL ITS FOOD BOX EMPTIED-OUT I LOOK-THERE OH RAN- NUT
OUT</p>
      </sec>
    </sec>
  </body>
  <back>
    <ref-list>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Arbib</surname>
            ,
            <given-names>M. A.</given-names>
          </string-name>
          (
          <year>2005</year>
          ).
          <article-title>Interweaving protosign and protospeech: Further developments beyond the mirror</article-title>
          .
          <source>Interaction Studies: Social Behaviour and Communication in Biological and Artificial Systems</source>
          ,
          <volume>6</volume>
          ,
          <fpage>145</fpage>
          -
          <lpage>171</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Arbib</surname>
            ,
            <given-names>M. A.</given-names>
          </string-name>
          (
          <year>2008</year>
          ).
          <article-title>From grasp to language: Embodied concepts and the challenge of abstraction</article-title>
          .
          <source>Journal of Physiology-Paris</source>
          ,
          <volume>102</volume>
          ,
          <fpage>4</fpage>
          -
          <lpage>20</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Barrett</surname>
            ,
            <given-names>S. E.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Rugg</surname>
            ,
            <given-names>M. D.</given-names>
          </string-name>
          (
          <year>1989</year>
          ).
          <article-title>Event-related potentials and the semantic matching of faces</article-title>
          .
          <source>Neuropsychologia</source>
          ,
          <volume>27</volume>
          ,
          <fpage>913</fpage>
          -
          <lpage>922</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Barrett</surname>
            ,
            <given-names>S. E.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Rugg</surname>
            ,
            <given-names>M. D.</given-names>
          </string-name>
          (
          <year>1990</year>
          ).
          <article-title>Event-related potentials and the semantic matching of pictures</article-title>
          .
          <source>Brain and Cognition</source>
          ,
          <volume>14</volume>
          ,
          <fpage>201</fpage>
          -
          <lpage>212</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Bentin</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          (
          <year>1987</year>
          ).
          <article-title>Event-related potentials, semantic processes, and expectancy factors in word recognition</article-title>
          .
          <source>Brain and Language</source>
          ,
          <volume>31</volume>
          ,
          <fpage>308</fpage>
          -
          <lpage>327</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Bentin</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>McCarthy</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Wood</surname>
            ,
            <given-names>C. C.</given-names>
          </string-name>
          (
          <year>1985</year>
          ).
          <article-title>Event-related potentials, lexical decision, and semantic priming</article-title>
          .
          <source>Electroencephalography &amp; Clinical Neurophysiology</source>
          ,
          <volume>60</volume>
          ,
          <fpage>353</fpage>
          -
          <lpage>355</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Bobes</surname>
            ,
            <given-names>M. A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Valdés-Sosa</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Olivares</surname>
            ,
            <given-names>E.</given-names>
          </string-name>
          (
          <year>1994</year>
          ).
          <article-title>An ERP study of expectancy violation in face perception</article-title>
          .
          <source>Brain and Cognition</source>
          ,
          <volume>26</volume>
          ,
          <fpage>1</fpage>
          -
          <lpage>22</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Brown</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Hagoort</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          (
          <year>1993</year>
          ).
          <article-title>The processing nature of the N400: Evidence from masked priming</article-title>
          .
          <source>Journal of Cognitive Neuroscience</source>
          ,
          <volume>5</volume>
          ,
          <fpage>34</fpage>
          -
          <lpage>44</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Capek</surname>
            ,
            <given-names>C. M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Grossi</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Newman</surname>
            ,
            <given-names>A. J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>McBurney</surname>
            ,
            <given-names>S. L.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Corina</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Roeder</surname>
            ,
            <given-names>B.</given-names>
          </string-name>
          , et al. (
          <year>2009</year>
          ).
          <article-title>Brain systems mediating semantic and syntactic processing in deaf native signers: Biological invariance and modality specificity</article-title>
          .
          <source>Proceedings of the National Academy of Sciences of the United States of America</source>
          ,
          <volume>106</volume>
          ,
          <fpage>8784</fpage>
          -
          <lpage>8789</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Chao</surname>
            ,
            <given-names>L. L.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Nielsen-Bohlman</surname>
            ,
            <given-names>L.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Knight</surname>
            ,
            <given-names>R. T.</given-names>
          </string-name>
          (
          <year>1995</year>
          ).
          <article-title>Auditory event-related potentials dissociate early and late memory processes</article-title>
          .
          <source>Electroencephalography &amp; Clinical Neurophysiology</source>
          ,
          <volume>96</volume>
          ,
          <fpage>157</fpage>
          -
          <lpage>168</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Corballis</surname>
            ,
            <given-names>M. C.</given-names>
          </string-name>
          (
          <year>2009</year>
          ).
          <article-title>The evolution of language</article-title>
          .
          <source>Annals of the New York Academy of Sciences</source>
          ,
          <volume>1156</volume>
          ,
          <fpage>19</fpage>
          -
          <lpage>43</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Corina</surname>
            ,
            <given-names>D. P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>McBurney</surname>
            ,
            <given-names>S. L.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Dodrill</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Hinshaw</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Brinkley</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Ojemann</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          (
          <year>1999</year>
          ).
          <article-title>Functional roles of Broca's area</article-title>
          and
          <article-title>SMG: Evidence from cortical stimulation mapping in a deaf signer</article-title>
          .
          <source>NeuroImage</source>
          ,
          <volume>10</volume>
          ,
          <fpage>570</fpage>
          -
          <lpage>581</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Corina</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Chiu</surname>
            ,
            <given-names>Y.-S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Knapp</surname>
            ,
            <given-names>H.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Greenwald</surname>
            , R., San Jose-Robertson,
            <given-names>L.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Braun</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          (
          <year>2007</year>
          ).
          <article-title>Neural correlates of human action observation in hearing and deaf subjects</article-title>
          .
          <source>Brain Research</source>
          ,
          <volume>1152</volume>
          ,
          <fpage>111</fpage>
          -
          <lpage>129</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Corina</surname>
            ,
            <given-names>D. P.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Knapp</surname>
            ,
            <given-names>H. P.</given-names>
          </string-name>
          (
          <year>2006</year>
          ).
          <article-title>Psycholinguistic and neurolinguistic perspectives on sign languages</article-title>
          . In M. J. Traxler &amp; M. A. Gernsbacher (Eds.), Handbook of psycholinguistics (2nd ed., pp.
          <fpage>1001</fpage>
          -
          <lpage>1024</lpage>
          ). San Diego, CA: Academic Press.
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Corina</surname>
            ,
            <given-names>D. P.</given-names>
          </string-name>
          , San Jose-Robertson,
          <string-name>
            <given-names>L.</given-names>
            ,
            <surname>Guillemin</surname>
          </string-name>
          ,
          <string-name>
            <given-names>A.</given-names>
            ,
            <surname>High</surname>
          </string-name>
          ,
          <string-name>
            <given-names>J.</given-names>
            , &amp;
            <surname>Braun</surname>
          </string-name>
          ,
          <string-name>
            <surname>A. R</surname>
          </string-name>
          . (
          <year>2003</year>
          ).
          <article-title>Language lateralization in a bimanual language</article-title>
          .
          <source>Journal of Cognitive Neuroscience</source>
          ,
          <volume>15</volume>
          ,
          <fpage>718</fpage>
          -
          <lpage>730</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Corina</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Grosvald</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Lachaud</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          (
          <year>2011</year>
          ).
          <article-title>Perceptual invariance or orientation specificity in American Sign Language? Evidence from repetition priming for signs and gestures</article-title>
          .
          <source>Language and Cognitive Processes</source>
          ,
          <volume>26</volume>
          ,
          <fpage>1102</fpage>
          -
          <lpage>1135</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Cornejo</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Simonetti</surname>
            ,
            <given-names>F.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Ibáñez</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Aldunate</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>López</surname>
            ,
            <given-names>V.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Ceric</surname>
            ,
            <given-names>F.</given-names>
          </string-name>
          (
          <year>2009</year>
          ).
          <article-title>Gesture and metaphor comprehension: Electrophysiological evidence of crossmodal coordination by audiovisual stimulation</article-title>
          .
          <source>Brain and Cognition</source>
          ,
          <volume>70</volume>
          ,
          <fpage>42</fpage>
          -
          <lpage>52</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Delorme</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Makeig</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          (
          <year>2004</year>
          ).
          <article-title>EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics</article-title>
          .
          <source>Journal of Neuroscience Methods</source>
          ,
          <volume>134</volume>
          ,
          <fpage>9</fpage>
          -
          <lpage>21</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Emmorey</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          (
          <year>2002</year>
          ).
          <article-title>Language, cognition, and the brain: Insights from sign language research</article-title>
          . Mahwah, NJ: Lawrence Erlbaum.
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Emmorey</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Xu</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Gannon</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Goldin-Meadow</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Braun</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          (
          <year>2010</year>
          ).
          <article-title>CNS activation and regional connectivity during pantomime observation: No engagement of the mirror neuron system for deaf signers</article-title>
          .
          <source>NeuroImage</source>
          ,
          <volume>49</volume>
          ,
          <fpage>994</fpage>
          -
          <lpage>1005</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Friedrich</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Eulitz</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Lahiri</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          (
          <year>2006</year>
          ).
          <article-title>Not every pseudoword disrupts word recognition: An ERP study</article-title>
          .
          <source>Behavioral and Brain Functions</source>
          ,
          <volume>2</volume>
          ,
          <fpage>36</fpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Friederici</surname>
            ,
            <given-names>A. D.</given-names>
          </string-name>
          (
          <year>2002</year>
          ).
          <article-title>Towards a neural basis of auditory sentence processing</article-title>
          .
          <source>Trends in Cognitive Sciences</source>
          ,
          <volume>6</volume>
          ,
          <fpage>78</fpage>
          -
          <lpage>84</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Frishberg</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          (
          <year>1987</year>
          ).
          <article-title>Ghanaian Sign Language</article-title>
          . In J. Van Cleve (Ed.),
          <article-title>Gallaudet encyclopaedia of deaf people and deafness</article-title>
          .
          <publisher-loc>New York</publisher-loc>
          :
          <publisher-name>McGraw-Gill Book Company.</publisher-name>
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Ganis</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Kutas</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          (
          <year>2003</year>
          ).
          <article-title>An electrophysiological study of scene effects on object identification</article-title>
          .
          <source>Cognitive Brain Research</source>
          ,
          <volume>16</volume>
          ,
          <fpage>123</fpage>
          -
          <lpage>144</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Ganis</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kutas</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Sereno</surname>
            ,
            <given-names>M. I.</given-names>
          </string-name>
          (
          <year>1996</year>
          ).
          <article-title>The search for ''common sense'': An electrophysiological study of the comprehension of words and pictures in reading</article-title>
          .
          <source>Journal of Cognitive Neuroscience</source>
          ,
          <volume>8</volume>
          ,
          <fpage>89</fpage>
          -
          <lpage>106</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Gentilucci</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Corballis</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          (
          <year>2006</year>
          ).
          <article-title>From manual gesture to speech: A gradual transition</article-title>
          .
          <source>Neuroscience &amp; Biobehavioral Reviews</source>
          ,
          <volume>30</volume>
          ,
          <fpage>949</fpage>
          -
          <lpage>960</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Goldin-Meadow</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          (
          <year>2003</year>
          ).
          <article-title>Hearing gestures: How our hands help us think</article-title>
          . Cambridge, MA: Harvard University Press.
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Greenhouse</surname>
            ,
            <given-names>W. W.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Geisser</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          (
          <year>1959</year>
          ).
          <article-title>On methods in the analysis of profile data</article-title>
          .
          <source>Psychometrika</source>
          ,
          <volume>24</volume>
          ,
          <fpage>95</fpage>
          -
          <lpage>112</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Hagoort</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Brown</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          (
          <year>1994</year>
          ).
          <article-title>Brain responses to lexical ambiguity resolution and parsing</article-title>
          . In L. Frazier, J. Clifton Charles, &amp; K. Rayner (Eds.),
          <source>Perspectives in sentence processing</source>
          (pp.
          <fpage>45</fpage>
          -
          <lpage>80</lpage>
          ). Hillsdale, NJ, UK: Lawrence Erlbaum Associates.
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Hagoort</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Kutas</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          (
          <year>1995</year>
          ).
          <article-title>Electrophysiological insights into language deficits</article-title>
          .
          <source>In F. Boller &amp; J</source>
          . Grafman (Eds.),
          <source>Handbook of neuropsychology</source>
          (pp.
          <fpage>105</fpage>
          -
          <lpage>134</lpage>
          ). Amsterdam: Elsevier.
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Hagoort</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          , &amp; van Berkum,
          <string-name>
            <surname>J.</surname>
          </string-name>
          (
          <year>2007</year>
          ).
          <article-title>Beyond the sentence given</article-title>
          .
          <source>Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences</source>
          ,
          <volume>362</volume>
          ,
          <fpage>801</fpage>
          -
          <lpage>811</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Holcomb</surname>
            ,
            <given-names>P. J.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>McPherson</surname>
            ,
            <given-names>W. B.</given-names>
          </string-name>
          (
          <year>1994</year>
          ).
          <article-title>Event-related brain potentials reflect semantic priming in an object decision task</article-title>
          .
          <source>Brain and Cognition</source>
          ,
          <volume>24</volume>
          ,
          <fpage>259</fpage>
          -
          <lpage>276</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Holcomb</surname>
            ,
            <given-names>P. J.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Neville</surname>
            ,
            <given-names>H. J.</given-names>
          </string-name>
          (
          <year>1990</year>
          ).
          <article-title>Auditory and visual semantic priming in lexical decision: A comparison using event-related brain potentials</article-title>
          .
          <source>Language and Cognitive Processes</source>
          ,
          <volume>5</volume>
          ,
          <fpage>281</fpage>
          -
          <lpage>312</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Holcomb</surname>
            ,
            <given-names>P. J.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Neville</surname>
            ,
            <given-names>H. J.</given-names>
          </string-name>
          (
          <year>1991</year>
          ).
          <article-title>Natural speech processing: An analysis using event-related brain potentials</article-title>
          .
          <source>Psychobiology</source>
          ,
          <volume>19</volume>
          ,
          <fpage>286</fpage>
          -
          <lpage>300</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Holle</surname>
            ,
            <given-names>H.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Gunter</surname>
            ,
            <given-names>T. C.</given-names>
          </string-name>
          (
          <year>2007</year>
          ).
          <article-title>The role of iconic gestures in speech disambiguation: ERP evidence</article-title>
          .
          <source>Journal of Cognitive Neuroscience</source>
          ,
          <volume>19</volume>
          ,
          <fpage>1175</fpage>
          -
          <lpage>1192</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Johnson</surname>
            ,
            <given-names>B. W.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Hamm</surname>
            ,
            <given-names>J. P.</given-names>
          </string-name>
          (
          <year>2000</year>
          ).
          <article-title>High-density mapping in an N400 paradigm: Evidence for bilateral temporal lobe generators</article-title>
          .
          <source>Clinical Neurophysiology</source>
          ,
          <volume>111</volume>
          ,
          <fpage>532</fpage>
          -
          <lpage>545</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Johnson</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          , Jr, &amp;
          <string-name>
            <surname>Donchin</surname>
            ,
            <given-names>E.</given-names>
          </string-name>
          (
          <year>1980</year>
          ).
          <article-title>P300 and stimulus categorization: Two plus one is not so different from one plus one</article-title>
          .
          <source>Psychophysiology</source>
          ,
          <volume>17</volume>
          ,
          <fpage>167</fpage>
          -
          <lpage>178</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Kegl</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Senghas</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Coppola</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          (
          <year>1999</year>
          ).
          <article-title>Creation through contact: Sign language emergence and sign language change in Nicaragua</article-title>
          . In M. DeGraff (Ed.),
          <source>Language creation and language change: Creolization</source>
          , diachrony, and development (pp.
          <fpage>179</fpage>
          -
          <lpage>237</lpage>
          ).
          <string-name>
            <surname>Cambridge</surname>
            <given-names>MA</given-names>
          </string-name>
          : MIT Press.
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Kelly</surname>
            ,
            <given-names>S. D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kravitz</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Hopkins</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          (
          <year>2004</year>
          ).
          <article-title>Neural correlates of bimodal speech and gesture comprehension</article-title>
          .
          <source>Brain and Language</source>
          ,
          <volume>89</volume>
          ,
          <fpage>243</fpage>
          -
          <lpage>260</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Kim</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Osterhout</surname>
            ,
            <given-names>L.</given-names>
          </string-name>
          (
          <year>2005</year>
          ).
          <article-title>The independence of combinatory semantic processing: Evidence from event-related potentials</article-title>
          .
          <source>Journal of Memory and Language</source>
          ,
          <volume>52</volume>
          ,
          <fpage>205</fpage>
          -
          <lpage>225</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Kutas</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Hillyard</surname>
            ,
            <given-names>S. A.</given-names>
          </string-name>
          (
          <year>1980</year>
          ).
          <article-title>Reading senseless sentences: Brain potentials reflect semantic incongruity</article-title>
          .
          <source>Science</source>
          ,
          <volume>207</volume>
          ,
          <fpage>203</fpage>
          -
          <lpage>208</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Kutas</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Hillyard</surname>
            ,
            <given-names>S. A.</given-names>
          </string-name>
          (
          <year>1984</year>
          ).
          <article-title>Brain potentials during reading reflect word expectancy and semantic association</article-title>
          .
          <source>Nature</source>
          ,
          <volume>307</volume>
          ,
          <fpage>161</fpage>
          -
          <lpage>163</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Kutas</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Neville</surname>
            ,
            <given-names>H. J.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Holcomb</surname>
            ,
            <given-names>P. J.</given-names>
          </string-name>
          (
          <year>1987</year>
          ).
          <article-title>A preliminary comparison of the N400 response to semantic anomalies during reading</article-title>
          , listening, and signing. Electroencephalography and
          <string-name>
            <given-names>Clinical</given-names>
            <surname>Neurophysiology</surname>
          </string-name>
          , Supplement,
          <volume>39</volume>
          ,
          <fpage>325</fpage>
          -
          <lpage>330</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Liddell</surname>
            ,
            <given-names>S. K.</given-names>
          </string-name>
          (
          <year>2003</year>
          ).
          <article-title>Grammar, gesture, and meaning in American Sign Language</article-title>
          .
          <publisher-name>Cambridge, UK: Cambridge University Press.</publisher-name>
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>MacSweeney</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Campbell</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Woll</surname>
            ,
            <given-names>B.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Giampietro</surname>
            ,
            <given-names>V.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>David</surname>
            ,
            <given-names>A. S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>McGuire</surname>
            ,
            <given-names>P. K.</given-names>
          </string-name>
          , et al. (
          <year>2004</year>
          ).
          <article-title>Dissociating linguistic and nonlinguistic gestural communication in the brain</article-title>
          .
          <source>NeuroImage</source>
          ,
          <volume>22</volume>
          ,
          <fpage>1605</fpage>
          -
          <lpage>1618</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>McPherson</surname>
            ,
            <given-names>W. B.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Holcomb</surname>
            ,
            <given-names>P. J.</given-names>
          </string-name>
          (
          <year>1999</year>
          ).
          <article-title>An electrophysiological investigation of semantic priming with pictures of real objects</article-title>
          .
          <source>Psychophysiology</source>
          ,
          <volume>36</volume>
          ,
          <fpage>53</fpage>
          -
          <lpage>65</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Meir</surname>
            ,
            <given-names>I.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Sandler</surname>
            ,
            <given-names>W.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Padden</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Aronoff</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          (
          <year>2010</year>
          ).
          <article-title>Emerging sign languages</article-title>
          . In M. Marschark &amp; P. Spencer (Eds.).
          <source>Oxford handbook of deaf studies, language, and education (Vol. 2)</source>
          .
          <publisher-loc>New York</publisher-loc>
          : Oxford University Press.
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Morford</surname>
            ,
            <given-names>J. P.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Kegl</surname>
            ,
            <given-names>J. A.</given-names>
          </string-name>
          (
          <year>2000</year>
          ).
          <article-title>Gestural precursors to linguistic constructs: How input shapes the form of language</article-title>
          . In D. McNeill (Ed.),
          <source>Language and gesture</source>
          (pp.
          <fpage>358</fpage>
          -
          <lpage>387</lpage>
          ). Cambridge, UK: Cambridge University Press.
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Neville</surname>
            ,
            <given-names>H. J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Bavelier</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Corina</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Rauschecker</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Karni</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Lalwani</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          , et al. (
          <year>1998</year>
          ).
          <article-title>Cerebral organization for language in deaf and hearing subjects: Biological constraints and effects of experience</article-title>
          .
          <source>Proceedings of the National Academy of Sciences of the United States of America</source>
          ,
          <volume>95</volume>
          ,
          <fpage>922</fpage>
          -
          <lpage>929</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Neville</surname>
            ,
            <given-names>H. J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Coffey</surname>
            ,
            <given-names>S. A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Lawson</surname>
            ,
            <given-names>D. S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Fischer</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Emmorey</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Bellugi</surname>
            ,
            <given-names>U.</given-names>
          </string-name>
          (
          <year>1997</year>
          ).
          <article-title>Neural systems mediating American Sign Language: Effects of sensory experience and age of acquisition</article-title>
          .
          <source>Brain and Language</source>
          ,
          <fpage>285</fpage>
          -
          <lpage>308</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Neville</surname>
            ,
            <given-names>H. J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Nicol</surname>
            ,
            <given-names>J. L.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Barss</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Forster</surname>
            ,
            <given-names>K. I.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Garrett</surname>
            ,
            <given-names>M. F.</given-names>
          </string-name>
          (
          <year>1991</year>
          ).
          <article-title>Syntactically based sentence processing classes: Evidence from event-related brain potentials</article-title>
          .
          <source>Journal of Cognitive Neuroscience</source>
          ,
          <volume>3</volume>
          ,
          <fpage>151</fpage>
          -
          <lpage>165</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Nigam</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Hoffman</surname>
            ,
            <given-names>J. E.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Simons</surname>
            ,
            <given-names>R. F.</given-names>
          </string-name>
          (
          <year>1992</year>
          ).
          <article-title>N400 and semantic anomaly with pictures and words</article-title>
          .
          <source>Journal of Cognitive Neuroscience</source>
          ,
          <volume>4</volume>
          ,
          <fpage>15</fpage>
          -
          <lpage>22</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Osterhout</surname>
            ,
            <given-names>L.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Holcomb</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          (
          <year>1992</year>
          ).
          <article-title>Event-related brain potentials elicited by syntactic anomaly</article-title>
          .
          <source>Journal of Memory and Language</source>
          ,
          <volume>31</volume>
          ,
          <fpage>785</fpage>
          -
          <lpage>806</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Ozyürek</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Willems</surname>
            ,
            <given-names>R. M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kita</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Hagoort</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          (
          <year>2007</year>
          ).
          <article-title>On-line integration of semantic information from speech and gesture: Insights from event-related brain potentials</article-title>
          .
          <source>Journal of Cognitive Neuroscience</source>
          ,
          <volume>19</volume>
          ,
          <fpage>605</fpage>
          -
          <lpage>616</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Pratarelli</surname>
            ,
            <given-names>M. E.</given-names>
          </string-name>
          (
          <year>1994</year>
          ).
          <article-title>Semantic processing of pictures and spoken words: Evidence from event-related brain potentials</article-title>
          .
          <source>Brain and Cognition</source>
          ,
          <volume>24</volume>
          ,
          <fpage>137</fpage>
          -
          <lpage>157</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Rizzolatti</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Arbib</surname>
            ,
            <given-names>M. A.</given-names>
          </string-name>
          (
          <year>1998</year>
          ).
          <article-title>Language within our grasp</article-title>
          . Trends in Neurosciences,
          <volume>21</volume>
          ,
          <fpage>188</fpage>
          -
          <lpage>194</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Rugg</surname>
            ,
            <given-names>M. D.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Nagy</surname>
            ,
            <given-names>M. E.</given-names>
          </string-name>
          (
          <year>1987</year>
          ).
          <article-title>Lexical contribution to non-word-repetition effects: Evidence from event-related potentials</article-title>
          .
          <source>Memory and Cognition</source>
          ,
          <volume>15</volume>
          ,
          <fpage>473</fpage>
          -
          <lpage>481</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Senghas</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          (
          <year>2005</year>
          ).
          <article-title>Language emergence</article-title>
          .
          <article-title>Clues from a new Bedouin sign language</article-title>
          .
          <source>Current Biology</source>
          ,
          <volume>15</volume>
          ,
          <fpage>463</fpage>
          -
          <lpage>465</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Sitnikova</surname>
            ,
            <given-names>T.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Holcomb</surname>
            ,
            <given-names>P. J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kiyonaga</surname>
            ,
            <given-names>K. A.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Kuperberg</surname>
            ,
            <given-names>G. R.</given-names>
          </string-name>
          (
          <year>2008</year>
          ).
          <article-title>Two neurocognitive mechanisms of semantic integration during the comprehension of real-world events</article-title>
          .
          <source>Journal of Cognitive Neuroscience</source>
          ,
          <volume>20</volume>
          ,
          <fpage>2037</fpage>
          -
          <lpage>2057</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Sitnikova</surname>
            ,
            <given-names>T.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kuperberg</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Holcomb</surname>
            ,
            <given-names>P. J.</given-names>
          </string-name>
          (
          <year>2003</year>
          ).
          <article-title>Semantic integration in videos of real-world events: An electrophysiological investigation</article-title>
          .
          <source>Psychophysiology</source>
          ,
          <volume>40</volume>
          ,
          <fpage>160</fpage>
          -
          <lpage>164</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Tomasello</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          (
          <year>2005</year>
          ).
          <article-title>Constructing a language: A usage-based theory of language acquisition</article-title>
          . Cambridge, MA: Harvard University Press.
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <given-names>Van</given-names>
            <surname>Petten</surname>
          </string-name>
          ,
          <string-name>
            <given-names>C.</given-names>
            , &amp;
            <surname>Rheinfelder</surname>
          </string-name>
          ,
          <string-name>
            <surname>H.</surname>
          </string-name>
          (
          <year>1995</year>
          ).
          <article-title>Conceptual relationships between spoken words and environmental sounds: Event-related brain potential measures</article-title>
          .
          <source>Neuropsychologia</source>
          ,
          <volume>33</volume>
          ,
          <fpage>485</fpage>
          -
          <lpage>508</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>West</surname>
            ,
            <given-names>W. C.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Holcomb</surname>
            ,
            <given-names>P. J.</given-names>
          </string-name>
          (
          <year>2002</year>
          ).
          <article-title>Event-related potentials during discourse-level semantic integration of complex pictures</article-title>
          .
          <source>Cognitive Brain Research</source>
          ,
          <volume>13</volume>
          ,
          <fpage>363</fpage>
          -
          <lpage>375</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Wilcox</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          (
          <year>2004</year>
          ).
          <article-title>Gesture and language: Cross-linguistic and historical data from signed languages</article-title>
          .
          <source>Gesture</source>
          ,
          <volume>4</volume>
          ,
          <fpage>43</fpage>
          -
          <lpage>73</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Wu</surname>
            ,
            <given-names>Y. C.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Coulson</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          (
          <year>2005</year>
          ).
          <article-title>Meaningful gestures: Electrophysiological indices of iconic gesture comprehension</article-title>
          .
          <source>Psychophysiology</source>
          ,
          <volume>42</volume>
          ,
          <fpage>654</fpage>
          -
          <lpage>667</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Wu</surname>
            ,
            <given-names>Y. C.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Coulson</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          (
          <year>2007a</year>
          ).
          <article-title>How iconic gestures enhance communication: An ERP study</article-title>
          .
          <source>Brain and Language</source>
          ,
          <volume>101</volume>
          ,
          <fpage>234</fpage>
          -
          <lpage>245</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Wu</surname>
            ,
            <given-names>Y. C.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Coulson</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          (
          <year>2007b</year>
          ).
          <article-title>Iconic gestures prime related concepts: An ERP study</article-title>
          .
          <source>Psychonomic Bulletin &amp; Review</source>
          ,
          <volume>14</volume>
          ,
          <fpage>57</fpage>
          -
          <lpage>63</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Ziegler</surname>
            ,
            <given-names>J. C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Besson</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Jacobs</surname>
            ,
            <given-names>A. M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Nazir</surname>
            ,
            <given-names>T. A.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Carr</surname>
            ,
            <given-names>T. H.</given-names>
          </string-name>
          (
          <year>1997</year>
          ).
          <article-title>Word, pseudoword, and nonword processing: A multitask comparison using eventrelated brain potentials</article-title>
          .
          <source>Journal of Cognitive Neuroscience</source>
          ,
          <volume>9</volume>
          ,
          <fpage>758</fpage>
          -
          <lpage>775</lpage>
          .
        </mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>
