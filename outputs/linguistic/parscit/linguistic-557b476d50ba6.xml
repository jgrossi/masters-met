<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<figure confidence="0.865468666666667">
Brain &amp; Language 121 (2012) 12–24
Contents lists available at SciVerse ScienceDirect
Brain &amp; Language
journal homepage: www.elsevier.com/locate/b&amp;l
Dissociating linguistic and non-linguistic gesture processing:
Electrophysiological evidence from American Sign Language
Michael Grosvald a,⇑, Eva Gutierrez b, Sarah Hafer b, David Corina b,c
a
Department of Neurology, University of California at Irvine, United States
Center for Mind and Brain, University of California at Davis, United States
c
Departments of Linguistics and Psychology, University of California at Davis, United States
b
a r t i c l e
i n f o
Article history:
Accepted 24 January 2012
Available online 17 February 2012
Keywords:
Sign language
ASL
ERP
N400
Deaf
Pseudo-word
Grooming gesture
a b s t r a c t
</figure>
<bodyText confidence="0.973464">
A fundamental advance in our understanding of human language would come from a detailed account of
how non-linguistic and linguistic manual actions are differentiated in real time by language users. To
explore this issue, we targeted the N400, an ERP component known to be sensitive to semantic context.
Deaf signers saw 120 American Sign Language sentences, each consisting of a ‘‘frame’’ (a sentence without the last word; e.g. BOY SLEEP IN HIS) followed by a ‘‘last item’’ belonging to one of four categories: a
high-close-probability sign (a ‘‘semantically reasonable’’ completion to the sentence; e.g. BED), a lowclose-probability sign (a real sign that is nonetheless a ‘‘semantically odd’’ completion to the sentence;
e.g. LEMON), a pseudo-sign (phonologically legal but non-lexical form), or a non-linguistic grooming gesture (e.g. the performer scratching her face). We found signiﬁcant N400-like responses in the incongruent
and pseudo-sign contexts, while the gestures elicited a large positivity.
Ó 2012 Elsevier Inc. All rights reserved.
</bodyText>
<sectionHeader confidence="0.991631" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.959014340909091">
While it is now widely accepted that signed languages used in
deaf communities around the world represent full-ﬂedged instantiations of human languages—languages which are expressed in the
visual–manual modality rather than the aural–oral modality—the
question of how a sign is recognized and integrated into a sentential
context in real time has received far less attention (see Corina &amp;
Knapp, 2006; Emmorey, 2002; for some discussions). Sign language
recognition may be more complicated than spoken language recognition by virtue of the fact that the primary articulators, the hands
and arms, are also used in a wide range of other common everyday
behaviors that include non-linguistic actions such a reaching and
grasping, waving, and scratching oneself, as well gesticulations that
accompany speech (i.e. co-speech gestures) or serve non-sign language deictic functions, such as pointing.
The formal relationship between signed languages and human
gestural actions is of considerable interest to a range of disciplines.
Linguists, psychologists and cognitive scientists have proposed a
critical role for manual gesture in the development and evolution
of human languages (Arbib, 2005, 2008; Gentilucci &amp; Corballis,
2006; Rizzolatti &amp; Arbib, 1998; Tomasello, 2005; Wilcox, 2004). Re-
⇑ Corresponding author. Address: Department of Neurology, University of California at Irvine, 101 The City Drive South, Bldg. 53, Room 204, Orange, CA 928684280, United States. Fax: +1 714 456 1697.
E-mail address: m.grosvald@uci.edu (M. Grosvald).
0093-934X/$ - see front matter Ó 2012 Elsevier Inc. All rights reserved.
doi:10.1016/j.bandl.2012.01.005
cently, linguists have documented compelling evidence that the
development of nascent sign languages derives from idiosyncratic
gestural and pantomimic systems used by isolated communities,
which in some cases may be limited to individual families who have
a need to communicate with a deaf child (Frishberg, 1987; GoldinMeadow, 2003; Kegl, Senghas, &amp; Coppola, 1999; Meir, Sandler, Padden, &amp; Aronoff, 2010; Morford &amp; Kegl, 2000; Senghas, 2005). Even
within mature sign languages of Deaf communities, linguistic accounts of sign language structure have also argued that lexical and
discourse components of American Sign Language (ASL) and other
signed languages may be best understood as being gesturally based
(Liddell, 2003). Thus diachronic and synchronic evidence from language research support the contention that signed languages might
make use of perceptual systems similar to those through which humans understand or parse human actions and gestures more generally (Corballis, 2009). In contrast, given its linguistic status, sign
language perception may require the attunement of specialized systems for recognizing sign forms.
A comprehensive theory of sign language recognition will be
enhanced by providing an account of when and how the processing
of sign forms diverges from the processing of human actions in
general. Recent behavioral and neuro-imaging studies have reported differences in deaf subjects’ responses to single signs compared to non-linguistic gestures (Corina, Grosvald, &amp; Lachaud,
2011; Corina et al., 2007; Emmorey, Xu, Gannon, Goldin-Meadow,
&amp; Braun, 2010; MacSweeney et al., 2004), but no studies to our
knowledge have examined the recognition of signs and gestures
M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24
under sentence processing constraints. Consider for example the
signer, who, in mid-sentence, fulﬁlls the urge to scratch his face,
or perhaps swat away a ﬂying insect. What is the fate of this
non-linguistic articulation? Does the sign perceiver attempt to
incorporate these manual behaviors into accruing sentential representations, or are these actions easily tagged as non-linguistic and
thus rejected by the parser? The goal of the present paper was to
use real-time electrophysiological measures to assess empirically
the time course of sentence processing in cases where subjects
encountered non-linguistic manual forms (here ‘‘self-grooming’’
behaviors, e.g. scratching the face, rubbing one’s eye, adjusting
the sleeves of a shirt, etc.). We sought to compare the processing
of these non-linguistic gestural forms within a sentential context
to cases in which deaf signers encountered violations of semantic
expectancy that have been observed to elicit a well-deﬁned electrophysiological component, the N400.
The N400 component (Holcomb &amp; Neville, 1991; Kutas &amp; Hillyard, 1980) has been frequently investigated in previous ERP research on written, spoken and signed language (e.g. Capek et al.,
2009; Kutas, Neville, &amp; Holcomb, 1987). The N400 is a broad negative deﬂection generally seen at central and parietal scalp sites
that peaks about 400 ms after the visual or auditory presentation
of a word. Although all content words elicit an N400 component,
the ERP response is larger for words that are semantically anomalous or less expected (Hagoort &amp; Brown, 1994; Kutas &amp; Hillyard,
1984); thus the N400 is often interpreted as an index of ease or difﬁculty in semantic conceptual integration (Brown &amp; Hagoort, 1993;
Hagoort &amp; Van Berkum, 2007). For example, for listeners encountering the two sentences ‘‘I like my coffee with milk and sugar’’
and ‘‘I like my coffee with milk and mud,’’ the N400 response to
the last word in the second item is expected to be larger.
An N400 or N400-like component can also be found in response
to orthographically/phonologically legal but non-occurring ‘‘pseudo-words’’ (e.g. ‘‘blork’’), and it has sometimes been reported that
pseudo-words elicit a stronger N400 response than semantically
incongruent real words (Bentin, 1987; Bentin, McCarthy, &amp; Wood,
1985; Hagoort &amp; Kutas, 1995), consistent with the idea that the
magnitude of N400 response is related to the difﬁculty of the ongoing process of semantic-contextual integration. However, orthographically illegal ‘‘non-words’’ (e.g. ‘‘rbsnk’’) do not generally
elicit an N400, and a positive component is sometimes seen instead
(Hagoort &amp; Kutas, 1995; Ziegler, Besson, Jacobs, Nazir, &amp; Carr,
1997). This may reﬂect the operation of some kind of ﬁltering
mechanism during online processing, through which language
users are able to quickly reject forms that lie beyond a certain point
of acceptability, or plausibility, during the ongoing processing of
the incoming language stream.1
The N400 (or N400-like responses) can also be observed in numerous contexts involving non-linguistic but meaningful stimuli, such as
pictures (Ganis &amp; Kutas, 2003; Ganis, Kutas, &amp; Sereno, 1996; Nigam,
Hoffman, &amp; Simons, 1992; Pratarelli, 1994), faces (Barrett &amp; Rugg,
1989; Bobes, Valdés-Sosa, &amp; Olivares, 1994), environmental noises
(Chao, Nielsen-Bohlman, &amp; Knight, 1995; Van Petten &amp; Rheinfelder,
1995), movie clips (Sitnikova, Holcomb, Kiyonaga, &amp; Kuperberg,
2008; Sitnikova, Kuperberg, &amp; Holcomb, 2003) and co-speech gestures (Kelly, Kravitz, &amp; Hopkins, 2004; Wu &amp; Coulson, 2005).
Linguistically anomalous stimuli are not always associated with
an N400 response. For example, the left anterior negativity (LAN;
Friederici, 2002; Neville, Nicol, Barss, Forster, &amp; Garrett, 1991) and
P600 (Osterhout &amp; Holcomb, 1992) are well-known ERP
components that have been found in syntactic violation contexts in
</bodyText>
<page confidence="0.882961">
1
</page>
<bodyText confidence="0.9681785">
This possibility is bolstered by recent work of Albert Kim and colleagues, who
have found that relative to real word controls, N400 amplitude decreases and P600
amplitude increases, parametrically, as orthographic irregularity increases (Kim &amp;
Pitkänen, submitted for publication).
</bodyText>
<page confidence="0.999016">
13
</page>
<bodyText confidence="0.99966512244898">
spoken and written language, and more recent work has shown that
these components can be elicited in the visual–manual modality as
well. For example, in a recent study Capek et al. (2009) compared
ERP responses to semantically and syntactically well-formed and
ill-formed sentences. While semantic violations elicited an N400 that
was largest over central and posterior sites, syntactic violations
elicited an anterior negativity followed by a widely distributed
P600. These ﬁndings are consistent with the idea that within written,
spoken and signed languages, semantic and syntactic processes are
mediated by non-identical brain systems (Capek et al., 2009).
The present study makes use of dynamic video stimuli showing
ASL sentences completed by four classes of ending item—semantically congruent signs, semantically incongruent signs, phonologically
legal but non-occurring pseudo-signs, and non-linguistic grooming
gestures. Based upon previous studies, we expected a gradation of
N400-like responses across conditions, with N400 effects of smaller
magnitude for semantically incongruent endings and of larger magnitude (i.e. more negative) for phonologically legal pseudo-signs.
The ERP response for the non-linguistic gesture condition is a
priori more difﬁcult to predict. Previous neuro-imaging studies of
deaf signers have reported differences in patterns of activation
associated with the perception of signs compared to non-linguistic
gestures (Corina et al., 2007; Emmorey et al., 2010; MacSweeney
et al., 2004), but the methodologies used in those studies lacked
the temporal resolution to determine at what stage of processing
these differences may occur. While N400-like responses have been
elicited to co-speech gestural mismatches (Kelly et al., 2004; Wu &amp;
Coulson, 2005), in our study, gestures occur in place of semantically
appropriate sentence-ending items, rather than as a possible
accompaniment. It should also be borne in mind that the relationship of signs and grooming gestures is probably not quite akin to
that between standard lexical items in spoken language and the
orthographically/phonotactically illegal pseudo-words used in earlier ERP studies. Unlike grooming gestures, which are part of everyday life, illegal non-words like ‘‘dkfpst’’ are probably alien to most
people’s routine experience. A better spoken-language analogue of
our grooming action condition might be something like ‘‘I like my
coffee with milk and [clearing of throat],’’ though we know of no
spoken-language studies which have incorporated such a condition. The non-linguistic grooming gestures used in the present
study may be another example of forms that language users (in
this case, signers) are able to quickly reject as non-linguistic during
language processing. If this is the case, then one might also expect
that such forms will not elicit an N400 but rather a positive-going
component (cf. Hagoort &amp; Kutas, 1995).
In summary, to the extent that semantic processing at the sentence level is similar for signed and spoken language, despite the
obvious difference in modality, the ERP responses associated with
our four sentence ending condition should be predictable. First, the
incongruent signs should elicit a negative-going component relative to the baseline (congruent sign) condition, consistent with
the classic N400 response seen for English and other spoken languages, as well as some previous ERP studies of ASL (Kutas et al.,
1987; Neville et al., 1997). Second, the pseudo-signs should also
elicit a negative-going wave, and this response can be expected
to be of larger magnitude (i.e. be more negative) than that seen
for the incongruent signs. Third, while the likely response to the
grooming gesture condition is more difﬁcult to predict, we may expect to see a positive-going component relative to the baseline.
</bodyText>
<sectionHeader confidence="0.987763" genericHeader="keywords">
2. Methodology
</sectionHeader>
<subsectionHeader confidence="0.648229">
2.1. Participants
</subsectionHeader>
<bodyText confidence="0.998487">
The 16 participants (12 female and 4 male; age range = [19, 45],
mean = 25.4 and SD = 8.3) were deaf users of ASL; all were students
</bodyText>
<page confidence="0.998654">
14
</page>
<bodyText confidence="0.974246043478261">
M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24
or staff at Gallaudet University in Washington DC and received a
small payment for participating. Three were left-handed. There
were 11 native signers (i.e. born to signing parents; the remaining
ﬁve non-natives’ mean self-reported age of acquisition of ASL was
9.0 (SD = 6.3, range = [2, 16]). All subjects were uninformed as to
the purpose of the study and gave informed consent in accordance
with established Institutional Review Board procedures at Gallaudet University.2
2.2. Stimuli and procedure
During the experiment, the subject was seated in a comfortable
chair facing the computer screen approximately 85 cm away, at
which distance the 4-inch-wide video stimuli subtended an angle
of about 7 degrees. The stimuli were delivered using a program
created by the ﬁrst author using Presentation software (Neurobehavioral Systems).
For each trial, the subject viewed an ASL sentence ‘‘frame’’ consisting of an entire sentence minus a last item (e.g. BOY SLEEP IN
HIS; see Fig. 1 for an illustration), followed by an ‘‘ending item’’
completing the sentence. The ending item could be one of four
types (shown, respectively, to the upper left, upper right, lower left
and lower right of the question mark in Fig. 1): a semantically congruent sign (e.g. BED for the sentence frame just given), a semantically incongruent sign (e.g. LEMON), a phonotactically legal but
non-occurring ‘‘pseudo-sign’’ (e.g. BARK.A, this notation indicating
that this pseudo-sign was formed by articulating the real sign
BARK with an A handshape3), or a grooming gesture such as eye
rubbing or head scratching. All stimulus items (sentence frames
and ending items) were performed by a female native signer of
ASL, who also veriﬁed that each sentence frame plus last sign item
was grammatically acceptable in ASL. The ending items for both sign
conditions (semantically congruent and incongruent) were all nouns.
As is the case with spoken languages, signed languages, including ASL, have regional variants that could potentially affect comprehension. In the present situation, this would be relevant if
subjects encountering extant but unfamiliar sign forms as ending
items produced an ERP response similar to that for pseudo-signs.
Our experience, however, suggests that in the majority of cases,
ﬂuent users of ASL have previously encountered regional variants.
This can be compared to the way a New England English speaker,
while not using the word the ‘‘sack’’ as part of his or her own dialect (instead using bag), would most likely comprehend a sentence
such as ‘‘The grocer placed the vegetables in the sack’’ without difﬁculty. In principle, one might expect such forms to produce increased processing difﬁculties, similar to encountering low
frequency forms. However, we are cognizant of the regional variants in ASL and in planning this study, aimed to use signs which
were unambiguous and would reﬂect the most frequent forms.
As a check, we asked two native signers (not participants in the
main study) to scrutinize all semantically congruent and semanti2
A group of 10 hearing non-signers was also run on the same experiment as a
control measure. All were undergraduate students at the University of California at
Davis with no substantial knowledge of sign language. Like the deaf group, these
subjects were uninformed as to the purpose of the study and gave informed consent
in accordance with established Institutional Review Board procedures at UC Davis.
However, unlike the deaf group, no signiﬁcant effects or interactions related to Ending
item were found, and no further analysis related to this group will be presented here.
</bodyText>
<page confidence="0.981764">
3
</page>
<bodyText confidence="0.994382157894737">
The pseudo-signs could be one-handed or two-handed. The two-handed variants
include cases where a handshape is articulated on a base hand, as well as cases in
which the two hands move symmetrically; both of these occur in real two-handed
signs. Both the one- and two-handed pseudo-sign items appear as compositional
forms that are non-occurring in ASL, and identiﬁcation of the handshape alone is not
sufﬁcient to determine whether the sign is true sign or a pseudo-sign. The consensus
view among the signers in our group is that our pseudo-signs are more akin to legal
but non-occurring items, (e.g. ‘‘glack’’), rather than being consistently seen as
recognizable but altered lexical items (e.g. ‘‘glassu’’).
Fig. 1. Still shots taken from one of the sentence frame stimuli, along with its four
possible endings (see text). Note that the actual stimuli were dynamic, not static.
cally incongruent ending signs, to mark whether they knew of any
regional variants or alternative pronunciations, and to list any such
forms. Note that without a proper sociolinguistic analysis, it is difﬁcult to ascertain whether such differences are indeed sociolinguistic variants, so we were most liberal in asking for any known
alternative form. Out of 240 critical items, 49 were deemed to have
a potential regional variant (e.g., PIZZA and FOOTBALL) or alternative pronunciations (e.g. the sign EARRINGS, which can be signed
with an F handshape, or a b0 handshape, aka ‘‘baby-O’’). Of these
potentially problematic items, we then asked whether if seen in
isolation, any would be unrecognized. Only 7 of those 49 sign
forms were deemed as potentially unrecognized. Most importantly, none of these variants were the forms used in the actual
experiment. For example, EASTER was a sign used as a semantically
incongruent item. Both signers agreed that the form used in our
study was the most common form (two E-handshapes held near
the shoulders with a twisting motion). One of the two signers knew
of an alternative form in which the E handshape rises off of the
palm of the B-handshape base hand. The second signer noted she
would not have known what that item meant if she had seen it
in isolation. Most importantly, such forms were not used in our
study. Thus we are conﬁdent that in choosing our sign ending
items (during which we took into account intuitions and feedback
from native signers, including one of our co-authors on this paper),
we have selected highly frequent and recognizable forms likely to
be recognized by sign users, even if they do not use the same forms
themselves in each and every case (as in the bag/sack example in
English).
Each sentence frame stimulus was ﬁlmed by having the signer
begin with her hands in her lap, raise her hands to sign the sentence frame, then place her hands back in her lap. Each ending item
was also ﬁlmed in this way, beginning and ending with the signer’s
hands in her lap. Each frame was ﬁlmed just once, with no ending
item in mind, rather than creating a separate instance of each
frame for each of the four possible ending items. This was done
to provide a consistent lead-up to each ending item, eliminating
the possibility that differing coarticulatory effects or other confounds might lead to diverging processing on the part of the signer
prior to the onset of each ending item. During video editing, the
stimuli were trimmed slightly so that the full movement of the
M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24
signer’s hands to and from her lap at the end of each sentence
frame and beginning of each ending item was not seen when the
stimuli were viewed during the running of the actual experiment.
Over the course of the entire experiment, the subject saw a sequence of 120 of these sign sentences, with 30 instances of each of
the four types of ending items. The ordering of the sentences was
randomized for each subject and the type of ending item (semantically congruent, semantically incongruent, pseudo-sign or gesture) shown for each sentence frame also varied among subjects.
A complete list of all 120 sentence frames with each of their possible ending items is given in Appendix A. For most trials, no behavioral responses were required, but in order to encourage subjects
to attend to the meaning of the sentences, an occasional comprehension check was given (see Section 2.3).
The sentence frames and ending items were separated by a 200ms blank screen, so that the slight visual discontinuity between the
two stimuli (which were ﬁlmed separately, as described earlier)
would be less jarring. In the Presentation program which delivered
the stimuli, the default color of the computer screen was set to
match the background behind the signer in the video stimuli for
color and intensity. Blink intervals of randomly varying length between 3200 and 3700 ms were given after each trial. Longer blink
intervals lasting an additional 4 s were provided after every eight
trials. Fixation crosses appeared at key moments to remind subjects to maintain a consistent gaze toward the center of the screen,
and to indicate when the next trial was about to begin. After every
30 trials (three times total during the experiment), open-ended
break sessions were given so that subjects could rest longer if they
desired. Before the experiment began, subjects were given a brief
practice session, six trials long, to acquaint them with the format
of the experiment. The six ASL sentences used in the practice session were different from the sentences used in the actual
experiment.
2.3. Behavioral task
In order to provide an objective measure that could be used
after-the-fact to verify that each subject had been paying attention
to the sentences, occasional comprehension checks appearing at
random intervals were programmed into the experiment; these
appeared after each ﬁve to eight sentences. At each comprehension
check, the subject was required to choose which of two words, presented on-screen, was most closely related to the meaning of the
just-shown ASL sentence. For example, after the ASL sentence
beginning with the frame ‘‘BOY SLEEP IN HIS,’’ the two candidate
words were ‘‘ﬁght’’ and ‘‘sleep,’’ with the latter being the correct
answer in this case. Two such words were chosen for each sentence
frame, and were always dependent only on the sentence frame,
never on any of the four possible ending items for that sentence
frame. The two quiz words always appeared side-by-side with
the left vs. right position on-screen being chosen at random on
each trial for the correct and incorrect word choices. The quiz
words were presented in English, but only frequent words were
used as quiz items, so that users of ASL would be unlikely to be
unfamiliar with them. All subjects’ scores were deemed sufﬁciently
high (mean: 97.3%, SD: 4.6%, range: [84.0%, 100%]) that no subjects
were excluded because of poor performance on the quizzes.
2.4. Electroencephalogram (EEG) recording and data analysis
EEG data were recorded continuously from 32 scalp locations at
frontal, parietal, occipital, temporal and central sites, using AgCl
electrodes attached to an elastic cap (BioSemi). Vertical and horizontal eye movements were monitored by means of two electrodes
placed above and below the left eye and two others located adjacent to the left and right eye. All electrodes were referenced to
</bodyText>
<page confidence="0.992756">
15
</page>
<bodyText confidence="0.99341096875">
the average of the left and right mastoids. The EEG was digitized
online at 256 Hz, and ﬁltered ofﬂine below 30 Hz and above
0.01 Hz. Scalp electrode impedance threshold values were set at
20 kX.
Initial analysis of the EEG data was performed using the ERPLAB
plugin (Lopez-Calderon &amp; Luck, in press) for EEGLAB (Delorme &amp;
Makeig, 2004). Epochs began 200 ms before stimulus onset and
ended 1000 ms after. Inspection of subjects’ EEG data was performed by eye to check rejections suggested by a script run in ERPLAB whose artifact rejection thresholds were set at ±120 lV. For all
16 subjects, in each of the four sentence ending conditions at least
20 of the original 30 trials remained after the rejection procedure
just described. The statistical analyses reported below were carried
out using the SPSS statistical package.
To assess the signiﬁcance of the observed effects, a column
analysis was conducted (cf. Kim &amp; Osterhout, 2005) for which a
separate ANOVA was run on each of four subsets of the scalp sites,
as illustrated in Fig. 2. For the midline scalp sites, colored green in
the ﬁgure, the two factors in the ANOVA were Electrode (one level
for each of the four electrodes) and sentence Ending (semantically
congruent sign, semantically incongruent sign, pseudo-sign and
grooming gesture). The other three ANOVAs, corresponding to
the inner (colored blue in the ﬁgure), outer (purple), and outermost
(orange) sites, included a third factor of hemisphere (left or right);
in addition, for these three ANOVAs the factor Electrode had one level for each pair of electrodes, from most anterior to most posterior. For the N400 analysis, the dependent measure was mean
amplitude of EEG response within the window from 400 to
600 ms after stimulus onset. Because the latency of the effects related to gesture was somewhat greater, a second column analysis
was run for the window from 600 to 800 ms after stimulus onset.
For the purposes of these column analyses, data from the two
frontmost sites (FP1 and FP2) and from two posterior sites (PO3
and PO4) were not used. In all cases, Greenhouse–Geisser (Greenhouse &amp; Geisser, 1959) adjustments for non-sphericity were performed where appropriate and are reﬂected in the reported results.
In the following section, the outcomes for the ANOVAs performed for each time window will be given in the order midline,
inner, outer, and outermost. This establishes the signiﬁcance of
the results, which we ﬁrst introduce with illustrations and descriptions of the waveforms and the associated topographic maps.
</bodyText>
<sectionHeader confidence="0.984327" genericHeader="introduction">
3. Results
</sectionHeader>
<bodyText confidence="0.948571416666666">
3.1. Waveforms and topographic maps
Pictured in Figs. 3 and 4 are grand-average waveforms at selected electrode sites for the four sentence Ending conditions. Visual inspection of the waveforms reveals that all conditions
evoked exogenous potentials often associated with written words
(Hagoort &amp; Kutas, 1995); these include a posteriorly distributed
positivity (P1) peaking at about 100 ms post-stimulus onset, followed by a posteriorly distributed negativity (N1) peaking at about
180 ms after target onset. Starting at approximately 300 ms after
stimulus onset, we begin to see a differentiation for different sentence ending conditions which we will quantify in detail in the statistical analysis. Relative to the baseline (semantically congruent
sign) condition, both the semantically incongruent and pseudosign conditions can be seen to have elicited negative-going waves.
In addition, the pseudo-sign negativity is generally greater in magnitude than the negativity elicited in the semantically incongruent
condition. In contrast, beginning at approximately 400 ms we observe a large positive-going wave relative to the baseline for the
grooming gesture condition. These effects appear to be long lasting,
extending beyond the end of the 1000 ms time window.
</bodyText>
<page confidence="0.99802">
16
</page>
<bodyText confidence="0.975897063829787">
M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24
Fig. 2. Electrode groupings for the ANOVAs.
Fig. 5 presents topographic maps of key contrasts, and reinforces the patterns seen Figs. 3 and 4. The maps show mean amplitude difference between the indicated conditions during the two
time windows we analyzed. Again, we see that the pseudo-signs
elicit a negativity that is overall more pronounced than the one
elicited by the semantically incongruent signs, both in terms of
magnitude and distribution. In contrast, the response to the gesture condition starts to diverge clearly from the others in the earlier time window, starting at posterior sites and then spreading
more generally.
We now continue with a presentation of the statistical results.
For simplicity of presentation, only main effects and interactions
related to sentence Ending are discussed in the text, but complete
ANOVA results are given in Table 1. Our analyses conﬁrm the very
consistent patterning of mean ERP response with respect to sentence Ending that was seen in Figs. 3–5 and noted in the foregoing
discussion. Relative to the baseline (semantically congruent sign)
condition, the incongruent signs and the pseudo-signs elicited a
negative-going wave, while the grooming gestures elicited a large
positivity; also, the negative-going component elicited by the
pseudo-signs was overall larger than the one elicited by the incongruent signs. This general pattern was observed for almost all scalp
sites; the relatively minor exceptions are noted below.
3.2. Window from 400 to 600 ms
For the earlier of the two time windows, the midline ANOVA
found a main effect of Ending (p &lt; 0.001) and an interaction of Ending by Electrode (p &lt; 0.001). Mean amplitude for Ending showed
the predicted pattern among condition means, with respective
means for pseudo-signs, incongruent signs, congruent signs and
grooming gestures equal to À4.95, À4.20, À3.36 and À1.16 lV.
The gesture condition mean differed signiﬁcantly from the rest
(p &lt; 0.05 for gesture vs. baseline, p’s &lt; 0.001 for the other two comparisons); the other differences did not reach signiﬁcance. It was at
the frontmost sites that the baseline differed the most from the
incongruent and pseudo-sign conditions (p &lt; 0.05 and p = 0.062,
respectively). At the vertex electrode CZ, a departure from the predicted progression of condition means was found; incongruent
signs showed the most negative amplitude here, which however
was not signiﬁcantly different from the amplitudes for pseudosigns or congruent signs. At the other three midline electrode sites,
the familiar pattern among the four Ending conditions was seen.
The inner-electrode ANOVA found a main effect of Ending
(p &lt; 0.001) and a marginal Ending by Electrode interaction
(p = 0.094). Mean amplitudes for the four conditions progressed
in the predicted way; for pseudo-signs, incongruent signs, congruent signs and grooming gestures, respective means were À4.54,
À3.57, À2.56 and À0.302. Pairwise differences between means
were all signiﬁcant, except for incongruent signs vs. congruent
signs (marginally signiﬁcant at p = 0.088) and pseudo-signs vs.
incongruent signs (p = 0.24). The same ordering from most negative to most positive means for the four conditions was seen at
all three levels of electrode. The most anterior sites showed the
greatest differences between the baseline condition mean and
the incongruent and pseudo-sign means; at these sites, both pairwise differences were signiﬁcant.
For the outer electrodes ANOVA, the results included a main effect of Ending (p &lt; 0.001). The respective overall mean amplitudes
for the pseudo-sign, incongruent, congruent and gesture
M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24
</bodyText>
<page confidence="0.998871">
17
</page>
<bodyText confidence="0.93205609090909">
(cases of the latter were: for pseudo-sign vs. incongruent sign,
p = 0.11; for incongruent sign vs. congruent sign, p = 0.14, for gesture vs. congruent sign, p = 0.062). The absence of a signiﬁcant Ending by Electrode interaction is due to the fact that the predicted
pattern of pseudo-sign &lt; incongruent sign &lt; congruent sign &lt; gesture was seen for all four levels of Electrode.
Finally, the ANOVA for the outermost set of electrodes found a
signiﬁcant main effect of sentence Ending (p &lt; 0.001) and an interaction of Ending by Electrode (p &lt; 0.05). The main effect of Ending
reﬂects the pattern already seen repeatedly; for the outermost
sites, mean EEG amplitudes were À2.45, À1.34, À1.03 and
0.083 lV, respectively, for the pseudo-signs, incongruent signs,
congruent signs and grooming gestures. Follow-up comparisons
showed that all of the pairwise differences among conditions were
signiﬁcant, except for the congruent sign vs. incongruent sign comparison (p = 0.39), and the incongruent sign vs. pseudo-sign comparison, which was however marginally signiﬁcant (p = 0.078).
The Ending by Electrode interaction reﬂects two exceptions to
the predicted pattern for the four Ending conditions. First, at the
two most posterior sites, the amplitude for the incongruent sign
condition was slightly greater than for the congruent sign condition (this difference, however, did not approach signiﬁcance;
p = 0.62). Second, at the anterior electrode sites the gesture condition was not associated with signiﬁcantly greater positivity than
the other three conditions.
3.3. Window from 600 to 800 ms
Fig. 3. Grand-average waveforms at (from top to bottom) frontal, central, parietal
and occipital sites. Units on the vertical axis are microvolts; those on the horizontal
axis are milliseconds. Negative is plotted downward.
Fig. 4. Grand-average waveforms at the OZ site. Units on the vertical axis are
microvolts; successive tick marks on the horizontal axis are 200 ms apart. Negative
is plotted downward.
conditions were À2.93, À1.97, À1.20 and 0.345, consistent with
the predicted pattern. The highly signiﬁcant main effect of Ending
here is due to the fact that the pairwise differences between conditions were either signiﬁcant or showed near-signiﬁcant trends
For the later time window, the midline-electrode ANOVA found
a main effect of Ending (p &lt; 0.001) and an interaction of Ending by
Electrode (p &lt; 0.001). Mean amplitude for the four Ending conditions diverged slightly here from the familiar pattern, with respective means for pseudo-signs, incongruent signs, congruent signs
and grooming gestures equal to À2.43, À2.47, À1.33 and 3.97.
The comparison of pseudo-sign vs. incongruent sign did not approach signiﬁcance however (p = 0.96), nor did the comparison of
pseudo-sign vs. congruent sign (p = 0.31). The congruent vs. incongruent sign comparison was marginally signiﬁcant at p = 0.063.
The gesture means were again very different from the rest (all
three p’s &lt; 0.001).
The inner-electrode ANOVA found a main effect of Ending
(p &lt; 0.001) and an Ending by Electrode interaction (p &lt; 0.001).
Mean amplitudes for the four conditions progressed in the predicted way, with the gesture means again very different from the
rest (all three p’s &lt; 0.001). The baseline vs. incongruent comparison
also reached signiﬁcance (p &lt; 0.05), and the baseline vs. pseudosign comparison showed a near-signiﬁcant trend (p = 0.13). The
same ordering from most negative to most positive means for
the four conditions was seen at all three levels of electrode (respective mean values for pseudo-sign, incongruent sign, congruent
sign, grooming gesture = À1.86, À1.60, À0.43, 5.05), with gesture
means being most divergent from the other three condition means
at posterior sites.
For the outer electrodes ANOVA, the results included a main effect of Ending (p &lt; 0.001) and an interaction of Ending by Electrode
(p &lt; 0.05). The familiar pattern of pseudo-sign &lt; incongruent
sign &lt; congruent sign &lt; grooming gesture was seen for all four levels of Electrode (mean values in the usual order = À1.60, À0.85,
À0.21, 3.35), but only the only differences reaching signiﬁcance
were for gesture vs. the other conditions (all three p’s &lt; 0.001).
The baseline vs. pseudo-sign difference showed a near-signiﬁcant
trend (p = 0.12). Again, gesture means showed the greatest differences from the other three condition means at posterior electrode
sites.
Finally, the ANOVA for the outermost set of electrodes found a
highly signiﬁcant effect of sentence Ending (p &lt; 0.001), as well as
</bodyText>
<page confidence="0.989173">
18
</page>
<tableCaption confidence="0.4012995">
M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24
Table 1
</tableCaption>
<figure confidence="0.953945841269841">
Summary of ANOVA results, with df = degrees of freedom, MSE = mean squared error.
400–600 ms
*
**
***
600–800 ms
F df
F
MSE
p
F df
F
MSE
p
Midline
Ending
Ending-Electrode
3, 45
9, 135
7.19
3.45
23.8
1.86
***
3,45
4.62, 69.3
26.6
7.68
22.6
5.05
***
Inner
Ending
Ending-Electrode
Electrode
3, 45
6, 90
2, 30
9.87
1.88
3.25
32.1
1.94
7.47
***
3, 45
6, 90
–
36.3
5.42
–
27.7
2.77
–
***
Outer
Ending
Ending-Electrode
Hemisphere-Electrode
3, 45
–
3, 45
8.16
–
18.7
30.1
–
4.01
3, 45
3.65, 54.8
3, 45
20.3
3.22
6.29
30.3
12.7
6.80
***
Outermost
Ending
Ending-Electrode
Electrode
Hemisphere-Electrode
3, 45
12, 180
1.40, 21.0
4, 60
7.23
2.15
5, 45
7.34
24.0
3.78
56.9
4.17
2.04, 30.6
5.84, 87.5
1.34, 20.1
4, 60
15.9
5.92
3.76
7.83
37.9
9.24
67.0
5.58
***
***
0.094
0.053
***
ns
***
***
*
*
***
***
***
ns
*
**
***
0.056
***
</figure>
<bodyText confidence="0.964250742268041">
p &lt; 0.05.
p &lt; 0.01.
p &lt; 0.001.
related to sentence Ending. An additional post hoc test was carried
out in order to investigate speciﬁcally the negative-going trend for
the pseudo-sign condition relative to baseline at the OZ site in the
200–400 ms window, seen most clearly in Fig. 4. However, this
contrast was found to be only marginally signiﬁcant in that time
window (p = 0.085).
4. General discussion
Fig. 5. Grand-average topographic maps (viewed from above, with anterior
oriented upward) for key contrasts between sentence Ending conditions over the
two indicated time intervals. Units on the scale are microvolts.
an interaction of Ending by Electrode (p &lt; 0.001). The predicted
pattern among condition means was seen again here (mean values
in same order as before: À1.63, À0.66, À0.39, 2.12). For this later
window, however, only the grooming gesture mean was signiﬁcantly different from the rest (all three p’s &lt; 0.001), but the baseline vs. pseudo-sign comparison was marginally signiﬁcant
(p = 0.085). The Ending by Electrode interaction is due to the fact
that the differences between the gesture mean and the means
associated with the other three conditions were greatest at posterior sites.
3.4. Earlier time windows
ANOVAs like those just described were also carried out for the
following time windows: from 0 to 100 ms post-stimulus onset,
100–200 ms, 200–300 ms, and 300–400 ms. For each of these
ANOVAs there was an absence of signiﬁcant effects or interactions
This study investigated sign language users’ ERP responses
when confronted with ASL sentences with four kinds of endings:
semantically congruent signs, semantically incongruent signs, phonologically legal pseudo-signs and non-linguistic grooming gestures. We hypothesized that the neurophysiological response
associated with these ending types would differ in a manner consistent with ﬁndings in analogous studies of spoken and written
language. Speciﬁcally, we predicted that the incongruent signs
would elicit negative-going waves relative to the baseline (congruent sign) condition, and that the pseudo-signs would elicit a negativity of larger magnitude than the incongruent sings. In addition,
we suggested that the non-linguistic gestures might elicit a positive-going wave, relative to baseline. While this last prediction
was more speculative than the others, the expected pattern was
in fact observed very consistently in our analysis of the data. Together, these ﬁndings lead to a number of important observations.
First, the outcome of our incongruent sign vs. congruent sign
comparison replicates earlier ﬁndings which have also indicated
that the N400 generalizes across modalities, including the visual–
manual modality of signed language (e.g. Capek et al., 2009; Kutas
et al., 1987). These ﬁndings are broadly consistent with other studies using a variety of methodologies including positron emission
tomography (PET; Corina, San Jose-Robertson, Guillemin, High, &amp;
Braun, 2003), functional magnetic resonance imaging (fMRI; Neville et al., 1998), and cortical stimulation mapping (Corina et al.,
1999), highlighting key neural processing similarities between
signed and spoken language, in spite of the obvious physical differences in the linguistic signal.
For instance, Neville et al. (1997) also found that deaf signers
exhibited an N400 response to semantically incongruent ASL
sentences, relative to congruent sentences. Like the effects in the
present study, this response was broadly distributed and had an
onset and peak that the researchers noted was somewhat later
M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24
than would be expected for written language, but consistent with
earlier studies on auditory language (Holcomb &amp; Neville, 1990,
1991). Neville et al. suggested that this delay might be due to the
fact that the recognition point of different signs will tend to vary
more than for printed language, in which all information is made
available at the same time. Capek et al. (2009) also found a relatively late N400 response to semantic incongruity in sign sentences; this bilateral and posteriorly prominent effect had an
onset of about 300 ms post-stimulus onset and peaked at about
600 ms post-stimulus onset, very much like the negativities we
have described in the present study.
These effects are somewhat different from those that have been
described in studies incorporating incongruent co-speech gestures
and other sorts of non-linguistic imagery like drawings, photographs and videos. In Wu and Coulson’s (2005) study of contextually incongruent gestures, a component described by the
researchers as a ‘‘gesture N450’’ was observed. Wu and Coulson
noted the similarity of this effect to the N450 reported by Barrett
and Rugg (1990) for second items in unrelated picture pairs relative to related picture pairs (e.g. wrench/fork vs. knife/fork), stating
(p. 659) that consistent with their own ﬁndings, ‘‘most such ‘picture’ ERP studies report a broadly distributed negativity largest
at frontal electrode sites and not evident at occipital sites (Barrett
&amp; Rugg, 1990; Holcomb &amp; McPherson, 1994; McPherson &amp; Holcomb, 1999; Sitnikova et al., 2003; West &amp; Holcomb, 2002).’’ In
contrast, the negativity reported in the present study was quite
evident at occipital sites, as can be seen clearly in Figs. 3 and 4.
A second notable ﬁnding in our study concerns deaf subjects’
ERP response in the phonologically legal pseudo-sign condition,
which was also consistent with an N400 response but was generally larger (more negative) than the negativity seen for semantically incongruent but fully lexical signs. This provides further
evidence for broad processing similarities for different linguistic
modalities, in the light of similar ﬁndings for pseudo-words in earlier studies (Bentin, 1987; Bentin et al., 1985; Hagoort &amp; Kutas,
1995). It is interesting, however, that phonologically legal pseudo-signs did not more strongly differentiate from the semantically
incongruent signs in the present study. This may be an indication
that our pseudo-signs (or some of their sub-lexical components)
are activating lexical representations to a substantial degree (cf.
Friedrich, Eulitz, &amp; Lahiri, 2006), and that at the same time, these
representations are incongruent with the sentential contexts in
which they have been presented. The pseudo-sign and incongruent
sign conditions shared another similarity in that the effects they
elicited were very prominent at occipital sites, which differs from
what has traditionally been observed in studies of word processing. Whether this is a reﬂection of the modality of expression or
other experimental factors must await further study, though results of Corina et al. (2007), discussed below, may offer some insights about useful directions such research might take.
A third set of ﬁndings, concerning the outcome related to our
non-linguistic grooming actions, is especially provocative. In contrast to the three other kinds of sentence-ﬁnal items, all of which
could be considered linguistic (i.e. as actual lexical items in two
cases, and phonologically legal lexical gaps in the third), the non-linguistic grooming actions elicited a large positivity. As noted earlier,
phonologically illegal words in ERP studies have in some cases elicited a positive-going component rather than an N400 (Holcomb &amp;
Neville, 1990; Ziegler et al., 1997). Holcomb and Neville (1990)
examined differences between pseudo-words and non-words in
the visual and auditory modalities in the context of a lexical decision
experiment. Pseudo-words accorded with phonotactic constraints
of English; visually presented non-words were composed of consonant strings and auditory non-words were words played backwards.
The researchers reported that within an early time window (150–
300 ms), auditory non-words (but not visual non-words) elicited a
</bodyText>
<page confidence="0.998123">
19
</page>
<bodyText confidence="0.999623630434783">
more negative response than pseudo-words, but only at anterior
and right hemisphere sites. In a later time window (300–500 ms),
the response to non-words was more positive for both modalities,
and like the positivity seen in the present study, this positivity was
long-lasting, continuing past the 1000 ms time-point.
Ziegler et al. (1997) examined the effects of task constraints on
the processing of visually presented words, pseudo-words and
non-words. In a letter search task, following a post-stimulus N1P2 complex, the researchers reported a negative component, peaking around 350 ms, which was larger for words and pseudo-words
than for non-words. A late positive component (LPC) was then generated that appeared to be slightly larger for non-words than for
words or pseudo-words. In a second experiment, in which subjects’
responses to the three types of stimuli were delayed, the ERP response in the 300–500 ms window was more positive to nonwords than to words and pseudo-words; responses for words
and pseudo-words did not signiﬁcantly differ. In a ﬁnal experiment
which required a semantic categorization of the target, a negative
component with a peak around 400 ms was elicited in response to
words and pseudo-words. In contrast, a striking late positive component was observed in response to non-words; this lasted from
300 ms to the end of the recording period.
Thus, across multiple studies we see that illegal non-words, relative to pseudo-words and real words, appear to elicit a large late
positivity. This positivity has sometimes been interpreted as a
P300 response (e.g. Holcomb &amp; Neville, 1990). In the present experiment, the centro-parietal distribution of the positive component
elicited in the gesture condition also corresponds to the typical distribution of the P300 component. At least three interpretations of
this effect may be relevant here. First, a P300 response is well-attested in studies making use of stimuli perceived by subjects to
be in a low-probability category (e.g. Johnson &amp; Donchin, 1980).
In our experiment, grooming gestures occurred 1/4 of the time,
rendering these non-linguistic events low-probability with respect
to the other three (linguistic) sentence ending conditions. Second,
ERP differences between non-words and words have been attributed to the fact that these non-linguistic items have little or nothing in common with lexical entries and therefore do not generate
lexical activity (cf. Rugg &amp; Nagy, 1987). Third, the ERP differences
observed between pseudo-words and non-words have also been
suggested to reﬂect a pre-lexical ﬁltering process that quickly rejects non-linguistic items based upon aberrant physical characteristics (Holcomb &amp; Neville, 1990). For example, Ziegler et al. (1997)
suggest such a categorization may be based on a spelling check in
the case of non-word consonant-string stimuli.
This last interpretation accords well with a possibility we noted
in the Introduction, that such an ERP response may be due to the
operation of a ﬁltering/rejection mechanism, allowing language
users to efﬁciently reject items in the incoming linguistic signal
that do not fall within some limits of linguistic acceptability. The
gesture stimuli in the present study, in lacking the semantic appropriateness of semantically congruent signs, the lexicality of incongruent signs, and even the phonological legality of pseudo-signs,
apparently fail to reach some ‘‘acceptability threshold,’’ causing
them to be rejected and thus dealt with during processing in a
qualitatively different way. This hypothesis also permits us to predict an answer to an interesting related question: what kind of ERP
response would be elicited by phonologically illegal non-signs in
sentence contexts like those explored in the present study? The
positivities seen in studies incorporating non-words; the crossmodality parallels we have already noted in ERP studies of spoken,
written and signed language; as well as the positive waveforms
seen in response to the non-linguistic gesture stimuli in the present study; all support the prediction that phonologically illegal
non-signs would elicit a positive-going waveform. However, conﬁrmation of this must await future research.
</bodyText>
<page confidence="0.947191">
20
</page>
<bodyText confidence="0.99859296875">
M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24
We have already alluded to the growing number of studies
which have used ERP methodology to examine the contributions
of co-speech manual gestures to the interpretation of both linguistic and non-linguistic stimuli (Holle &amp; Gunter, 2007; Kelly et al.,
2004; Ozyürek, Willems, Kita, &amp; Hagoort, 2007; Wu &amp; Coulson,
2005, 2007a, 2007b). Many of these studies have used iconic manual gestures that depict a salient visual–spatial property of concrete objects, such as their size and shape or an associated
manner of movement (but see also Cornejo et al., 2009). Collectively these studies suggest that co-speech manual gestures inﬂuence semantic representations, and that discrepancies between
gestural forms and the semantic contexts in which they occur lead
to greater processing costs on the part of language perceivers. This
in turn results in increased negativities in the time window often
associated with the classic N400 effect, observed in response to
word meanings that violate the wider semantic context (Kutas &amp;
Hillyard, 1980).
For example, Kelly et al. (2004) observed modulation of ERP responses for speech tokens that were either accompanied by matching, complementary or mismatched hand gestures. An N400-like
component was observed for mismatched gesture-speech tokens
relative to the other conditions. Wu and Coulson (2005) examined
ERPs for subjects who watched cartoons followed by a gestural
depiction that either matched or mismatched the events shown
in the cartoons. Gestures elicited an N400-like component (a socalled ‘‘gesture N450’’) that was larger for incongruent than congruent items. Ozyürek et al. (2007) recorded EEG while subjects listened to sentences with a critical verb (e.g. ‘‘knock’’) accompanied
by a related co-speech gesture (e.g. KNOCK). Verbal/gestural
semantic content either matched or mismatched the earlier part
of the sentence. The researchers noted that following the N1–P2
complex, the ERP response to mismatch conditions started to deviate from the response to the correct condition in the latency window of the P2 component, around 225–275 ms post stimulus
onset, while at around 350 ms, the mismatch conditions deviated
from the congruent condition. This was followed by a similar effect
with a peak latency somewhat later than the one usually seen for
the N400. These data were taken as evidence that that the brain
integrates both speech and gestural information simultaneously.
However, it is interesting to note that double violations (speech
and gesture) did not produce additive effects, suggesting parallel
integration of speech and gesture in this context.
In contrast, the grooming gesture condition in the present study
was not associated with any N400-like effects. We suggest that this
is due to the fact that subjects were unlikely to perceive these actions as being akin to co-speech (or co-sign) gestures, but instead
as something qualitatively different. This detection process evidently occurred quite rapidly during online processing of these
stimuli, comparable to the speed at which semantic processing
was carried out for the linguistic stimuli. The ﬁndings of a PET
study by Corina et al. (2007) may shed some additional light on
this outcome. In that study, deaf signers were found to have engaged different brain regions when processing ASL signs and selfgrooming gestures, in contrast with the hearing non-signers who
also took part in the study. Speciﬁcally, deaf signers engaged lefthemisphere perisylvian language areas when processing ASL sign
forms, but recruited middle-occipital temporal–ventral regions
when processing self-grooming actions. The latter areas are known
to be involved in the detection of human bodies, faces, and movements. The present ﬁndings add temporal precision the results of
that study, enabling us to determine when such information is rejected as non-linguistic during the course of ASL sentence
processing.
The ﬁndings of Corina et al. (2007) may also speak to the fact,
noted earlier, that the N400 effects observed in the present study
were more prominent at occipital sites than the N400 effects typ-
ically seen in analogous speech studies. The effects seen in the
present study’s gesture condition were strongest in posterior areas
as well, though in both cases, one must be cautious in making a
connection between the scalp topography of ERP effects and location of their source.4
Finally, an alternative interpretation of the positivity seen in the
grooming gesture condition in the present study is that it is due to
subjects’ interpreting these non-linguistic ﬁnal items as missing
information, i.e. as the absence of a ﬁnal item, rather than a ﬁnal
item which is present yet enigmatic. While this cannot be entirely
ruled out, it should be noted that the pseudo-signs could also
potentially be considered ‘‘semantically void’’ items, but the responses to the pseudo-signs (which have a discernible linguistic
structure consistent with that of real ASL lexical items) and the
gestures (which do not) were qualitatively different. Relative to
the baseline condition, no signiﬁcant positivity was seen in any
time window for the pseudo-signs, and no signiﬁcant negativity
was seen in any time window for the grooming gestures.
</bodyText>
<sectionHeader confidence="0.976197" genericHeader="method">
5. Conclusion
</sectionHeader>
<bodyText confidence="0.996943375">
To the best of our knowledge, this is the ﬁrst ERP study of sign
language users that investigates sentential processing in such a
wide a range of lexical/semantic contexts. Consistent with previous
research on both spoken and signed language, we found that ASL
sentences ending with semantically incongruent signs were associated with signiﬁcant N400-like responses relative to the baseline
condition, in which sentences ended with semantically congruent
signs. Furthermore, we found that phonologically legal pseudosign sentence endings elicited an N400-like effect that was somewhat stronger than the response to the semantically incongruent
signs; this is consistent with existing work on spoken language,
but represents a new ﬁnding for signed language. In contrast to
the incongruent sign and pseudo-sign conditions, grooming actions elicited a very large positive-going wave; this is also a new
ﬁnding, and complements earlier work on spoken language. The
fact that our results largely parallel those seen in analogous ERP
studies of spoken language constitutes strong evidence that highlevel linguistic processing shows remarkable consistency across
modalities. Moreover, our results offer important new information
about the relationship between sign and action processing, particularly the topography and timing of the processes that are
involved.
</bodyText>
<sectionHeader confidence="0.926119" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.992706181818182">
We thank the staff and students at Gallaudet University for
helping make this study possible, and Kearnan Welch and Deborah
Williams for their assistance in data collection. We also thank two
anonymous reviewers for valuable feedback concerning the presentation of our results. This work was supported in part by Grant
NIH-NIDCD 2ROI-DC03099-11, awarded to David Corina.
Appendix A. List of stimulus items
Table A1 lists all 120 sentence frames and three of the corresponding endings for each sentence: the semantically congruent
signs, semantically incongruent signs and pseudo-signs. In the
fourth class of endings, the gesture stimuli, the sign performer
was seen making brief grooming actions such as head scratching,
eye rubbing or passing her ﬁngers through her hair. To create
</bodyText>
<page confidence="0.962975">
4
</page>
<bodyText confidence="0.998897666666667">
It should also be noted that the effects seen in the present study were for gestures
in a sentence context, while in Corina et al. (2007) the sign and gesture stimuli were
seen in isolation.
</bodyText>
<page confidence="0.990866">
21
</page>
<bodyText confidence="0.972710875">
M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24
120 unique gesture stimuli, the actions were performed with differences in the number and conﬁguration of hands or ﬁngers used,
the location of the body involved, and so on. Also shown in the
rightmost two columns of the table are the ‘‘correct’’ and ‘‘incor-
rect’’ word choices for the occasional quiz items. A number sign
(#) preceding an item means that item was ﬁngerspelled. Many
of these sentences were adapted from the English-language stimuli
used in Johnson and Hamm (2000).
</bodyText>
<figure confidence="0.986505833333333">
Table A1
FRAME
Semantically
congruent
Semantically
incongruent
Pseudo-sign
Correct
Incorrect
1
2
3
4
5
6
7
8
9
</figure>
<page confidence="0.708325">
10
</page>
<table confidence="0.9505062">
BOY SLEEP IN HIS
HEAR BARK BARK LOOK RIGHT PRO
MAN PRO CARPENTER BUILD
DOOR PRO LOCK LOOK-FOR
WATER+CL:pond PRO CL:many-across
MOTORCYCLE BROKE-DOWN-REPEATEDLY FINALLY BOUGHT NEW
PRO3 BECOME-ILL SICK CAN’T GO-TO
MY HOUSE LIGHTS BLACKOUT-POW CL:set-up-around
DOG ANGRY CHASE
#PATIENT SIT-HABITUAL ANALYZE-PRO1, PRO3
</table>
<figure confidence="0.9662131">
LEMON
SECRET
HAIRCUT
NOSE
WORD
GHOST
BOTTLE
ELEPHANT
EASTER
FRANCE
BARK.A’
HOUSE.V
NOSE.V’’
G_2H (contact 2x)
GHOST.S
WRIST.CS (open &amp; close)
NOSE.5 (2x)
KNOCK.8
KNOCK.G
MONEY.X
sleep
hear
build
lock
water
buy
sick
lights
angry
sit
ﬁght
touch
eat
hat
wine
wear
one
bath
green
drive
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
</figure>
<page confidence="0.932894666666667">
31
32
33
</page>
<table confidence="0.988053254545454">
BOY STOLE
KIDS PRO3 CL:go-out-in-group WATCH
DAUGHTER MY LIKE READ
PLUMBER HIS JOB FIX
PRIEST WORK THERE
STUDENTS WRITE-TOOK
POLICEMAN PRO CATCH
MOTHER HAVE 3
DARK ROOM PRO FOR-FOR DEVELOP
HUNTER SHOOT-AT KILL
AIRPLANE SEAT FULL CL:mass 300
PIRATE PRO3 HUNT WHERE
GOLFER STROKE CL:ball-ﬂy-across WRONG CL:ball-into-water
SPRING THIS YEAR MINE TAX HEAVY
KING-PRO FALL-IN-LOVE
TELESCOPE I TELESCOPE-FOCUS SEE
#APOLLO ROCKET-MAN GO-TO TOUCH
FARMER NOW CL:milk (verb)
SCIENTIST INVENT++ HARD
MURDERER PRO3 CAUGHT PUT-INTO
FATHER COMMAND SON GO CLEAN
NEW YORK TIMES NEWSPAPER ITS TENDENCY I READ MANY
JUDGE PRO3 ARGUMENT LISTEN LISTEN THINK-IT-OVER READY
MAKE
MY BIRTHDAY SOON COME MY MOM PRO3 BAKE
#ZOO ITS-TENDENCY HAVE MANY VARIOUS
MEN PRO3 CL:group-up GO-OUT CHUG
I JOIN ARMY I SHOPPING CLOTHES NEED SHIRT PANTS
TEA DRINK TASTE BITTER NEED
PANTS CL:pull-on CL:ﬁt-loose NEED
I CALL HOTEL RESERVE
WOMAN CL:lie-down SUNNING THERE
MUSEUM I LOOK-AT HALLWAY LOOK-AT WOW BEAUTIFUL
#OFFICE MAX I ENTER SHOP-AROUND PRO #FAX PRO COMPUTER
PRO
MORNING BOY PRO CL:get-on-bike RIDE-BIKE CL:deliver
MY LIVING ROOM EMPTY NONE
MONEY FATHER GAVE-ME I PUT-IN WHERE
POPCORN I MAKE FINISH I CL:pour-over
I THIRST-FOR WATER NEED
ME ENTER BDRM I SPOT MOUSE CL:sneak-under
FROG CL:tongue-stick-out-retract GULP
GRASS CL:thereabout I WALK CL:walk-around WET OH
GIRLFRIEND GO FURNITURE STORE BUY BRING
BIRD EAT SLEEP WHERE
WOMAN PRO3 TOP ATHLETE PARTICIPATE
COOK PRO3 EXPERT THEIRS COOKING
I ENTER HOUSE I HEAR TICK-TICK AHA! PRO3
VW BUS I BUY DRIVE WRONG BROKEDOWN
OUTSIDE GIRL CL:lie-down OBSERVE
LAWYER CL:sit-down-with-someone DISCUSS WITH PRO
I INFORM SISTER PLEASE PACK BRING
MAIL/LTR I GOT OPEN-ENVELOPE I GOT
MAN PRO3 I SEE PRO3 FIX FINISH CONSTRUCT
TARA PRO WANT MAKE PIE NEED BUY
</table>
<figure confidence="0.999076178181818">
BED
DOG
HOUSE
KEY
FISH
CAR
WORK
CANDLE
CAT
PSYCHOLOGIST
MONEY
(theater) PLAY
BOOK
TOILET
CHURCH
EXAM
THIEF
CHILDREN
PICTURE
DUCK
PEOPLE
GOLD
POND
DEBT
QUEEN
STAR
MOON
COW
EXPERIMENT
JAIL
BATHROOM
ARTICLE
DECISION
INJURY
WEALTH
VEGETABLE
APPLE
BEANS
RING
MUSIC
REASON
SLED
INTERNET
PAPER
NAPKIN
PICKLE
CHOCOLATE
EXAMPLE
RUBBISH
BANANA
PENCIL
TEACHER
LOBSTER
MATHEMATICS
EARRINGS
DOLL
PALM_up.5
BOOK.openB on chin
X (moving left to right)
BEANS.C
EXAM_2H.openF
MUSIC.V’’
REASON.5’’
PIC.X
INTERNET.H’’
PAPER.3’
GOLD.F
EMAIL.L’’
NOSE(vb).openB
EXAMPLE.Z
STAR_2H.X
PENNY.ﬂat0’
MONEY.C’
DANCE.openH
BUTTERFLY. bent5
JAIL.T
WHERE.bent4
VOICE-UP.1
WHITE (on neck)
boy
watch
read
ﬁx
work
students
police
mother
room
kill
full
pirate
ball
tax
love
see
touch
milk
scientist
catch
son
newspaper
listen
mouse
help
steal
burn
shower
hat
student
father
bread
love
hard
circle
tennis
card
try
hear
dry
hat
cover
draw
aunt
chair
taste
CAKE
ANIMAL
BEER
#BOOTS
SUGAR
BELT
ROOM
BEACH
PAINTING
PRINTER
GIRAFFE
MAGAZINE
SAUSAGE
HAMBUR-GER
DANCE
LUNGS
BUTTERFLY
LANGUAGE
CABBAGE
CAP
XMAS.5’’
TALL.3
TIME.openB’’
DANCE. openH’’
TIRED_2H.H
MATH_2H.B
DAMAGE.Y
BEACH.L
MONEY.5’’
KNOW.20
mom
zoo
men
army
drink
pants
hotel
sun
beautiful
computer
cousin
arm
rats
hill
cut
arm
mall
mouse
bad
wheel
NEWSPAPER
COUCH
WALLET
BUTTER
CUP
DRESSER
BUG
RAIN
Table
NEST
RACE
DESSERT
CLOCK
ENGINE
CLOUDS
CLIENT
BOX
CHECK
BRIDGE
FRUIT
PLANE
SKIN
#DORM
SOCKS
HOCKEY
GOVERNMENT
#TRUCK
MEMORY
SCARED-NESS
BICYCLE
SMOKING
KEYBOARD
SHIRT
SHELF
SETUP
DOG
GIRL
EARTH
GRANDFATHER
T.V.
MONEY.IL
NOSE.X
BUTTER.1
JUMP.X
CORNER_2H.1
BUG.openB
KNOW.5’’
MORE.B
BIKE.4
CHANGE.4
DINNER.9
TIME.9
ENGINE.B (palms in)
LOCK.1
CHECK.2
MOVIE.V’’
EGG.7
LET.8
FRUIT.3
ENGINE.9
bike
empty
father
popcorn
water
mouse
frog
wet
store
sleep
woman
cook
house
drive
girl
lawyer
sister
mail
man
pie
tiger
yellow
nephew
cigarette
hat
cookie
square
high
mouse
move
shell
breathe
meal
eat
boy
dream
uncle
bird
dream
roof
</figure>
<page confidence="0.7290435">
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
</page>
<figure confidence="0.799793946428571">
50
51
52
53
54
55
56
57
58
59
60
61
62
63
Quiz items
(continued on next page)
22
M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24
Table A1 (continued)
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
</figure>
<page confidence="0.959467105263158">
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
</page>
<figure confidence="0.9843117">
120
FRAME
Semantically
congruent
Semantically
incongruent
Pseudo-sign
Quiz items
Correct
Incorrect
</figure>
<table confidence="0.938841016129032">
THIS WKND I GO HIKE I LOOK-AT WOW BEAUTIFUL
HOMEWORK I WRITE CL:glass-breaking I LOOK-AT
PLANT CL:branching I LOOK PRO3 OH
MOM CL:walk-by CL:pick-up
BUILDING PRO3 CRUMBLE OH SEEM
SCHOOL THERE BOY TROUBLE MUST GO-TO
COLLEGE I ENTER AUDIENCE CL:sit-down WATCH GOOD
GIRL PRO3 THIRSTY WANT DRINK
I HUNGRY WANT EAT I GO-TO
CAR CL:car-stalls RAN-OUT
#STEAK MAN EAT TASTE-FUNNY NEED SEASON SALT
MOVIE DIRECTOR MAKE MOVIE WANT SEEK INTERVIEW GOOD
DR PRO3 SELF PLASTIC SURGEON HIS SPECIALTY
STAMP I SICK-OF ANNUAL INCREASE
BOY SICK I LOOK CHECK OH HURT
COURT I GO-TO CL:sit-down FACE
DOWNTOWN LAUNDROMAT I GO ARRIVE I ENTER OH I NEED
SCHOOL CLOSED NEXT WEEK STUDENTS GO HOME (lowered
eyebrows)
RESTAURANT PRO3 FIRST TIME I ENTER ME FEAST WHOA
DELICIOUS
PRO WOMAN SHORT THIN PRO3 EXPERT PRO3 CL:drink-shot
OUTSIDE THERE WEATHER BAD THERE TORNADO PRO3 HIT++
DESTROY++
BOY PRO3 NOT-WANT GO SCHOOL PRO3 MOM SAY GO SCHOOL
MUST TAKE
A-LONG-TIME-AGO STREET CL:ﬂat-surface BUMPY-SURFACE
GIRL PRO CL:foot-limping SHOE CL:shoe-taken-off OH
NEW HOME I MOVE-IN OH EMPTY NEED
DOWNTOWN MOVIE THERE I WATCH ANNOYED PEOPLE PRO3
RUDE CHAT++ IN
LAST_NIGHT I FEEL-LIKE SIT WATCH FIRE WRONG GONE
THIS MORNING CHILDREN I GO-TO PARK I LOOK-AROUND OH
NONE
FATHER GONE 2 MONTHS CAME HOME I LOOK-STUNNED GROW
KNOW-THAT SMOKING MANY YEARS CAUSE (nod)
RIVER I STAND LOOK-ACROSS OH MUST
BICYCLE TIRE CL:ﬂat NEED
CAR CL:gone-by-fast SPEED WRONG CL:pull-over GET
THIS SUMMER VACATION I WANT TRAVEL OVER-THERE
X-MAS GIFT I WRAP I LOOK-AROUND NONE
DAUGHTER PRO BECOME-SICK I TAP COME I GIVE
WATER DRINK TOO-WARM PRO CL:cup MUST PUT-IN
RESTAURANT PRO FANCY ENTER WANT MUST DRESS-UP DRESS
COAT
BATHROOM I ENTER MIRROR I LOOK-AT PUZZLED DIRTY
I TYPE++ ALL-NIGHT WRONG EYES CL:eyes-fuzzy FROM
CHILDREN PRO3 WANT ACTIVITY DIFF PRO3 WANT PAINTING PRO3
WANT BASKETBALL PRO3 WANT
DAUGHTER PRO GO-TO DR TODAY NEED PRO3 WRONG I SICK
CANCEL
RUN EVERY-MORNING RELISH I NOW MY FEET HURT BUY NEW
GRAD PARTY COMING-UP-SOON I NEED SHOP THINGS FOOD CAKE
#HILTON SELF FANCY (nod)
MY SON PRO I INFORM HIM TONITE EAT FINISH PRO3 MUST GO
WASH
TONITE I DO-ERRANDS MUST I COOK, VACUUM, CLEAN
I LOOK-AROUND NOTICE++ CHILDREN NOWADAYS INCREASINGLY
NEED
EVERY-FRI NITE MY FAMILY LIKES WATCH BASEBALL
BUGS CL:hovering CL:biting-me I LOOK OH
GIRL PRO HER TOOTH BROKE GO SEE
</table>
<sectionHeader confidence="0.883285166666667" genericHeader="method">
RESTAURANT THERE ITALIAN ITS FOOD DELICIOUS PIZZA RAVIOLI
KNOW-THAT JULY MONTH ITS-TENDENCY HOT
APPLICATION YOU FILL-OUT FINISH SIGN (nod)
BUY BOOK NOT-NECESSARY SIMPLY GO-TO
EVERY-MORNING MY FAMILY EAT EGGS #HASH BROWN
SQUIRREL ITS FOOD BOX EMPTIED-OUT I LOOK-THERE OH RANOUT
</sectionHeader>
<figure confidence="0.99413143205575">
MOUNTAIN
WINDOW
SEED
BABY
BOMB
PRINCIPAL
LECTURE
WATER
RESTAURANT
GAS
PEPPER
ACTOR(S)
BREASTS
PRICE
THROAT
JUDGE
COINS
HOLIDAY
PIZZA
FOOTBALL
METAL
BUS
IRISH
CATERPILLAR
TREE
LIGHT
Table
SHEEP
JESUS
MEAT
VERB
COLOR
EGG
#SALE
CLOTHES
SODA
WINDOW.V
GRASSHOPPER.bent5
BABY.1
DAMAGE.V’’
PRINCIPAL.5
LECTURE.I
TELL-ME.ﬂatO
ARIZONA.4
EAR.A
OPEN.F
ACTOR.F
JAWS_2H.H
ARCHITECTURE.4
THROAT.H
HAMMER_2H.S
DRESS.bentV
GAS.3
EAT.openB (palm in)
hike
break
plant
mom
building
boy
watch
thirsty
hungry
car
steak
movie
doctor
stamp
sick
court
arrive
home
paint
drink
north
dad
yellow
wheel
drink
sad
small
head
rabbit
hat
wood
dream
free
mouse
write
country
FOOD
DOOR
TIME.1I
restaurant
pen
WHISKEY
SCHOOL
WATCH (n)
INTERVIEW
TIRE.1I
EARS_2H.X (2x)
drink
bad
eat
good
TEST
RADIO
STREET_2H.F
school
mouse
BRICK
STONE
FURNITURE
AUDIENCE
#STEAK
BOSS
FARM
FEVER
ROCK.5’’
FARM.F
FEVER.5
WOOD.I
street
shoe
home
rude
eye
hair
nose
tall
WOOD
SWING/SLIDE
IDEA
SUMMARY
SUMMARY.H
BEARD.H’’
ﬁre
park
air
mouse
BEARD
CANCER
BOAT
PUMP
TICKET
EUROPE
TAPE
MEDICINE
ICE
TIE
PROBLEM
FLOWER
LAW
COMMUNITY
WORM
CLOSET
HYPOCRITE
ELEVATOR
TENT
RAINBOW
CAFETERIA.bent V’’
NAME_2H.^20
TICKET.split5
WORM.S
GRASS.K
ANIMAL.B
ELEVATOR.open8
WATER.1I
FLAG.D
HOTEL.E
grow
smoke
river
tire
speed
travel
gift
sick
warm
dress
mix
drink
hat
yellow
bread
use
lake
sharp
sad
snow
FACE
COMPUTER
BASEBALL
ALARM
HANDBAG
HEADACHE
COMPUTER.3
DENTIST_2H.S
APPOINTMENT_2H.bentV
mirror
eyes
children
sandwich
toes
lions
APPOINTMENT
PERFUME
horizontal wiggle.V
daughter
father
SHOES
CHAMPAGNE
HOTEL
DISHES
JAPAN
HEARING-AID
TAIL
VALLEY
CLOTHES.1
CLOTHES.1 (no contact)
EUROPE.X
FLOOR.^20
feet
shop
Hilton
wash
ears
wait
Microsoft
throw
FLOOR
GLASSES
WINK
TRASH
GLASSES.1
CONTACT.V
clean
children
ride
men
GAME
MOSQUITO
DENTIST
SPAGHETTI
MONTH
NAME
LIBRARY
BACON
NUT
PLUG
PRIEST
POSSUM
CONTACT-LENS
CRACKER
COP
FRIDGE
MOUSTACHE
MASK
CHEEK.G
DENTIST.I’’
CONTACT-LENS.A
MONTH.A
PRACTICE.C
LIBRARY.V’’
6_2H (horizontal wiggle)
NUT.5’’
RAINBOW.H
watch
bugs
tooth
pizza
hot
sign
book
eat
food
swim
bread
nose
taco
plain
sleep
lamp
drink
idea
M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24
References
</figure>
<reference confidence="0.999728073170731">
Arbib, M. A. (2005). Interweaving protosign and protospeech: Further developments
beyond the mirror. Interaction Studies: Social Behaviour and Communication in
Biological and Artiﬁcial Systems, 6, 145–171.
Arbib, M. A. (2008). From grasp to language: Embodied concepts and the challenge
of abstraction. Journal of Physiology-Paris, 102, 4–20.
Barrett, S. E., &amp; Rugg, M. D. (1989). Event-related potentials and the semantic
matching of faces. Neuropsychologia, 27, 913–922.
Barrett, S. E., &amp; Rugg, M. D. (1990). Event-related potentials and the semantic
matching of pictures. Brain and Cognition, 14, 201–212.
Bentin, S. (1987). Event-related potentials, semantic processes, and expectancy
factors in word recognition. Brain and Language, 31, 308–327.
Bentin, S., McCarthy, G., &amp; Wood, C. C. (1985). Event-related potentials, lexical
decision, and semantic priming. Electroencephalography &amp; Clinical
Neurophysiology, 60, 353–355.
Bobes, M. A., Valdés-Sosa, M., &amp; Olivares, E. (1994). An ERP study of expectancy
violation in face perception. Brain and Cognition, 26, 1–22.
Brown, C., &amp; Hagoort, P. (1993). The processing nature of the N400: Evidence from
masked priming. Journal of Cognitive Neuroscience, 5, 34–44.
Capek, C. M., Grossi, G., Newman, A. J., McBurney, S. L., Corina, D., Roeder, B., et al.
(2009). Brain systems mediating semantic and syntactic processing in deaf
native signers: Biological invariance and modality speciﬁcity. Proceedings of the
National Academy of Sciences of the United States of America, 106, 8784–8789.
Chao, L. L., Nielsen-Bohlman, L., &amp; Knight, R. T. (1995). Auditory event-related
potentials dissociate early and late memory processes. Electroencephalography
&amp; Clinical Neurophysiology, 96, 157–168.
Corballis, M. C. (2009). The evolution of language. Annals of the New York Academy of
Sciences, 1156, 19–43.
Corina, D. P., McBurney, S. L., Dodrill, C., Hinshaw, K., Brinkley, J., &amp; Ojemann, G.
(1999). Functional roles of Broca’s area and SMG: Evidence from cortical
stimulation mapping in a deaf signer. NeuroImage, 10, 570–581.
Corina, D., Chiu, Y.-S., Knapp, H., Greenwald, R., San Jose-Robertson, L., &amp; Braun, A.
(2007). Neural correlates of human action observation in hearing and deaf
subjects. Brain Research, 1152, 111–129.
Corina, D. P., &amp; Knapp, H. P. (2006). Psycholinguistic and neurolinguistic
perspectives on sign languages. In M. J. Traxler &amp; M. A. Gernsbacher (Eds.),
Handbook of psycholinguistics (2nd ed., pp. 1001–1024). San Diego, CA:
Academic Press.
Corina, D. P., San Jose-Robertson, L., Guillemin, A., High, J., &amp; Braun, A. R. (2003).
Language lateralization in a bimanual language. Journal of Cognitive
Neuroscience, 15, 718–730.
Corina, D., Grosvald, M., &amp; Lachaud, C. (2011). Perceptual invariance or orientation
speciﬁcity in American Sign Language? Evidence from repetition priming for
signs and gestures. Language and Cognitive Processes, 26, 1102–1135.
Cornejo, C., Simonetti, F., Ibáñez, A., Aldunate, N., López, V., &amp; Ceric, F. (2009).
Gesture and metaphor comprehension: Electrophysiological evidence of crossmodal coordination by audiovisual stimulation. Brain and Cognition, 70, 42–52.
Delorme, A., &amp; Makeig, S. (2004). EEGLAB: An open source toolbox for analysis of
single-trial EEG dynamics. Journal of Neuroscience Methods, 134, 9–21.
Emmorey, K. (2002). Language, cognition, and the brain: Insights from sign language
research. Mahwah, NJ: Lawrence Erlbaum.
Emmorey, K., Xu, J., Gannon, P., Goldin-Meadow, S., &amp; Braun, A. (2010). CNS
activation and regional connectivity during pantomime observation: No
engagement of the mirror neuron system for deaf signers. NeuroImage, 49,
994–1005.
Friedrich, C., Eulitz, C., &amp; Lahiri, A. (2006). Not every pseudoword disrupts word
recognition: An ERP study. Behavioral and Brain Functions, 2, 36.
Friederici, A. D. (2002). Towards a neural basis of auditory sentence processing.
Trends in Cognitive Sciences, 6, 78–84.
Frishberg, N. (1987). Ghanaian Sign Language. In J. Van Cleve (Ed.), Gallaudet
encyclopaedia of deaf people and deafness. New York: McGraw-Gill Book
Company.
Ganis, G., &amp; Kutas, M. (2003). An electrophysiological study of scene effects on
object identiﬁcation. Cognitive Brain Research, 16, 123–144.
Ganis, G., Kutas, M., &amp; Sereno, M. I. (1996). The search for ‘‘common sense’’: An
electrophysiological study of the comprehension of words and pictures in
reading. Journal of Cognitive Neuroscience, 8, 89–106.
Gentilucci, M., &amp; Corballis, M. (2006). From manual gesture to speech: A gradual
transition. Neuroscience &amp; Biobehavioral Reviews, 30, 949–960.
Goldin-Meadow, S. (2003). Hearing gestures: How our hands help us think.
Cambridge, MA: Harvard University Press.
Greenhouse, W. W., &amp; Geisser, S. (1959). On methods in the analysis of proﬁle data.
Psychometrika, 24, 95–112.
Hagoort, P., &amp; Brown, C. (1994). Brain responses to lexical ambiguity resolution and
parsing. In L. Frazier, J. Clifton Charles, &amp; K. Rayner (Eds.), Perspectives in
sentence processing (pp. 45–80). Hillsdale, NJ, UK: Lawrence Erlbaum Associates.
Hagoort, P., &amp; Kutas, M. (1995). Electrophysiological insights into language deﬁcits.
In F. Boller &amp; J. Grafman (Eds.), Handbook of neuropsychology (pp. 105–134).
Amsterdam: Elsevier.
Hagoort, P., &amp; van Berkum, J. (2007). Beyond the sentence given. Philosophical
Transactions of the Royal Society of London. Series B: Biological Sciences, 362,
801–811.
Holcomb, P. J., &amp; McPherson, W. B. (1994). Event-related brain potentials reﬂect
semantic priming in an object decision task. Brain and Cognition, 24, 259–276.
</reference>
<page confidence="0.976785">
23
</page>
<reference confidence="0.999633453488373">
Holcomb, P. J., &amp; Neville, H. J. (1990). Auditory and visual semantic priming in
lexical decision: A comparison using event-related brain potentials. Language
and Cognitive Processes, 5, 281–312.
Holcomb, P. J., &amp; Neville, H. J. (1991). Natural speech processing: An analysis using
event-related brain potentials. Psychobiology, 19, 286–300.
Holle, H., &amp; Gunter, T. C. (2007). The role of iconic gestures in speech
disambiguation: ERP evidence. Journal of Cognitive Neuroscience, 19,
1175–1192.
Johnson, B. W., &amp; Hamm, J. P. (2000). High-density mapping in an N400 paradigm:
Evidence for bilateral temporal lobe generators. Clinical Neurophysiology, 111,
532–545.
Johnson, R., Jr, &amp; Donchin, E. (1980). P300 and stimulus categorization: Two plus
one is not so different from one plus one. Psychophysiology, 17, 167–178.
Kegl, J., Senghas, A., &amp; Coppola, M. (1999). Creation through contact: Sign language
emergence and sign language change in Nicaragua. In M. DeGraff (Ed.),
Language creation and language change: Creolization, diachrony, and
development (pp. 179–237). Cambridge MA: MIT Press.
Kelly, S. D., Kravitz, C., &amp; Hopkins, M. (2004). Neural correlates of bimodal speech
and gesture comprehension. Brain and Language, 89, 243–260.
Kim, A., &amp; Osterhout, L. (2005). The independence of combinatory semantic
processing: Evidence from event-related potentials. Journal of Memory and
Language, 52, 205–225.
Kim, A., &amp; Pitkänen, I. (submitted for publication). Dissociation of ERPs to structural
and semantic processing difﬁculty during sentence-embedded pseudoword
processing.
Kutas, M., &amp; Hillyard, S. A. (1980). Reading senseless sentences: Brain potentials
reﬂect semantic incongruity. Science, 207, 203–208.
Kutas, M., &amp; Hillyard, S. A. (1984). Brain potentials during reading reﬂect word
expectancy and semantic association. Nature, 307, 161–163.
Kutas, M., Neville, H. J., &amp; Holcomb, P. J. (1987). A preliminary comparison of the
N400 response to semantic anomalies during reading, listening, and signing.
Electroencephalography and Clinical Neurophysiology, Supplement, 39, 325–330.
Liddell, S. K. (2003). Grammar, gesture, and meaning in American Sign Language.
Cambridge, UK: Cambridge University Press.
Lopez-Calderon, J., &amp; Luck, S. (in press). ERPLAB. Plug-in for EEGLAB. In development
at the Center for Mind and Brain, University of California at Davis.
MacSweeney, M., Campbell, R., Woll, B., Giampietro, V., David, A. S., McGuire, P. K.,
et al. (2004). Dissociating linguistic and nonlinguistic gestural communication
in the brain. NeuroImage, 22, 1605–1618.
McPherson, W. B., &amp; Holcomb, P. J. (1999). An electrophysiological investigation of
semantic priming with pictures of real objects. Psychophysiology, 36, 53–65.
Meir, I., Sandler, W., Padden, C., &amp; Aronoff, M. (2010). Emerging sign languages. In M.
Marschark &amp; P. Spencer (Eds.). Oxford handbook of deaf studies, language, and
education (Vol. 2). New York: Oxford University Press.
Morford, J. P., &amp; Kegl, J. A. (2000). Gestural precursors to linguistic constructs: How
input shapes the form of language. In D. McNeill (Ed.), Language and gesture
(pp. 358–387). Cambridge, UK: Cambridge University Press.
Neville, H. J., Bavelier, D., Corina, D., Rauschecker, J., Karni, A., Lalwani, A., et al.
(1998). Cerebral organization for language in deaf and hearing subjects:
Biological constraints and effects of experience. Proceedings of the National
Academy of Sciences of the United States of America, 95, 922–929.
Neville, H. J., Coffey, S. A., Lawson, D. S., Fischer, A., Emmorey, K., &amp; Bellugi, U. (1997).
Neural systems mediating American Sign Language: Effects of sensory
experience and age of acquisition. Brain and Language, 285–308.
Neville, H. J., Nicol, J. L., Barss, A., Forster, K. I., &amp; Garrett, M. F. (1991). Syntactically
based sentence processing classes: Evidence from event-related brain
potentials. Journal of Cognitive Neuroscience, 3, 151–165.
Nigam, A., Hoffman, J. E., &amp; Simons, R. F. (1992). N400 and semantic anomaly with
pictures and words. Journal of Cognitive Neuroscience, 4, 15–22.
Osterhout, L., &amp; Holcomb, P. (1992). Event-related brain potentials elicited by
syntactic anomaly. Journal of Memory and Language, 31, 785–806.
Ozyürek, A., Willems, R. M., Kita, S., &amp; Hagoort, P. (2007). On-line integration of
semantic information from speech and gesture: Insights from event-related
brain potentials. Journal of Cognitive Neuroscience, 19, 605–616.
Pratarelli, M. E. (1994). Semantic processing of pictures and spoken words:
Evidence from event-related brain potentials. Brain and Cognition, 24, 137–157.
Rizzolatti, G., &amp; Arbib, M. A. (1998). Language within our grasp. Trends in
Neurosciences, 21, 188–194.
Rugg, M. D., &amp; Nagy, M. E. (1987). Lexical contribution to non-word-repetition
effects: Evidence from event-related potentials. Memory and Cognition, 15,
473–481.
Senghas, A. (2005). Language emergence. Clues from a new Bedouin sign language.
Current Biology, 15, 463–465.
Sitnikova, T., Holcomb, P. J., Kiyonaga, K. A., &amp; Kuperberg, G. R. (2008). Two
neurocognitive mechanisms of semantic integration during the comprehension
of real-world events. Journal of Cognitive Neuroscience, 20, 2037–2057.
Sitnikova, T., Kuperberg, G., &amp; Holcomb, P. J. (2003). Semantic integration in videos
of real-world events: An electrophysiological investigation. Psychophysiology,
40, 160–164.
Tomasello, M. (2005). Constructing a language: A usage-based theory of language
acquisition. Cambridge, MA: Harvard University Press.
Van Petten, C., &amp; Rheinfelder, H. (1995). Conceptual relationships between spoken
words and environmental sounds: Event-related brain potential measures.
Neuropsychologia, 33, 485–508.
West, W. C., &amp; Holcomb, P. J. (2002). Event-related potentials during discourse-level
semantic integration of complex pictures. Cognitive Brain Research, 13, 363–375.
</reference>
<page confidence="0.965518">
24
</page>
<reference confidence="0.997456181818182">
M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24
Wilcox, S. (2004). Gesture and language: Cross-linguistic and historical data from
signed languages. Gesture, 4, 43–73.
Wu, Y. C., &amp; Coulson, S. (2005). Meaningful gestures: Electrophysiological indices of
iconic gesture comprehension. Psychophysiology, 42, 654–667.
Wu, Y. C., &amp; Coulson, S. (2007a). How iconic gestures enhance communication: An
ERP study. Brain and Language, 101, 234–245.
Wu, Y. C., &amp; Coulson, S. (2007b). Iconic gestures prime related concepts: An ERP
study. Psychonomic Bulletin &amp; Review, 14, 57–63.
Ziegler, J. C., Besson, M., Jacobs, A. M., Nazir, T. A., &amp; Carr, T. H. (1997). Word,
pseudoword, and nonword processing: A multitask comparison using eventrelated brain potentials. Journal of Cognitive Neuroscience, 9, 758–775.
</reference>
</variant>
</algorithm>

<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M A Arbib</author>
</authors>
<title>Interweaving protosign and protospeech: Further developments beyond the mirror.</title>
<date>2005</date>
<journal>Interaction Studies: Social Behaviour and Communication in Biological and Artiﬁcial Systems,</journal>
<volume>6</volume>
<pages>145--171</pages>
<contexts>
<context position="3029" citStr="Arbib, 2005" startWordPosition="449" endWordPosition="450">he hands and arms, are also used in a wide range of other common everyday behaviors that include non-linguistic actions such a reaching and grasping, waving, and scratching oneself, as well gesticulations that accompany speech (i.e. co-speech gestures) or serve non-sign language deictic functions, such as pointing. The formal relationship between signed languages and human gestural actions is of considerable interest to a range of disciplines. Linguists, psychologists and cognitive scientists have proposed a critical role for manual gesture in the development and evolution of human languages (Arbib, 2005, 2008; Gentilucci &amp; Corballis, 2006; Rizzolatti &amp; Arbib, 1998; Tomasello, 2005; Wilcox, 2004). Re⇑ Corresponding author. Address: Department of Neurology, University of California at Irvine, 101 The City Drive South, Bldg. 53, Room 204, Orange, CA 928684280, United States. Fax: +1 714 456 1697. E-mail address: m.grosvald@uci.edu (M. Grosvald). 0093-934X/$ - see front matter Ó 2012 Elsevier Inc. All rights reserved. doi:10.1016/j.bandl.2012.01.005 cently, linguists have documented compelling evidence that the development of nascent sign languages derives from idiosyncratic gestural and pantomi</context>
</contexts>
<marker>Arbib, 2005</marker>
<rawString>Arbib, M. A. (2005). Interweaving protosign and protospeech: Further developments beyond the mirror. Interaction Studies: Social Behaviour and Communication in Biological and Artiﬁcial Systems, 6, 145–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Arbib</author>
</authors>
<title>From grasp to language: Embodied concepts and the challenge of abstraction.</title>
<date>2008</date>
<journal>Journal of Physiology-Paris,</journal>
<volume>102</volume>
<pages>4--20</pages>
<marker>Arbib, 2008</marker>
<rawString>Arbib, M. A. (2008). From grasp to language: Embodied concepts and the challenge of abstraction. Journal of Physiology-Paris, 102, 4–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Barrett</author>
<author>M D Rugg</author>
</authors>
<title>Event-related potentials and the semantic matching of faces.</title>
<date>1989</date>
<journal>Neuropsychologia,</journal>
<volume>27</volume>
<pages>913--922</pages>
<contexts>
<context position="8490" citStr="Barrett &amp; Rugg, 1989" startWordPosition="1261" endWordPosition="1264">t &amp; Kutas, 1995; Ziegler, Besson, Jacobs, Nazir, &amp; Carr, 1997). This may reﬂect the operation of some kind of ﬁltering mechanism during online processing, through which language users are able to quickly reject forms that lie beyond a certain point of acceptability, or plausibility, during the ongoing processing of the incoming language stream.1 The N400 (or N400-like responses) can also be observed in numerous contexts involving non-linguistic but meaningful stimuli, such as pictures (Ganis &amp; Kutas, 2003; Ganis, Kutas, &amp; Sereno, 1996; Nigam, Hoffman, &amp; Simons, 1992; Pratarelli, 1994), faces (Barrett &amp; Rugg, 1989; Bobes, Valdés-Sosa, &amp; Olivares, 1994), environmental noises (Chao, Nielsen-Bohlman, &amp; Knight, 1995; Van Petten &amp; Rheinfelder, 1995), movie clips (Sitnikova, Holcomb, Kiyonaga, &amp; Kuperberg, 2008; Sitnikova, Kuperberg, &amp; Holcomb, 2003) and co-speech gestures (Kelly, Kravitz, &amp; Hopkins, 2004; Wu &amp; Coulson, 2005). Linguistically anomalous stimuli are not always associated with an N400 response. For example, the left anterior negativity (LAN; Friederici, 2002; Neville, Nicol, Barss, Forster, &amp; Garrett, 1991) and P600 (Osterhout &amp; Holcomb, 1992) are well-known ERP components that have been found i</context>
</contexts>
<marker>Barrett, Rugg, 1989</marker>
<rawString>Barrett, S. E., &amp; Rugg, M. D. (1989). Event-related potentials and the semantic matching of faces. Neuropsychologia, 27, 913–922.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Barrett</author>
<author>M D Rugg</author>
</authors>
<title>Event-related potentials and the semantic matching of pictures.</title>
<date>1990</date>
<journal>Brain and Cognition,</journal>
<volume>14</volume>
<pages>201--212</pages>
<contexts>
<context position="42254" citStr="Barrett and Rugg (1990)" startWordPosition="6611" endWordPosition="6614">d an onset of about 300 ms post-stimulus onset and peaked at about 600 ms post-stimulus onset, very much like the negativities we have described in the present study. These effects are somewhat different from those that have been described in studies incorporating incongruent co-speech gestures and other sorts of non-linguistic imagery like drawings, photographs and videos. In Wu and Coulson’s (2005) study of contextually incongruent gestures, a component described by the researchers as a ‘‘gesture N450’’ was observed. Wu and Coulson noted the similarity of this effect to the N450 reported by Barrett and Rugg (1990) for second items in unrelated picture pairs relative to related picture pairs (e.g. wrench/fork vs. knife/fork), stating (p. 659) that consistent with their own ﬁndings, ‘‘most such ‘picture’ ERP studies report a broadly distributed negativity largest at frontal electrode sites and not evident at occipital sites (Barrett &amp; Rugg, 1990; Holcomb &amp; McPherson, 1994; McPherson &amp; Holcomb, 1999; Sitnikova et al., 2003; West &amp; Holcomb, 2002).’’ In contrast, the negativity reported in the present study was quite evident at occipital sites, as can be seen clearly in Figs. 3 and 4. A second notable ﬁndin</context>
<context position="42590" citStr="Barrett &amp; Rugg, 1990" startWordPosition="6661" endWordPosition="6664">ike drawings, photographs and videos. In Wu and Coulson’s (2005) study of contextually incongruent gestures, a component described by the researchers as a ‘‘gesture N450’’ was observed. Wu and Coulson noted the similarity of this effect to the N450 reported by Barrett and Rugg (1990) for second items in unrelated picture pairs relative to related picture pairs (e.g. wrench/fork vs. knife/fork), stating (p. 659) that consistent with their own ﬁndings, ‘‘most such ‘picture’ ERP studies report a broadly distributed negativity largest at frontal electrode sites and not evident at occipital sites (Barrett &amp; Rugg, 1990; Holcomb &amp; McPherson, 1994; McPherson &amp; Holcomb, 1999; Sitnikova et al., 2003; West &amp; Holcomb, 2002).’’ In contrast, the negativity reported in the present study was quite evident at occipital sites, as can be seen clearly in Figs. 3 and 4. A second notable ﬁnding in our study concerns deaf subjects’ ERP response in the phonologically legal pseudo-sign condition, which was also consistent with an N400 response but was generally larger (more negative) than the negativity seen for semantically incongruent but fully lexical signs. This provides further evidence for broad processing similarities </context>
</contexts>
<marker>Barrett, Rugg, 1990</marker>
<rawString>Barrett, S. E., &amp; Rugg, M. D. (1990). Event-related potentials and the semantic matching of pictures. Brain and Cognition, 14, 201–212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bentin</author>
</authors>
<title>Event-related potentials, semantic processes, and expectancy factors in word recognition.</title>
<date>1987</date>
<journal>Brain and Language,</journal>
<volume>31</volume>
<pages>308--327</pages>
<contexts>
<context position="7508" citStr="Bentin, 1987" startWordPosition="1117" endWordPosition="1118">ﬁculty in semantic conceptual integration (Brown &amp; Hagoort, 1993; Hagoort &amp; Van Berkum, 2007). For example, for listeners encountering the two sentences ‘‘I like my coffee with milk and sugar’’ and ‘‘I like my coffee with milk and mud,’’ the N400 response to the last word in the second item is expected to be larger. An N400 or N400-like component can also be found in response to orthographically/phonologically legal but non-occurring ‘‘pseudo-words’’ (e.g. ‘‘blork’’), and it has sometimes been reported that pseudo-words elicit a stronger N400 response than semantically incongruent real words (Bentin, 1987; Bentin, McCarthy, &amp; Wood, 1985; Hagoort &amp; Kutas, 1995), consistent with the idea that the magnitude of N400 response is related to the difﬁculty of the ongoing process of semantic-contextual integration. However, orthographically illegal ‘‘non-words’’ (e.g. ‘‘rbsnk’’) do not generally elicit an N400, and a positive component is sometimes seen instead (Hagoort &amp; Kutas, 1995; Ziegler, Besson, Jacobs, Nazir, &amp; Carr, 1997). This may reﬂect the operation of some kind of ﬁltering mechanism during online processing, through which language users are able to quickly reject forms that lie beyond a cer</context>
<context position="43308" citStr="Bentin, 1987" startWordPosition="6772" endWordPosition="6773">n contrast, the negativity reported in the present study was quite evident at occipital sites, as can be seen clearly in Figs. 3 and 4. A second notable ﬁnding in our study concerns deaf subjects’ ERP response in the phonologically legal pseudo-sign condition, which was also consistent with an N400 response but was generally larger (more negative) than the negativity seen for semantically incongruent but fully lexical signs. This provides further evidence for broad processing similarities for different linguistic modalities, in the light of similar ﬁndings for pseudo-words in earlier studies (Bentin, 1987; Bentin et al., 1985; Hagoort &amp; Kutas, 1995). It is interesting, however, that phonologically legal pseudo-signs did not more strongly differentiate from the semantically incongruent signs in the present study. This may be an indication that our pseudo-signs (or some of their sub-lexical components) are activating lexical representations to a substantial degree (cf. Friedrich, Eulitz, &amp; Lahiri, 2006), and that at the same time, these representations are incongruent with the sentential contexts in which they have been presented. The pseudo-sign and incongruent sign conditions shared another si</context>
</contexts>
<marker>Bentin, 1987</marker>
<rawString>Bentin, S. (1987). Event-related potentials, semantic processes, and expectancy factors in word recognition. Brain and Language, 31, 308–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bentin</author>
<author>G McCarthy</author>
<author>C C Wood</author>
</authors>
<title>Event-related potentials, lexical decision, and semantic priming.</title>
<date>1985</date>
<journal>Electroencephalography &amp; Clinical Neurophysiology,</journal>
<volume>60</volume>
<pages>353--355</pages>
<contexts>
<context position="7540" citStr="Bentin, McCarthy, &amp; Wood, 1985" startWordPosition="1119" endWordPosition="1123">ntic conceptual integration (Brown &amp; Hagoort, 1993; Hagoort &amp; Van Berkum, 2007). For example, for listeners encountering the two sentences ‘‘I like my coffee with milk and sugar’’ and ‘‘I like my coffee with milk and mud,’’ the N400 response to the last word in the second item is expected to be larger. An N400 or N400-like component can also be found in response to orthographically/phonologically legal but non-occurring ‘‘pseudo-words’’ (e.g. ‘‘blork’’), and it has sometimes been reported that pseudo-words elicit a stronger N400 response than semantically incongruent real words (Bentin, 1987; Bentin, McCarthy, &amp; Wood, 1985; Hagoort &amp; Kutas, 1995), consistent with the idea that the magnitude of N400 response is related to the difﬁculty of the ongoing process of semantic-contextual integration. However, orthographically illegal ‘‘non-words’’ (e.g. ‘‘rbsnk’’) do not generally elicit an N400, and a positive component is sometimes seen instead (Hagoort &amp; Kutas, 1995; Ziegler, Besson, Jacobs, Nazir, &amp; Carr, 1997). This may reﬂect the operation of some kind of ﬁltering mechanism during online processing, through which language users are able to quickly reject forms that lie beyond a certain point of acceptability, or </context>
<context position="43329" citStr="Bentin et al., 1985" startWordPosition="6774" endWordPosition="6777">e negativity reported in the present study was quite evident at occipital sites, as can be seen clearly in Figs. 3 and 4. A second notable ﬁnding in our study concerns deaf subjects’ ERP response in the phonologically legal pseudo-sign condition, which was also consistent with an N400 response but was generally larger (more negative) than the negativity seen for semantically incongruent but fully lexical signs. This provides further evidence for broad processing similarities for different linguistic modalities, in the light of similar ﬁndings for pseudo-words in earlier studies (Bentin, 1987; Bentin et al., 1985; Hagoort &amp; Kutas, 1995). It is interesting, however, that phonologically legal pseudo-signs did not more strongly differentiate from the semantically incongruent signs in the present study. This may be an indication that our pseudo-signs (or some of their sub-lexical components) are activating lexical representations to a substantial degree (cf. Friedrich, Eulitz, &amp; Lahiri, 2006), and that at the same time, these representations are incongruent with the sentential contexts in which they have been presented. The pseudo-sign and incongruent sign conditions shared another similarity in that the </context>
</contexts>
<marker>Bentin, McCarthy, Wood, 1985</marker>
<rawString>Bentin, S., McCarthy, G., &amp; Wood, C. C. (1985). Event-related potentials, lexical decision, and semantic priming. Electroencephalography &amp; Clinical Neurophysiology, 60, 353–355.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Bobes</author>
<author>M Valdés-Sosa</author>
<author>E Olivares</author>
</authors>
<title>An ERP study of expectancy violation in face perception.</title>
<date>1994</date>
<journal>Brain and Cognition,</journal>
<volume>26</volume>
<pages>1--22</pages>
<contexts>
<context position="8528" citStr="Bobes, Valdés-Sosa, &amp; Olivares, 1994" startWordPosition="1265" endWordPosition="1269">er, Besson, Jacobs, Nazir, &amp; Carr, 1997). This may reﬂect the operation of some kind of ﬁltering mechanism during online processing, through which language users are able to quickly reject forms that lie beyond a certain point of acceptability, or plausibility, during the ongoing processing of the incoming language stream.1 The N400 (or N400-like responses) can also be observed in numerous contexts involving non-linguistic but meaningful stimuli, such as pictures (Ganis &amp; Kutas, 2003; Ganis, Kutas, &amp; Sereno, 1996; Nigam, Hoffman, &amp; Simons, 1992; Pratarelli, 1994), faces (Barrett &amp; Rugg, 1989; Bobes, Valdés-Sosa, &amp; Olivares, 1994), environmental noises (Chao, Nielsen-Bohlman, &amp; Knight, 1995; Van Petten &amp; Rheinfelder, 1995), movie clips (Sitnikova, Holcomb, Kiyonaga, &amp; Kuperberg, 2008; Sitnikova, Kuperberg, &amp; Holcomb, 2003) and co-speech gestures (Kelly, Kravitz, &amp; Hopkins, 2004; Wu &amp; Coulson, 2005). Linguistically anomalous stimuli are not always associated with an N400 response. For example, the left anterior negativity (LAN; Friederici, 2002; Neville, Nicol, Barss, Forster, &amp; Garrett, 1991) and P600 (Osterhout &amp; Holcomb, 1992) are well-known ERP components that have been found in syntactic violation contexts in 1 Th</context>
</contexts>
<marker>Bobes, Valdés-Sosa, Olivares, 1994</marker>
<rawString>Bobes, M. A., Valdés-Sosa, M., &amp; Olivares, E. (1994). An ERP study of expectancy violation in face perception. Brain and Cognition, 26, 1–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Brown</author>
<author>P Hagoort</author>
</authors>
<title>The processing nature of the N400: Evidence from masked priming.</title>
<date>1993</date>
<journal>Journal of Cognitive Neuroscience,</journal>
<volume>5</volume>
<pages>34--44</pages>
<contexts>
<context position="6960" citStr="Brown &amp; Hagoort, 1993" startWordPosition="1030" endWordPosition="1033">tly investigated in previous ERP research on written, spoken and signed language (e.g. Capek et al., 2009; Kutas, Neville, &amp; Holcomb, 1987). The N400 is a broad negative deﬂection generally seen at central and parietal scalp sites that peaks about 400 ms after the visual or auditory presentation of a word. Although all content words elicit an N400 component, the ERP response is larger for words that are semantically anomalous or less expected (Hagoort &amp; Brown, 1994; Kutas &amp; Hillyard, 1984); thus the N400 is often interpreted as an index of ease or difﬁculty in semantic conceptual integration (Brown &amp; Hagoort, 1993; Hagoort &amp; Van Berkum, 2007). For example, for listeners encountering the two sentences ‘‘I like my coffee with milk and sugar’’ and ‘‘I like my coffee with milk and mud,’’ the N400 response to the last word in the second item is expected to be larger. An N400 or N400-like component can also be found in response to orthographically/phonologically legal but non-occurring ‘‘pseudo-words’’ (e.g. ‘‘blork’’), and it has sometimes been reported that pseudo-words elicit a stronger N400 response than semantically incongruent real words (Bentin, 1987; Bentin, McCarthy, &amp; Wood, 1985; Hagoort &amp; Kutas, 1</context>
</contexts>
<marker>Brown, Hagoort, 1993</marker>
<rawString>Brown, C., &amp; Hagoort, P. (1993). The processing nature of the N400: Evidence from masked priming. Journal of Cognitive Neuroscience, 5, 34–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Capek</author>
<author>G Grossi</author>
<author>A J Newman</author>
<author>S L McBurney</author>
<author>D Corina</author>
<author>B Roeder</author>
</authors>
<title>Brain systems mediating semantic and syntactic processing in deaf native signers: Biological invariance and modality speciﬁcity.</title>
<date>2009</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States of America,</booktitle>
<volume>106</volume>
<pages>8784--8789</pages>
<contexts>
<context position="6444" citStr="Capek et al., 2009" startWordPosition="945" endWordPosition="948">ountered non-linguistic manual forms (here ‘‘self-grooming’’ behaviors, e.g. scratching the face, rubbing one’s eye, adjusting the sleeves of a shirt, etc.). We sought to compare the processing of these non-linguistic gestural forms within a sentential context to cases in which deaf signers encountered violations of semantic expectancy that have been observed to elicit a well-deﬁned electrophysiological component, the N400. The N400 component (Holcomb &amp; Neville, 1991; Kutas &amp; Hillyard, 1980) has been frequently investigated in previous ERP research on written, spoken and signed language (e.g. Capek et al., 2009; Kutas, Neville, &amp; Holcomb, 1987). The N400 is a broad negative deﬂection generally seen at central and parietal scalp sites that peaks about 400 ms after the visual or auditory presentation of a word. Although all content words elicit an N400 component, the ERP response is larger for words that are semantically anomalous or less expected (Hagoort &amp; Brown, 1994; Kutas &amp; Hillyard, 1984); thus the N400 is often interpreted as an index of ease or difﬁculty in semantic conceptual integration (Brown &amp; Hagoort, 1993; Hagoort &amp; Van Berkum, 2007). For example, for listeners encountering the two sente</context>
<context position="9598" citStr="Capek et al. (2009)" startWordPosition="1422" endWordPosition="1425">ter, &amp; Garrett, 1991) and P600 (Osterhout &amp; Holcomb, 1992) are well-known ERP components that have been found in syntactic violation contexts in 1 This possibility is bolstered by recent work of Albert Kim and colleagues, who have found that relative to real word controls, N400 amplitude decreases and P600 amplitude increases, parametrically, as orthographic irregularity increases (Kim &amp; Pitkänen, submitted for publication). 13 spoken and written language, and more recent work has shown that these components can be elicited in the visual–manual modality as well. For example, in a recent study Capek et al. (2009) compared ERP responses to semantically and syntactically well-formed and ill-formed sentences. While semantic violations elicited an N400 that was largest over central and posterior sites, syntactic violations elicited an anterior negativity followed by a widely distributed P600. These ﬁndings are consistent with the idea that within written, spoken and signed languages, semantic and syntactic processes are mediated by non-identical brain systems (Capek et al., 2009). The present study makes use of dynamic video stimuli showing ASL sentences completed by four classes of ending item—semantical</context>
<context position="40248" citStr="Capek et al., 2009" startWordPosition="6303" endWordPosition="6306">han the incongruent sings. In addition, we suggested that the non-linguistic gestures might elicit a positive-going wave, relative to baseline. While this last prediction was more speculative than the others, the expected pattern was in fact observed very consistently in our analysis of the data. Together, these ﬁndings lead to a number of important observations. First, the outcome of our incongruent sign vs. congruent sign comparison replicates earlier ﬁndings which have also indicated that the N400 generalizes across modalities, including the visual– manual modality of signed language (e.g. Capek et al., 2009; Kutas et al., 1987). These ﬁndings are broadly consistent with other studies using a variety of methodologies including positron emission tomography (PET; Corina, San Jose-Robertson, Guillemin, High, &amp; Braun, 2003), functional magnetic resonance imaging (fMRI; Neville et al., 1998), and cortical stimulation mapping (Corina et al., 1999), highlighting key neural processing similarities between signed and spoken language, in spite of the obvious physical differences in the linguistic signal. For instance, Neville et al. (1997) also found that deaf signers exhibited an N400 response to semantic</context>
<context position="41494" citStr="Capek et al. (2009)" startWordPosition="6495" endWordPosition="6498">ces, relative to congruent sentences. Like the effects in the present study, this response was broadly distributed and had an onset and peak that the researchers noted was somewhat later M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24 than would be expected for written language, but consistent with earlier studies on auditory language (Holcomb &amp; Neville, 1990, 1991). Neville et al. suggested that this delay might be due to the fact that the recognition point of different signs will tend to vary more than for printed language, in which all information is made available at the same time. Capek et al. (2009) also found a relatively late N400 response to semantic incongruity in sign sentences; this bilateral and posteriorly prominent effect had an onset of about 300 ms post-stimulus onset and peaked at about 600 ms post-stimulus onset, very much like the negativities we have described in the present study. These effects are somewhat different from those that have been described in studies incorporating incongruent co-speech gestures and other sorts of non-linguistic imagery like drawings, photographs and videos. In Wu and Coulson’s (2005) study of contextually incongruent gestures, a component des</context>
</contexts>
<marker>Capek, Grossi, Newman, McBurney, Corina, Roeder, 2009</marker>
<rawString>Capek, C. M., Grossi, G., Newman, A. J., McBurney, S. L., Corina, D., Roeder, B., et al. (2009). Brain systems mediating semantic and syntactic processing in deaf native signers: Biological invariance and modality speciﬁcity. Proceedings of the National Academy of Sciences of the United States of America, 106, 8784–8789.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L L Chao</author>
<author>L Nielsen-Bohlman</author>
<author>R T Knight</author>
</authors>
<title>Auditory event-related potentials dissociate early and late memory processes.</title>
<date>1995</date>
<journal>Electroencephalography &amp; Clinical Neurophysiology,</journal>
<volume>96</volume>
<pages>157--168</pages>
<contexts>
<context position="8590" citStr="Chao, Nielsen-Bohlman, &amp; Knight, 1995" startWordPosition="1272" endWordPosition="1276"> operation of some kind of ﬁltering mechanism during online processing, through which language users are able to quickly reject forms that lie beyond a certain point of acceptability, or plausibility, during the ongoing processing of the incoming language stream.1 The N400 (or N400-like responses) can also be observed in numerous contexts involving non-linguistic but meaningful stimuli, such as pictures (Ganis &amp; Kutas, 2003; Ganis, Kutas, &amp; Sereno, 1996; Nigam, Hoffman, &amp; Simons, 1992; Pratarelli, 1994), faces (Barrett &amp; Rugg, 1989; Bobes, Valdés-Sosa, &amp; Olivares, 1994), environmental noises (Chao, Nielsen-Bohlman, &amp; Knight, 1995; Van Petten &amp; Rheinfelder, 1995), movie clips (Sitnikova, Holcomb, Kiyonaga, &amp; Kuperberg, 2008; Sitnikova, Kuperberg, &amp; Holcomb, 2003) and co-speech gestures (Kelly, Kravitz, &amp; Hopkins, 2004; Wu &amp; Coulson, 2005). Linguistically anomalous stimuli are not always associated with an N400 response. For example, the left anterior negativity (LAN; Friederici, 2002; Neville, Nicol, Barss, Forster, &amp; Garrett, 1991) and P600 (Osterhout &amp; Holcomb, 1992) are well-known ERP components that have been found in syntactic violation contexts in 1 This possibility is bolstered by recent work of Albert Kim and c</context>
</contexts>
<marker>Chao, Nielsen-Bohlman, Knight, 1995</marker>
<rawString>Chao, L. L., Nielsen-Bohlman, L., &amp; Knight, R. T. (1995). Auditory event-related potentials dissociate early and late memory processes. Electroencephalography &amp; Clinical Neurophysiology, 96, 157–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C Corballis</author>
</authors>
<title>The evolution of language.</title>
<date>2009</date>
<journal>Annals of the New York Academy of Sciences,</journal>
<volume>1156</volume>
<contexts>
<context position="4470" citStr="Corballis, 2009" startWordPosition="655" endWordPosition="656">Sandler, Padden, &amp; Aronoff, 2010; Morford &amp; Kegl, 2000; Senghas, 2005). Even within mature sign languages of Deaf communities, linguistic accounts of sign language structure have also argued that lexical and discourse components of American Sign Language (ASL) and other signed languages may be best understood as being gesturally based (Liddell, 2003). Thus diachronic and synchronic evidence from language research support the contention that signed languages might make use of perceptual systems similar to those through which humans understand or parse human actions and gestures more generally (Corballis, 2009). In contrast, given its linguistic status, sign language perception may require the attunement of specialized systems for recognizing sign forms. A comprehensive theory of sign language recognition will be enhanced by providing an account of when and how the processing of sign forms diverges from the processing of human actions in general. Recent behavioral and neuro-imaging studies have reported differences in deaf subjects’ responses to single signs compared to non-linguistic gestures (Corina, Grosvald, &amp; Lachaud, 2011; Corina et al., 2007; Emmorey, Xu, Gannon, Goldin-Meadow, &amp; Braun, 2010;</context>
</contexts>
<marker>Corballis, 2009</marker>
<rawString>Corballis, M. C. (2009). The evolution of language. Annals of the New York Academy of Sciences, 1156, 19–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D P Corina</author>
<author>S L McBurney</author>
<author>C Dodrill</author>
<author>K Hinshaw</author>
<author>J Brinkley</author>
<author>G Ojemann</author>
</authors>
<title>Functional roles of Broca’s area and SMG: Evidence from cortical stimulation mapping in a deaf signer.</title>
<date>1999</date>
<journal>NeuroImage,</journal>
<volume>10</volume>
<pages>570--581</pages>
<contexts>
<context position="40588" citStr="Corina et al., 1999" startWordPosition="6350" endWordPosition="6353"> of important observations. First, the outcome of our incongruent sign vs. congruent sign comparison replicates earlier ﬁndings which have also indicated that the N400 generalizes across modalities, including the visual– manual modality of signed language (e.g. Capek et al., 2009; Kutas et al., 1987). These ﬁndings are broadly consistent with other studies using a variety of methodologies including positron emission tomography (PET; Corina, San Jose-Robertson, Guillemin, High, &amp; Braun, 2003), functional magnetic resonance imaging (fMRI; Neville et al., 1998), and cortical stimulation mapping (Corina et al., 1999), highlighting key neural processing similarities between signed and spoken language, in spite of the obvious physical differences in the linguistic signal. For instance, Neville et al. (1997) also found that deaf signers exhibited an N400 response to semantically incongruent ASL sentences, relative to congruent sentences. Like the effects in the present study, this response was broadly distributed and had an onset and peak that the researchers noted was somewhat later M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24 than would be expected for written language, but consistent with earlie</context>
</contexts>
<marker>Corina, McBurney, Dodrill, Hinshaw, Brinkley, Ojemann, 1999</marker>
<rawString>Corina, D. P., McBurney, S. L., Dodrill, C., Hinshaw, K., Brinkley, J., &amp; Ojemann, G. (1999). Functional roles of Broca’s area and SMG: Evidence from cortical stimulation mapping in a deaf signer. NeuroImage, 10, 570–581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Corina</author>
<author>Y-S Chiu</author>
<author>H Knapp</author>
<author>R Greenwald</author>
<author>San Jose-Robertson</author>
<author>L</author>
<author>A Braun</author>
</authors>
<title>Neural correlates of human action observation in hearing and deaf subjects.</title>
<date>2007</date>
<journal>Brain Research,</journal>
<volume>1152</volume>
<pages>111--129</pages>
<contexts>
<context position="5018" citStr="Corina et al., 2007" startWordPosition="733" endWordPosition="736">nd or parse human actions and gestures more generally (Corballis, 2009). In contrast, given its linguistic status, sign language perception may require the attunement of specialized systems for recognizing sign forms. A comprehensive theory of sign language recognition will be enhanced by providing an account of when and how the processing of sign forms diverges from the processing of human actions in general. Recent behavioral and neuro-imaging studies have reported differences in deaf subjects’ responses to single signs compared to non-linguistic gestures (Corina, Grosvald, &amp; Lachaud, 2011; Corina et al., 2007; Emmorey, Xu, Gannon, Goldin-Meadow, &amp; Braun, 2010; MacSweeney et al., 2004), but no studies to our knowledge have examined the recognition of signs and gestures M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24 under sentence processing constraints. Consider for example the signer, who, in mid-sentence, fulﬁlls the urge to scratch his face, or perhaps swat away a ﬂying insect. What is the fate of this non-linguistic articulation? Does the sign perceiver attempt to incorporate these manual behaviors into accruing sentential representations, or are these actions easily tagged as non-lingu</context>
<context position="10887" citStr="Corina et al., 2007" startWordPosition="1596" endWordPosition="1599">al but non-occurring pseudo-signs, and non-linguistic grooming gestures. Based upon previous studies, we expected a gradation of N400-like responses across conditions, with N400 effects of smaller magnitude for semantically incongruent endings and of larger magnitude (i.e. more negative) for phonologically legal pseudo-signs. The ERP response for the non-linguistic gesture condition is a priori more difﬁcult to predict. Previous neuro-imaging studies of deaf signers have reported differences in patterns of activation associated with the perception of signs compared to non-linguistic gestures (Corina et al., 2007; Emmorey et al., 2010; MacSweeney et al., 2004), but the methodologies used in those studies lacked the temporal resolution to determine at what stage of processing these differences may occur. While N400-like responses have been elicited to co-speech gestural mismatches (Kelly et al., 2004; Wu &amp; Coulson, 2005), in our study, gestures occur in place of semantically appropriate sentence-ending items, rather than as a possible accompaniment. It should also be borne in mind that the relationship of signs and grooming gestures is probably not quite akin to that between standard lexical items in s</context>
<context position="44230" citStr="Corina et al. (2007)" startWordPosition="6907" endWordPosition="6910">are activating lexical representations to a substantial degree (cf. Friedrich, Eulitz, &amp; Lahiri, 2006), and that at the same time, these representations are incongruent with the sentential contexts in which they have been presented. The pseudo-sign and incongruent sign conditions shared another similarity in that the effects they elicited were very prominent at occipital sites, which differs from what has traditionally been observed in studies of word processing. Whether this is a reﬂection of the modality of expression or other experimental factors must await further study, though results of Corina et al. (2007), discussed below, may offer some insights about useful directions such research might take. A third set of ﬁndings, concerning the outcome related to our non-linguistic grooming actions, is especially provocative. In contrast to the three other kinds of sentence-ﬁnal items, all of which could be considered linguistic (i.e. as actual lexical items in two cases, and phonologically legal lexical gaps in the third), the non-linguistic grooming actions elicited a large positivity. As noted earlier, phonologically illegal words in ERP studies have in some cases elicited a positive-going component r</context>
<context position="52836" citStr="Corina et al. (2007)" startWordPosition="8205" endWordPosition="8208">gesting parallel integration of speech and gesture in this context. In contrast, the grooming gesture condition in the present study was not associated with any N400-like effects. We suggest that this is due to the fact that subjects were unlikely to perceive these actions as being akin to co-speech (or co-sign) gestures, but instead as something qualitatively different. This detection process evidently occurred quite rapidly during online processing of these stimuli, comparable to the speed at which semantic processing was carried out for the linguistic stimuli. The ﬁndings of a PET study by Corina et al. (2007) may shed some additional light on this outcome. In that study, deaf signers were found to have engaged different brain regions when processing ASL signs and selfgrooming gestures, in contrast with the hearing non-signers who also took part in the study. Speciﬁcally, deaf signers engaged lefthemisphere perisylvian language areas when processing ASL sign forms, but recruited middle-occipital temporal–ventral regions when processing self-grooming actions. The latter areas are known to be involved in the detection of human bodies, faces, and movements. The present ﬁndings add temporal precision t</context>
<context position="57220" citStr="Corina et al. (2007)" startWordPosition="8871" endWordPosition="8874">n part by Grant NIH-NIDCD 2ROI-DC03099-11, awarded to David Corina. Appendix A. List of stimulus items Table A1 lists all 120 sentence frames and three of the corresponding endings for each sentence: the semantically congruent signs, semantically incongruent signs and pseudo-signs. In the fourth class of endings, the gesture stimuli, the sign performer was seen making brief grooming actions such as head scratching, eye rubbing or passing her ﬁngers through her hair. To create 4 It should also be noted that the effects seen in the present study were for gestures in a sentence context, while in Corina et al. (2007) the sign and gesture stimuli were seen in isolation. 21 M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24 120 unique gesture stimuli, the actions were performed with differences in the number and conﬁguration of hands or ﬁngers used, the location of the body involved, and so on. Also shown in the rightmost two columns of the table are the ‘‘correct’’ and ‘‘incorrect’’ word choices for the occasional quiz items. A number sign (#) preceding an item means that item was ﬁngerspelled. Many of these sentences were adapted from the English-language stimuli used in Johnson and Hamm (2000). Table</context>
</contexts>
<marker>Corina, Chiu, Knapp, Greenwald, Jose-Robertson, L, Braun, 2007</marker>
<rawString>Corina, D., Chiu, Y.-S., Knapp, H., Greenwald, R., San Jose-Robertson, L., &amp; Braun, A. (2007). Neural correlates of human action observation in hearing and deaf subjects. Brain Research, 1152, 111–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D P Corina</author>
<author>H P Knapp</author>
</authors>
<title>Psycholinguistic and neurolinguistic perspectives on sign languages.</title>
<date>2006</date>
<booktitle>Handbook of psycholinguistics (2nd ed.,</booktitle>
<pages>1001--1024</pages>
<editor>In M. J. Traxler &amp; M. A. Gernsbacher (Eds.),</editor>
<publisher>Academic Press.</publisher>
<location>San Diego, CA:</location>
<contexts>
<context position="2241" citStr="Corina &amp; Knapp, 2006" startWordPosition="333" endWordPosition="336">former scratching her face). We found signiﬁcant N400-like responses in the incongruent and pseudo-sign contexts, while the gestures elicited a large positivity. Ó 2012 Elsevier Inc. All rights reserved. 1. Introduction While it is now widely accepted that signed languages used in deaf communities around the world represent full-ﬂedged instantiations of human languages—languages which are expressed in the visual–manual modality rather than the aural–oral modality—the question of how a sign is recognized and integrated into a sentential context in real time has received far less attention (see Corina &amp; Knapp, 2006; Emmorey, 2002; for some discussions). Sign language recognition may be more complicated than spoken language recognition by virtue of the fact that the primary articulators, the hands and arms, are also used in a wide range of other common everyday behaviors that include non-linguistic actions such a reaching and grasping, waving, and scratching oneself, as well gesticulations that accompany speech (i.e. co-speech gestures) or serve non-sign language deictic functions, such as pointing. The formal relationship between signed languages and human gestural actions is of considerable interest to</context>
</contexts>
<marker>Corina, Knapp, 2006</marker>
<rawString>Corina, D. P., &amp; Knapp, H. P. (2006). Psycholinguistic and neurolinguistic perspectives on sign languages. In M. J. Traxler &amp; M. A. Gernsbacher (Eds.), Handbook of psycholinguistics (2nd ed., pp. 1001–1024). San Diego, CA: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D P Corina</author>
<author>San Jose-Robertson</author>
<author>L Guillemin</author>
<author>A High</author>
<author>J</author>
<author>A R Braun</author>
</authors>
<title>Language lateralization in a bimanual language.</title>
<date>2003</date>
<journal>Journal of Cognitive Neuroscience,</journal>
<volume>15</volume>
<pages>718--730</pages>
<marker>Corina, Jose-Robertson, Guillemin, High, J, Braun, 2003</marker>
<rawString>Corina, D. P., San Jose-Robertson, L., Guillemin, A., High, J., &amp; Braun, A. R. (2003). Language lateralization in a bimanual language. Journal of Cognitive Neuroscience, 15, 718–730.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Corina</author>
<author>M Grosvald</author>
<author>C Lachaud</author>
</authors>
<title>Perceptual invariance or orientation speciﬁcity in American Sign Language? Evidence from repetition priming for signs and gestures.</title>
<date>2011</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<volume>26</volume>
<pages>1102--1135</pages>
<contexts>
<context position="4997" citStr="Corina, Grosvald, &amp; Lachaud, 2011" startWordPosition="728" endWordPosition="732">those through which humans understand or parse human actions and gestures more generally (Corballis, 2009). In contrast, given its linguistic status, sign language perception may require the attunement of specialized systems for recognizing sign forms. A comprehensive theory of sign language recognition will be enhanced by providing an account of when and how the processing of sign forms diverges from the processing of human actions in general. Recent behavioral and neuro-imaging studies have reported differences in deaf subjects’ responses to single signs compared to non-linguistic gestures (Corina, Grosvald, &amp; Lachaud, 2011; Corina et al., 2007; Emmorey, Xu, Gannon, Goldin-Meadow, &amp; Braun, 2010; MacSweeney et al., 2004), but no studies to our knowledge have examined the recognition of signs and gestures M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24 under sentence processing constraints. Consider for example the signer, who, in mid-sentence, fulﬁlls the urge to scratch his face, or perhaps swat away a ﬂying insect. What is the fate of this non-linguistic articulation? Does the sign perceiver attempt to incorporate these manual behaviors into accruing sentential representations, or are these actions easil</context>
</contexts>
<marker>Corina, Grosvald, Lachaud, 2011</marker>
<rawString>Corina, D., Grosvald, M., &amp; Lachaud, C. (2011). Perceptual invariance or orientation speciﬁcity in American Sign Language? Evidence from repetition priming for signs and gestures. Language and Cognitive Processes, 26, 1102–1135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cornejo</author>
<author>F Simonetti</author>
<author>A Ibáñez</author>
<author>N Aldunate</author>
<author>V López</author>
<author>F Ceric</author>
</authors>
<title>Gesture and metaphor comprehension: Electrophysiological evidence of crossmodal coordination by audiovisual stimulation.</title>
<date>2009</date>
<journal>Brain and Cognition,</journal>
<volume>70</volume>
<pages>42--52</pages>
<contexts>
<context position="50222" citStr="Cornejo et al., 2009" startWordPosition="7811" endWordPosition="7814">earch. 20 M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24 We have already alluded to the growing number of studies which have used ERP methodology to examine the contributions of co-speech manual gestures to the interpretation of both linguistic and non-linguistic stimuli (Holle &amp; Gunter, 2007; Kelly et al., 2004; Ozyürek, Willems, Kita, &amp; Hagoort, 2007; Wu &amp; Coulson, 2005, 2007a, 2007b). Many of these studies have used iconic manual gestures that depict a salient visual–spatial property of concrete objects, such as their size and shape or an associated manner of movement (but see also Cornejo et al., 2009). Collectively these studies suggest that co-speech manual gestures inﬂuence semantic representations, and that discrepancies between gestural forms and the semantic contexts in which they occur lead to greater processing costs on the part of language perceivers. This in turn results in increased negativities in the time window often associated with the classic N400 effect, observed in response to word meanings that violate the wider semantic context (Kutas &amp; Hillyard, 1980). For example, Kelly et al. (2004) observed modulation of ERP responses for speech tokens that were either accompanied by</context>
</contexts>
<marker>Cornejo, Simonetti, Ibáñez, Aldunate, López, Ceric, 2009</marker>
<rawString>Cornejo, C., Simonetti, F., Ibáñez, A., Aldunate, N., López, V., &amp; Ceric, F. (2009). Gesture and metaphor comprehension: Electrophysiological evidence of crossmodal coordination by audiovisual stimulation. Brain and Cognition, 70, 42–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Delorme</author>
<author>S Makeig</author>
</authors>
<title>EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics.</title>
<date>2004</date>
<journal>Journal of Neuroscience Methods,</journal>
<volume>134</volume>
<pages>9--21</pages>
<contexts>
<context position="24806" citStr="Delorme &amp; Makeig, 2004" startWordPosition="3851" endWordPosition="3854">central sites, using AgCl electrodes attached to an elastic cap (BioSemi). Vertical and horizontal eye movements were monitored by means of two electrodes placed above and below the left eye and two others located adjacent to the left and right eye. All electrodes were referenced to 15 the average of the left and right mastoids. The EEG was digitized online at 256 Hz, and ﬁltered ofﬂine below 30 Hz and above 0.01 Hz. Scalp electrode impedance threshold values were set at 20 kX. Initial analysis of the EEG data was performed using the ERPLAB plugin (Lopez-Calderon &amp; Luck, in press) for EEGLAB (Delorme &amp; Makeig, 2004). Epochs began 200 ms before stimulus onset and ended 1000 ms after. Inspection of subjects’ EEG data was performed by eye to check rejections suggested by a script run in ERPLAB whose artifact rejection thresholds were set at ±120 lV. For all 16 subjects, in each of the four sentence ending conditions at least 20 of the original 30 trials remained after the rejection procedure just described. The statistical analyses reported below were carried out using the SPSS statistical package. To assess the signiﬁcance of the observed effects, a column analysis was conducted (cf. Kim &amp; Osterhout, 2005)</context>
</contexts>
<marker>Delorme, Makeig, 2004</marker>
<rawString>Delorme, A., &amp; Makeig, S. (2004). EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics. Journal of Neuroscience Methods, 134, 9–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Emmorey</author>
</authors>
<title>Language, cognition, and the brain: Insights from sign language research.</title>
<date>2002</date>
<location>Mahwah, NJ: Lawrence Erlbaum.</location>
<contexts>
<context position="2256" citStr="Emmorey, 2002" startWordPosition="337" endWordPosition="338">face). We found signiﬁcant N400-like responses in the incongruent and pseudo-sign contexts, while the gestures elicited a large positivity. Ó 2012 Elsevier Inc. All rights reserved. 1. Introduction While it is now widely accepted that signed languages used in deaf communities around the world represent full-ﬂedged instantiations of human languages—languages which are expressed in the visual–manual modality rather than the aural–oral modality—the question of how a sign is recognized and integrated into a sentential context in real time has received far less attention (see Corina &amp; Knapp, 2006; Emmorey, 2002; for some discussions). Sign language recognition may be more complicated than spoken language recognition by virtue of the fact that the primary articulators, the hands and arms, are also used in a wide range of other common everyday behaviors that include non-linguistic actions such a reaching and grasping, waving, and scratching oneself, as well gesticulations that accompany speech (i.e. co-speech gestures) or serve non-sign language deictic functions, such as pointing. The formal relationship between signed languages and human gestural actions is of considerable interest to a range of dis</context>
</contexts>
<marker>Emmorey, 2002</marker>
<rawString>Emmorey, K. (2002). Language, cognition, and the brain: Insights from sign language research. Mahwah, NJ: Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Emmorey</author>
<author>J Xu</author>
<author>P Gannon</author>
<author>S Goldin-Meadow</author>
<author>A Braun</author>
</authors>
<title>CNS activation and regional connectivity during pantomime observation: No engagement of the mirror neuron system for deaf signers.</title>
<date>2010</date>
<journal>NeuroImage,</journal>
<volume>49</volume>
<pages>994--1005</pages>
<contexts>
<context position="5069" citStr="Emmorey, Xu, Gannon, Goldin-Meadow, &amp; Braun, 2010" startWordPosition="737" endWordPosition="743">ions and gestures more generally (Corballis, 2009). In contrast, given its linguistic status, sign language perception may require the attunement of specialized systems for recognizing sign forms. A comprehensive theory of sign language recognition will be enhanced by providing an account of when and how the processing of sign forms diverges from the processing of human actions in general. Recent behavioral and neuro-imaging studies have reported differences in deaf subjects’ responses to single signs compared to non-linguistic gestures (Corina, Grosvald, &amp; Lachaud, 2011; Corina et al., 2007; Emmorey, Xu, Gannon, Goldin-Meadow, &amp; Braun, 2010; MacSweeney et al., 2004), but no studies to our knowledge have examined the recognition of signs and gestures M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24 under sentence processing constraints. Consider for example the signer, who, in mid-sentence, fulﬁlls the urge to scratch his face, or perhaps swat away a ﬂying insect. What is the fate of this non-linguistic articulation? Does the sign perceiver attempt to incorporate these manual behaviors into accruing sentential representations, or are these actions easily tagged as non-linguistic and thus rejected by the parser? The goal of </context>
<context position="10909" citStr="Emmorey et al., 2010" startWordPosition="1600" endWordPosition="1603">pseudo-signs, and non-linguistic grooming gestures. Based upon previous studies, we expected a gradation of N400-like responses across conditions, with N400 effects of smaller magnitude for semantically incongruent endings and of larger magnitude (i.e. more negative) for phonologically legal pseudo-signs. The ERP response for the non-linguistic gesture condition is a priori more difﬁcult to predict. Previous neuro-imaging studies of deaf signers have reported differences in patterns of activation associated with the perception of signs compared to non-linguistic gestures (Corina et al., 2007; Emmorey et al., 2010; MacSweeney et al., 2004), but the methodologies used in those studies lacked the temporal resolution to determine at what stage of processing these differences may occur. While N400-like responses have been elicited to co-speech gestural mismatches (Kelly et al., 2004; Wu &amp; Coulson, 2005), in our study, gestures occur in place of semantically appropriate sentence-ending items, rather than as a possible accompaniment. It should also be borne in mind that the relationship of signs and grooming gestures is probably not quite akin to that between standard lexical items in spoken language and the</context>
</contexts>
<marker>Emmorey, Xu, Gannon, Goldin-Meadow, Braun, 2010</marker>
<rawString>Emmorey, K., Xu, J., Gannon, P., Goldin-Meadow, S., &amp; Braun, A. (2010). CNS activation and regional connectivity during pantomime observation: No engagement of the mirror neuron system for deaf signers. NeuroImage, 49, 994–1005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Friedrich</author>
<author>C Eulitz</author>
<author>A Lahiri</author>
</authors>
<title>Not every pseudoword disrupts word recognition: An ERP study.</title>
<date>2006</date>
<journal>Behavioral and Brain Functions,</journal>
<volume>2</volume>
<pages>36</pages>
<contexts>
<context position="43711" citStr="Friedrich, Eulitz, &amp; Lahiri, 2006" startWordPosition="6827" endWordPosition="6831">antically incongruent but fully lexical signs. This provides further evidence for broad processing similarities for different linguistic modalities, in the light of similar ﬁndings for pseudo-words in earlier studies (Bentin, 1987; Bentin et al., 1985; Hagoort &amp; Kutas, 1995). It is interesting, however, that phonologically legal pseudo-signs did not more strongly differentiate from the semantically incongruent signs in the present study. This may be an indication that our pseudo-signs (or some of their sub-lexical components) are activating lexical representations to a substantial degree (cf. Friedrich, Eulitz, &amp; Lahiri, 2006), and that at the same time, these representations are incongruent with the sentential contexts in which they have been presented. The pseudo-sign and incongruent sign conditions shared another similarity in that the effects they elicited were very prominent at occipital sites, which differs from what has traditionally been observed in studies of word processing. Whether this is a reﬂection of the modality of expression or other experimental factors must await further study, though results of Corina et al. (2007), discussed below, may offer some insights about useful directions such research </context>
</contexts>
<marker>Friedrich, Eulitz, Lahiri, 2006</marker>
<rawString>Friedrich, C., Eulitz, C., &amp; Lahiri, A. (2006). Not every pseudoword disrupts word recognition: An ERP study. Behavioral and Brain Functions, 2, 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A D Friederici</author>
</authors>
<title>Towards a neural basis of auditory sentence processing.</title>
<date>2002</date>
<journal>Trends in Cognitive Sciences,</journal>
<volume>6</volume>
<pages>78--84</pages>
<contexts>
<context position="8950" citStr="Friederici, 2002" startWordPosition="1325" endWordPosition="1326">stimuli, such as pictures (Ganis &amp; Kutas, 2003; Ganis, Kutas, &amp; Sereno, 1996; Nigam, Hoffman, &amp; Simons, 1992; Pratarelli, 1994), faces (Barrett &amp; Rugg, 1989; Bobes, Valdés-Sosa, &amp; Olivares, 1994), environmental noises (Chao, Nielsen-Bohlman, &amp; Knight, 1995; Van Petten &amp; Rheinfelder, 1995), movie clips (Sitnikova, Holcomb, Kiyonaga, &amp; Kuperberg, 2008; Sitnikova, Kuperberg, &amp; Holcomb, 2003) and co-speech gestures (Kelly, Kravitz, &amp; Hopkins, 2004; Wu &amp; Coulson, 2005). Linguistically anomalous stimuli are not always associated with an N400 response. For example, the left anterior negativity (LAN; Friederici, 2002; Neville, Nicol, Barss, Forster, &amp; Garrett, 1991) and P600 (Osterhout &amp; Holcomb, 1992) are well-known ERP components that have been found in syntactic violation contexts in 1 This possibility is bolstered by recent work of Albert Kim and colleagues, who have found that relative to real word controls, N400 amplitude decreases and P600 amplitude increases, parametrically, as orthographic irregularity increases (Kim &amp; Pitkänen, submitted for publication). 13 spoken and written language, and more recent work has shown that these components can be elicited in the visual–manual modality as well. Fo</context>
</contexts>
<marker>Friederici, 2002</marker>
<rawString>Friederici, A. D. (2002). Towards a neural basis of auditory sentence processing. Trends in Cognitive Sciences, 6, 78–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Frishberg</author>
</authors>
<title>Ghanaian Sign Language. In</title>
<date>1987</date>
<publisher>McGraw-Gill Book Company.</publisher>
<location>New York:</location>
<contexts>
<context position="3794" citStr="Frishberg, 1987" startWordPosition="558" endWordPosition="559">logy, University of California at Irvine, 101 The City Drive South, Bldg. 53, Room 204, Orange, CA 928684280, United States. Fax: +1 714 456 1697. E-mail address: m.grosvald@uci.edu (M. Grosvald). 0093-934X/$ - see front matter Ó 2012 Elsevier Inc. All rights reserved. doi:10.1016/j.bandl.2012.01.005 cently, linguists have documented compelling evidence that the development of nascent sign languages derives from idiosyncratic gestural and pantomimic systems used by isolated communities, which in some cases may be limited to individual families who have a need to communicate with a deaf child (Frishberg, 1987; GoldinMeadow, 2003; Kegl, Senghas, &amp; Coppola, 1999; Meir, Sandler, Padden, &amp; Aronoff, 2010; Morford &amp; Kegl, 2000; Senghas, 2005). Even within mature sign languages of Deaf communities, linguistic accounts of sign language structure have also argued that lexical and discourse components of American Sign Language (ASL) and other signed languages may be best understood as being gesturally based (Liddell, 2003). Thus diachronic and synchronic evidence from language research support the contention that signed languages might make use of perceptual systems similar to those through which humans und</context>
</contexts>
<marker>Frishberg, 1987</marker>
<rawString>Frishberg, N. (1987). Ghanaian Sign Language. In J. Van Cleve (Ed.), Gallaudet encyclopaedia of deaf people and deafness. New York: McGraw-Gill Book Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ganis</author>
<author>M Kutas</author>
</authors>
<title>An electrophysiological study of scene effects on object identiﬁcation.</title>
<date>2003</date>
<journal>Cognitive Brain Research,</journal>
<volume>16</volume>
<pages>123--144</pages>
<contexts>
<context position="8380" citStr="Ganis &amp; Kutas, 2003" startWordPosition="1244" endWordPosition="1247"> (e.g. ‘‘rbsnk’’) do not generally elicit an N400, and a positive component is sometimes seen instead (Hagoort &amp; Kutas, 1995; Ziegler, Besson, Jacobs, Nazir, &amp; Carr, 1997). This may reﬂect the operation of some kind of ﬁltering mechanism during online processing, through which language users are able to quickly reject forms that lie beyond a certain point of acceptability, or plausibility, during the ongoing processing of the incoming language stream.1 The N400 (or N400-like responses) can also be observed in numerous contexts involving non-linguistic but meaningful stimuli, such as pictures (Ganis &amp; Kutas, 2003; Ganis, Kutas, &amp; Sereno, 1996; Nigam, Hoffman, &amp; Simons, 1992; Pratarelli, 1994), faces (Barrett &amp; Rugg, 1989; Bobes, Valdés-Sosa, &amp; Olivares, 1994), environmental noises (Chao, Nielsen-Bohlman, &amp; Knight, 1995; Van Petten &amp; Rheinfelder, 1995), movie clips (Sitnikova, Holcomb, Kiyonaga, &amp; Kuperberg, 2008; Sitnikova, Kuperberg, &amp; Holcomb, 2003) and co-speech gestures (Kelly, Kravitz, &amp; Hopkins, 2004; Wu &amp; Coulson, 2005). Linguistically anomalous stimuli are not always associated with an N400 response. For example, the left anterior negativity (LAN; Friederici, 2002; Neville, Nicol, Barss, Forst</context>
</contexts>
<marker>Ganis, Kutas, 2003</marker>
<rawString>Ganis, G., &amp; Kutas, M. (2003). An electrophysiological study of scene effects on object identiﬁcation. Cognitive Brain Research, 16, 123–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ganis</author>
<author>M Kutas</author>
<author>M I Sereno</author>
</authors>
<title>The search for ‘‘common sense’’: An electrophysiological study of the comprehension of words and pictures in reading.</title>
<date>1996</date>
<journal>Journal of Cognitive Neuroscience,</journal>
<volume>8</volume>
<pages>89--106</pages>
<contexts>
<context position="8410" citStr="Ganis, Kutas, &amp; Sereno, 1996" startWordPosition="1248" endWordPosition="1252">not generally elicit an N400, and a positive component is sometimes seen instead (Hagoort &amp; Kutas, 1995; Ziegler, Besson, Jacobs, Nazir, &amp; Carr, 1997). This may reﬂect the operation of some kind of ﬁltering mechanism during online processing, through which language users are able to quickly reject forms that lie beyond a certain point of acceptability, or plausibility, during the ongoing processing of the incoming language stream.1 The N400 (or N400-like responses) can also be observed in numerous contexts involving non-linguistic but meaningful stimuli, such as pictures (Ganis &amp; Kutas, 2003; Ganis, Kutas, &amp; Sereno, 1996; Nigam, Hoffman, &amp; Simons, 1992; Pratarelli, 1994), faces (Barrett &amp; Rugg, 1989; Bobes, Valdés-Sosa, &amp; Olivares, 1994), environmental noises (Chao, Nielsen-Bohlman, &amp; Knight, 1995; Van Petten &amp; Rheinfelder, 1995), movie clips (Sitnikova, Holcomb, Kiyonaga, &amp; Kuperberg, 2008; Sitnikova, Kuperberg, &amp; Holcomb, 2003) and co-speech gestures (Kelly, Kravitz, &amp; Hopkins, 2004; Wu &amp; Coulson, 2005). Linguistically anomalous stimuli are not always associated with an N400 response. For example, the left anterior negativity (LAN; Friederici, 2002; Neville, Nicol, Barss, Forster, &amp; Garrett, 1991) and P600 </context>
</contexts>
<marker>Ganis, Kutas, Sereno, 1996</marker>
<rawString>Ganis, G., Kutas, M., &amp; Sereno, M. I. (1996). The search for ‘‘common sense’’: An electrophysiological study of the comprehension of words and pictures in reading. Journal of Cognitive Neuroscience, 8, 89–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gentilucci</author>
<author>M Corballis</author>
</authors>
<title>From manual gesture to speech: A gradual transition.</title>
<date>2006</date>
<journal>Neuroscience &amp; Biobehavioral Reviews,</journal>
<volume>30</volume>
<pages>949--960</pages>
<contexts>
<context position="3065" citStr="Gentilucci &amp; Corballis, 2006" startWordPosition="452" endWordPosition="455">are also used in a wide range of other common everyday behaviors that include non-linguistic actions such a reaching and grasping, waving, and scratching oneself, as well gesticulations that accompany speech (i.e. co-speech gestures) or serve non-sign language deictic functions, such as pointing. The formal relationship between signed languages and human gestural actions is of considerable interest to a range of disciplines. Linguists, psychologists and cognitive scientists have proposed a critical role for manual gesture in the development and evolution of human languages (Arbib, 2005, 2008; Gentilucci &amp; Corballis, 2006; Rizzolatti &amp; Arbib, 1998; Tomasello, 2005; Wilcox, 2004). Re⇑ Corresponding author. Address: Department of Neurology, University of California at Irvine, 101 The City Drive South, Bldg. 53, Room 204, Orange, CA 928684280, United States. Fax: +1 714 456 1697. E-mail address: m.grosvald@uci.edu (M. Grosvald). 0093-934X/$ - see front matter Ó 2012 Elsevier Inc. All rights reserved. doi:10.1016/j.bandl.2012.01.005 cently, linguists have documented compelling evidence that the development of nascent sign languages derives from idiosyncratic gestural and pantomimic systems used by isolated communi</context>
</contexts>
<marker>Gentilucci, Corballis, 2006</marker>
<rawString>Gentilucci, M., &amp; Corballis, M. (2006). From manual gesture to speech: A gradual transition. Neuroscience &amp; Biobehavioral Reviews, 30, 949–960.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldin-Meadow</author>
</authors>
<title>Hearing gestures: How our hands help us think.</title>
<date>2003</date>
<publisher>Harvard University Press.</publisher>
<location>Cambridge, MA:</location>
<marker>Goldin-Meadow, 2003</marker>
<rawString>Goldin-Meadow, S. (2003). Hearing gestures: How our hands help us think. Cambridge, MA: Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W W Greenhouse</author>
<author>S Geisser</author>
</authors>
<title>On methods in the analysis of proﬁle data.</title>
<date>1959</date>
<journal>Psychometrika,</journal>
<volume>24</volume>
<pages>95--112</pages>
<contexts>
<context position="26606" citStr="Greenhouse &amp; Geisser, 1959" startWordPosition="4148" endWordPosition="4151">ree ANOVAs the factor Electrode had one level for each pair of electrodes, from most anterior to most posterior. For the N400 analysis, the dependent measure was mean amplitude of EEG response within the window from 400 to 600 ms after stimulus onset. Because the latency of the effects related to gesture was somewhat greater, a second column analysis was run for the window from 600 to 800 ms after stimulus onset. For the purposes of these column analyses, data from the two frontmost sites (FP1 and FP2) and from two posterior sites (PO3 and PO4) were not used. In all cases, Greenhouse–Geisser (Greenhouse &amp; Geisser, 1959) adjustments for non-sphericity were performed where appropriate and are reﬂected in the reported results. In the following section, the outcomes for the ANOVAs performed for each time window will be given in the order midline, inner, outer, and outermost. This establishes the signiﬁcance of the results, which we ﬁrst introduce with illustrations and descriptions of the waveforms and the associated topographic maps. 3. Results 3.1. Waveforms and topographic maps Pictured in Figs. 3 and 4 are grand-average waveforms at selected electrode sites for the four sentence Ending conditions. Visual ins</context>
</contexts>
<marker>Greenhouse, Geisser, 1959</marker>
<rawString>Greenhouse, W. W., &amp; Geisser, S. (1959). On methods in the analysis of proﬁle data. Psychometrika, 24, 95–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hagoort</author>
<author>C Brown</author>
</authors>
<title>Brain responses to lexical ambiguity resolution and parsing. In</title>
<date>1994</date>
<pages>45--80</pages>
<location>Hillsdale, NJ, UK:</location>
<contexts>
<context position="6808" citStr="Hagoort &amp; Brown, 1994" startWordPosition="1005" endWordPosition="1008"> to elicit a well-deﬁned electrophysiological component, the N400. The N400 component (Holcomb &amp; Neville, 1991; Kutas &amp; Hillyard, 1980) has been frequently investigated in previous ERP research on written, spoken and signed language (e.g. Capek et al., 2009; Kutas, Neville, &amp; Holcomb, 1987). The N400 is a broad negative deﬂection generally seen at central and parietal scalp sites that peaks about 400 ms after the visual or auditory presentation of a word. Although all content words elicit an N400 component, the ERP response is larger for words that are semantically anomalous or less expected (Hagoort &amp; Brown, 1994; Kutas &amp; Hillyard, 1984); thus the N400 is often interpreted as an index of ease or difﬁculty in semantic conceptual integration (Brown &amp; Hagoort, 1993; Hagoort &amp; Van Berkum, 2007). For example, for listeners encountering the two sentences ‘‘I like my coffee with milk and sugar’’ and ‘‘I like my coffee with milk and mud,’’ the N400 response to the last word in the second item is expected to be larger. An N400 or N400-like component can also be found in response to orthographically/phonologically legal but non-occurring ‘‘pseudo-words’’ (e.g. ‘‘blork’’), and it has sometimes been reported that</context>
</contexts>
<marker>Hagoort, Brown, 1994</marker>
<rawString>Hagoort, P., &amp; Brown, C. (1994). Brain responses to lexical ambiguity resolution and parsing. In L. Frazier, J. Clifton Charles, &amp; K. Rayner (Eds.), Perspectives in sentence processing (pp. 45–80). Hillsdale, NJ, UK: Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hagoort</author>
<author>M Kutas</author>
</authors>
<title>Electrophysiological insights into language deﬁcits.</title>
<date>1995</date>
<booktitle>In F. Boller &amp; J. Grafman (Eds.), Handbook of neuropsychology</booktitle>
<pages>105--134</pages>
<publisher>Elsevier.</publisher>
<location>Amsterdam:</location>
<contexts>
<context position="7564" citStr="Hagoort &amp; Kutas, 1995" startWordPosition="1124" endWordPosition="1127">wn &amp; Hagoort, 1993; Hagoort &amp; Van Berkum, 2007). For example, for listeners encountering the two sentences ‘‘I like my coffee with milk and sugar’’ and ‘‘I like my coffee with milk and mud,’’ the N400 response to the last word in the second item is expected to be larger. An N400 or N400-like component can also be found in response to orthographically/phonologically legal but non-occurring ‘‘pseudo-words’’ (e.g. ‘‘blork’’), and it has sometimes been reported that pseudo-words elicit a stronger N400 response than semantically incongruent real words (Bentin, 1987; Bentin, McCarthy, &amp; Wood, 1985; Hagoort &amp; Kutas, 1995), consistent with the idea that the magnitude of N400 response is related to the difﬁculty of the ongoing process of semantic-contextual integration. However, orthographically illegal ‘‘non-words’’ (e.g. ‘‘rbsnk’’) do not generally elicit an N400, and a positive component is sometimes seen instead (Hagoort &amp; Kutas, 1995; Ziegler, Besson, Jacobs, Nazir, &amp; Carr, 1997). This may reﬂect the operation of some kind of ﬁltering mechanism during online processing, through which language users are able to quickly reject forms that lie beyond a certain point of acceptability, or plausibility, during the</context>
<context position="12342" citStr="Hagoort &amp; Kutas, 1995" startWordPosition="1818" endWordPosition="1821">s routine experience. A better spoken-language analogue of our grooming action condition might be something like ‘‘I like my coffee with milk and [clearing of throat],’’ though we know of no spoken-language studies which have incorporated such a condition. The non-linguistic grooming gestures used in the present study may be another example of forms that language users (in this case, signers) are able to quickly reject as non-linguistic during language processing. If this is the case, then one might also expect that such forms will not elicit an N400 but rather a positive-going component (cf. Hagoort &amp; Kutas, 1995). In summary, to the extent that semantic processing at the sentence level is similar for signed and spoken language, despite the obvious difference in modality, the ERP responses associated with our four sentence ending condition should be predictable. First, the incongruent signs should elicit a negative-going component relative to the baseline (congruent sign) condition, consistent with the classic N400 response seen for English and other spoken languages, as well as some previous ERP studies of ASL (Kutas et al., 1987; Neville et al., 1997). Second, the pseudo-signs should also elicit a ne</context>
<context position="27346" citStr="Hagoort &amp; Kutas, 1995" startWordPosition="4257" endWordPosition="4260">owing section, the outcomes for the ANOVAs performed for each time window will be given in the order midline, inner, outer, and outermost. This establishes the signiﬁcance of the results, which we ﬁrst introduce with illustrations and descriptions of the waveforms and the associated topographic maps. 3. Results 3.1. Waveforms and topographic maps Pictured in Figs. 3 and 4 are grand-average waveforms at selected electrode sites for the four sentence Ending conditions. Visual inspection of the waveforms reveals that all conditions evoked exogenous potentials often associated with written words (Hagoort &amp; Kutas, 1995); these include a posteriorly distributed positivity (P1) peaking at about 100 ms post-stimulus onset, followed by a posteriorly distributed negativity (N1) peaking at about 180 ms after target onset. Starting at approximately 300 ms after stimulus onset, we begin to see a differentiation for different sentence ending conditions which we will quantify in detail in the statistical analysis. Relative to the baseline (semantically congruent sign) condition, both the semantically incongruent and pseudosign conditions can be seen to have elicited negative-going waves. In addition, the pseudo-sign n</context>
<context position="43353" citStr="Hagoort &amp; Kutas, 1995" startWordPosition="6778" endWordPosition="6781"> in the present study was quite evident at occipital sites, as can be seen clearly in Figs. 3 and 4. A second notable ﬁnding in our study concerns deaf subjects’ ERP response in the phonologically legal pseudo-sign condition, which was also consistent with an N400 response but was generally larger (more negative) than the negativity seen for semantically incongruent but fully lexical signs. This provides further evidence for broad processing similarities for different linguistic modalities, in the light of similar ﬁndings for pseudo-words in earlier studies (Bentin, 1987; Bentin et al., 1985; Hagoort &amp; Kutas, 1995). It is interesting, however, that phonologically legal pseudo-signs did not more strongly differentiate from the semantically incongruent signs in the present study. This may be an indication that our pseudo-signs (or some of their sub-lexical components) are activating lexical representations to a substantial degree (cf. Friedrich, Eulitz, &amp; Lahiri, 2006), and that at the same time, these representations are incongruent with the sentential contexts in which they have been presented. The pseudo-sign and incongruent sign conditions shared another similarity in that the effects they elicited we</context>
</contexts>
<marker>Hagoort, Kutas, 1995</marker>
<rawString>Hagoort, P., &amp; Kutas, M. (1995). Electrophysiological insights into language deﬁcits. In F. Boller &amp; J. Grafman (Eds.), Handbook of neuropsychology (pp. 105–134). Amsterdam: Elsevier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hagoort</author>
<author>J van Berkum</author>
</authors>
<title>Beyond the sentence given.</title>
<date>2007</date>
<journal>Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences,</journal>
<volume>362</volume>
<pages>801--811</pages>
<marker>Hagoort, van Berkum, 2007</marker>
<rawString>Hagoort, P., &amp; van Berkum, J. (2007). Beyond the sentence given. Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences, 362, 801–811.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Holcomb</author>
<author>W B McPherson</author>
</authors>
<title>Event-related brain potentials reﬂect semantic priming in an object decision task.</title>
<date>1994</date>
<journal>Brain and Cognition,</journal>
<volume>24</volume>
<pages>259--276</pages>
<contexts>
<context position="42617" citStr="Holcomb &amp; McPherson, 1994" startWordPosition="6665" endWordPosition="6668">phs and videos. In Wu and Coulson’s (2005) study of contextually incongruent gestures, a component described by the researchers as a ‘‘gesture N450’’ was observed. Wu and Coulson noted the similarity of this effect to the N450 reported by Barrett and Rugg (1990) for second items in unrelated picture pairs relative to related picture pairs (e.g. wrench/fork vs. knife/fork), stating (p. 659) that consistent with their own ﬁndings, ‘‘most such ‘picture’ ERP studies report a broadly distributed negativity largest at frontal electrode sites and not evident at occipital sites (Barrett &amp; Rugg, 1990; Holcomb &amp; McPherson, 1994; McPherson &amp; Holcomb, 1999; Sitnikova et al., 2003; West &amp; Holcomb, 2002).’’ In contrast, the negativity reported in the present study was quite evident at occipital sites, as can be seen clearly in Figs. 3 and 4. A second notable ﬁnding in our study concerns deaf subjects’ ERP response in the phonologically legal pseudo-sign condition, which was also consistent with an N400 response but was generally larger (more negative) than the negativity seen for semantically incongruent but fully lexical signs. This provides further evidence for broad processing similarities for different linguistic mo</context>
</contexts>
<marker>Holcomb, McPherson, 1994</marker>
<rawString>Holcomb, P. J., &amp; McPherson, W. B. (1994). Event-related brain potentials reﬂect semantic priming in an object decision task. Brain and Cognition, 24, 259–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Holcomb</author>
<author>H J Neville</author>
</authors>
<title>Auditory and visual semantic priming in lexical decision: A comparison using event-related brain potentials.</title>
<date>1990</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<volume>5</volume>
<pages>281--312</pages>
<contexts>
<context position="44924" citStr="Holcomb and Neville (1990)" startWordPosition="7011" endWordPosition="7014">uch research might take. A third set of ﬁndings, concerning the outcome related to our non-linguistic grooming actions, is especially provocative. In contrast to the three other kinds of sentence-ﬁnal items, all of which could be considered linguistic (i.e. as actual lexical items in two cases, and phonologically legal lexical gaps in the third), the non-linguistic grooming actions elicited a large positivity. As noted earlier, phonologically illegal words in ERP studies have in some cases elicited a positive-going component rather than an N400 (Holcomb &amp; Neville, 1990; Ziegler et al., 1997). Holcomb and Neville (1990) examined differences between pseudo-words and non-words in the visual and auditory modalities in the context of a lexical decision experiment. Pseudo-words accorded with phonotactic constraints of English; visually presented non-words were composed of consonant strings and auditory non-words were words played backwards. The researchers reported that within an early time window (150– 300 ms), auditory non-words (but not visual non-words) elicited a 19 more negative response than pseudo-words, but only at anterior and right hemisphere sites. In a later time window (300–500 ms), the response to </context>
<context position="41243" citStr="Holcomb &amp; Neville, 1990" startWordPosition="6450" endWordPosition="6453">sing similarities between signed and spoken language, in spite of the obvious physical differences in the linguistic signal. For instance, Neville et al. (1997) also found that deaf signers exhibited an N400 response to semantically incongruent ASL sentences, relative to congruent sentences. Like the effects in the present study, this response was broadly distributed and had an onset and peak that the researchers noted was somewhat later M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24 than would be expected for written language, but consistent with earlier studies on auditory language (Holcomb &amp; Neville, 1990, 1991). Neville et al. suggested that this delay might be due to the fact that the recognition point of different signs will tend to vary more than for printed language, in which all information is made available at the same time. Capek et al. (2009) also found a relatively late N400 response to semantic incongruity in sign sentences; this bilateral and posteriorly prominent effect had an onset of about 300 ms post-stimulus onset and peaked at about 600 ms post-stimulus onset, very much like the negativities we have described in the present study. These effects are somewhat different from tho</context>
<context position="44873" citStr="Holcomb &amp; Neville, 1990" startWordPosition="7003" endWordPosition="7006">ay offer some insights about useful directions such research might take. A third set of ﬁndings, concerning the outcome related to our non-linguistic grooming actions, is especially provocative. In contrast to the three other kinds of sentence-ﬁnal items, all of which could be considered linguistic (i.e. as actual lexical items in two cases, and phonologically legal lexical gaps in the third), the non-linguistic grooming actions elicited a large positivity. As noted earlier, phonologically illegal words in ERP studies have in some cases elicited a positive-going component rather than an N400 (Holcomb &amp; Neville, 1990; Ziegler et al., 1997). Holcomb and Neville (1990) examined differences between pseudo-words and non-words in the visual and auditory modalities in the context of a lexical decision experiment. Pseudo-words accorded with phonotactic constraints of English; visually presented non-words were composed of consonant strings and auditory non-words were words played backwards. The researchers reported that within an early time window (150– 300 ms), auditory non-words (but not visual non-words) elicited a 19 more negative response than pseudo-words, but only at anterior and right hemisphere sites. In</context>
<context position="46998" citStr="Holcomb &amp; Neville, 1990" startWordPosition="7328" endWordPosition="7331">nses for words and pseudo-words did not signiﬁcantly differ. In a ﬁnal experiment which required a semantic categorization of the target, a negative component with a peak around 400 ms was elicited in response to words and pseudo-words. In contrast, a striking late positive component was observed in response to non-words; this lasted from 300 ms to the end of the recording period. Thus, across multiple studies we see that illegal non-words, relative to pseudo-words and real words, appear to elicit a large late positivity. This positivity has sometimes been interpreted as a P300 response (e.g. Holcomb &amp; Neville, 1990). In the present experiment, the centro-parietal distribution of the positive component elicited in the gesture condition also corresponds to the typical distribution of the P300 component. At least three interpretations of this effect may be relevant here. First, a P300 response is well-attested in studies making use of stimuli perceived by subjects to be in a low-probability category (e.g. Johnson &amp; Donchin, 1980). In our experiment, grooming gestures occurred 1/4 of the time, rendering these non-linguistic events low-probability with respect to the other three (linguistic) sentence ending c</context>
</contexts>
<marker>Holcomb, Neville, 1990</marker>
<rawString>Holcomb, P. J., &amp; Neville, H. J. (1990). Auditory and visual semantic priming in lexical decision: A comparison using event-related brain potentials. Language and Cognitive Processes, 5, 281–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Holcomb</author>
<author>H J Neville</author>
</authors>
<title>Natural speech processing: An analysis using event-related brain potentials.</title>
<date>1991</date>
<journal>Psychobiology,</journal>
<volume>19</volume>
<pages>286--300</pages>
<contexts>
<context position="6297" citStr="Holcomb &amp; Neville, 1991" startWordPosition="922" endWordPosition="925">resent paper was to use real-time electrophysiological measures to assess empirically the time course of sentence processing in cases where subjects encountered non-linguistic manual forms (here ‘‘self-grooming’’ behaviors, e.g. scratching the face, rubbing one’s eye, adjusting the sleeves of a shirt, etc.). We sought to compare the processing of these non-linguistic gestural forms within a sentential context to cases in which deaf signers encountered violations of semantic expectancy that have been observed to elicit a well-deﬁned electrophysiological component, the N400. The N400 component (Holcomb &amp; Neville, 1991; Kutas &amp; Hillyard, 1980) has been frequently investigated in previous ERP research on written, spoken and signed language (e.g. Capek et al., 2009; Kutas, Neville, &amp; Holcomb, 1987). The N400 is a broad negative deﬂection generally seen at central and parietal scalp sites that peaks about 400 ms after the visual or auditory presentation of a word. Although all content words elicit an N400 component, the ERP response is larger for words that are semantically anomalous or less expected (Hagoort &amp; Brown, 1994; Kutas &amp; Hillyard, 1984); thus the N400 is often interpreted as an index of ease or difﬁ</context>
</contexts>
<marker>Holcomb, Neville, 1991</marker>
<rawString>Holcomb, P. J., &amp; Neville, H. J. (1991). Natural speech processing: An analysis using event-related brain potentials. Psychobiology, 19, 286–300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Holle</author>
<author>T C Gunter</author>
</authors>
<title>The role of iconic gestures in speech disambiguation: ERP evidence.</title>
<date>2007</date>
<journal>Journal of Cognitive Neuroscience,</journal>
<volume>19</volume>
<pages>1175--1192</pages>
<contexts>
<context position="49902" citStr="Holle &amp; Gunter, 2007" startWordPosition="7758" endWordPosition="7761"> ERP studies of spoken, written and signed language; as well as the positive waveforms seen in response to the non-linguistic gesture stimuli in the present study; all support the prediction that phonologically illegal non-signs would elicit a positive-going waveform. However, conﬁrmation of this must await future research. 20 M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24 We have already alluded to the growing number of studies which have used ERP methodology to examine the contributions of co-speech manual gestures to the interpretation of both linguistic and non-linguistic stimuli (Holle &amp; Gunter, 2007; Kelly et al., 2004; Ozyürek, Willems, Kita, &amp; Hagoort, 2007; Wu &amp; Coulson, 2005, 2007a, 2007b). Many of these studies have used iconic manual gestures that depict a salient visual–spatial property of concrete objects, such as their size and shape or an associated manner of movement (but see also Cornejo et al., 2009). Collectively these studies suggest that co-speech manual gestures inﬂuence semantic representations, and that discrepancies between gestural forms and the semantic contexts in which they occur lead to greater processing costs on the part of language perceivers. This in turn res</context>
</contexts>
<marker>Holle, Gunter, 2007</marker>
<rawString>Holle, H., &amp; Gunter, T. C. (2007). The role of iconic gestures in speech disambiguation: ERP evidence. Journal of Cognitive Neuroscience, 19, 1175–1192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B W Johnson</author>
<author>J P Hamm</author>
</authors>
<title>High-density mapping in an N400 paradigm: Evidence for bilateral temporal lobe generators.</title>
<date>2000</date>
<journal>Clinical Neurophysiology,</journal>
<volume>111</volume>
<pages>532--545</pages>
<contexts>
<context position="57813" citStr="Johnson and Hamm (2000)" startWordPosition="8972" endWordPosition="8975"> while in Corina et al. (2007) the sign and gesture stimuli were seen in isolation. 21 M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24 120 unique gesture stimuli, the actions were performed with differences in the number and conﬁguration of hands or ﬁngers used, the location of the body involved, and so on. Also shown in the rightmost two columns of the table are the ‘‘correct’’ and ‘‘incorrect’’ word choices for the occasional quiz items. A number sign (#) preceding an item means that item was ﬁngerspelled. Many of these sentences were adapted from the English-language stimuli used in Johnson and Hamm (2000). Table A1 FRAME Semantically congruent Semantically incongruent Pseudo-sign Correct Incorrect 1 2 3 4 5 6 7 8 9 10 BOY SLEEP IN HIS HEAR BARK BARK LOOK RIGHT PRO MAN PRO CARPENTER BUILD DOOR PRO LOCK LOOK-FOR WATER+CL:pond PRO CL:many-across MOTORCYCLE BROKE-DOWN-REPEATEDLY FINALLY BOUGHT NEW PRO3 BECOME-ILL SICK CAN’T GO-TO MY HOUSE LIGHTS BLACKOUT-POW CL:set-up-around DOG ANGRY CHASE #PATIENT SIT-HABITUAL ANALYZE-PRO1, PRO3 LEMON SECRET HAIRCUT NOSE WORD GHOST BOTTLE ELEPHANT EASTER FRANCE BARK.A’ HOUSE.V NOSE.V’’ G_2H (contact 2x) GHOST.S WRIST.CS (open &amp; close) NOSE.5 (2x) KNOCK.8 KNOCK.G</context>
</contexts>
<marker>Johnson, Hamm, 2000</marker>
<rawString>Johnson, B. W., &amp; Hamm, J. P. (2000). High-density mapping in an N400 paradigm: Evidence for bilateral temporal lobe generators. Clinical Neurophysiology, 111, 532–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johnson</author>
<author>E Donchin</author>
</authors>
<title>P300 and stimulus categorization: Two plus one is not so different from one plus one.</title>
<date>1980</date>
<journal>Psychophysiology,</journal>
<volume>17</volume>
<pages>167--178</pages>
<contexts>
<context position="47417" citStr="Johnson &amp; Donchin, 1980" startWordPosition="7391" endWordPosition="7394">e that illegal non-words, relative to pseudo-words and real words, appear to elicit a large late positivity. This positivity has sometimes been interpreted as a P300 response (e.g. Holcomb &amp; Neville, 1990). In the present experiment, the centro-parietal distribution of the positive component elicited in the gesture condition also corresponds to the typical distribution of the P300 component. At least three interpretations of this effect may be relevant here. First, a P300 response is well-attested in studies making use of stimuli perceived by subjects to be in a low-probability category (e.g. Johnson &amp; Donchin, 1980). In our experiment, grooming gestures occurred 1/4 of the time, rendering these non-linguistic events low-probability with respect to the other three (linguistic) sentence ending conditions. Second, ERP differences between non-words and words have been attributed to the fact that these non-linguistic items have little or nothing in common with lexical entries and therefore do not generate lexical activity (cf. Rugg &amp; Nagy, 1987). Third, the ERP differences observed between pseudo-words and non-words have also been suggested to reﬂect a pre-lexical ﬁltering process that quickly rejects non-lin</context>
</contexts>
<marker>Johnson, Donchin, 1980</marker>
<rawString>Johnson, R., Jr, &amp; Donchin, E. (1980). P300 and stimulus categorization: Two plus one is not so different from one plus one. Psychophysiology, 17, 167–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kegl</author>
<author>A Senghas</author>
<author>M Coppola</author>
</authors>
<title>Creation through contact: Sign language emergence and sign language change in Nicaragua. In</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3846" citStr="Kegl, Senghas, &amp; Coppola, 1999" startWordPosition="562" endWordPosition="566">ine, 101 The City Drive South, Bldg. 53, Room 204, Orange, CA 928684280, United States. Fax: +1 714 456 1697. E-mail address: m.grosvald@uci.edu (M. Grosvald). 0093-934X/$ - see front matter Ó 2012 Elsevier Inc. All rights reserved. doi:10.1016/j.bandl.2012.01.005 cently, linguists have documented compelling evidence that the development of nascent sign languages derives from idiosyncratic gestural and pantomimic systems used by isolated communities, which in some cases may be limited to individual families who have a need to communicate with a deaf child (Frishberg, 1987; GoldinMeadow, 2003; Kegl, Senghas, &amp; Coppola, 1999; Meir, Sandler, Padden, &amp; Aronoff, 2010; Morford &amp; Kegl, 2000; Senghas, 2005). Even within mature sign languages of Deaf communities, linguistic accounts of sign language structure have also argued that lexical and discourse components of American Sign Language (ASL) and other signed languages may be best understood as being gesturally based (Liddell, 2003). Thus diachronic and synchronic evidence from language research support the contention that signed languages might make use of perceptual systems similar to those through which humans understand or parse human actions and gestures more gen</context>
</contexts>
<marker>Kegl, Senghas, Coppola, 1999</marker>
<rawString>Kegl, J., Senghas, A., &amp; Coppola, M. (1999). Creation through contact: Sign language emergence and sign language change in Nicaragua. In M. DeGraff (Ed.), Language creation and language change: Creolization, diachrony, and development (pp. 179–237). Cambridge MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S D Kelly</author>
<author>C Kravitz</author>
<author>M Hopkins</author>
</authors>
<title>Neural correlates of bimodal speech and gesture comprehension.</title>
<date>2004</date>
<journal>Brain and Language,</journal>
<volume>89</volume>
<pages>243--260</pages>
<contexts>
<context position="8781" citStr="Kelly, Kravitz, &amp; Hopkins, 2004" startWordPosition="1298" endWordPosition="1302">ty, during the ongoing processing of the incoming language stream.1 The N400 (or N400-like responses) can also be observed in numerous contexts involving non-linguistic but meaningful stimuli, such as pictures (Ganis &amp; Kutas, 2003; Ganis, Kutas, &amp; Sereno, 1996; Nigam, Hoffman, &amp; Simons, 1992; Pratarelli, 1994), faces (Barrett &amp; Rugg, 1989; Bobes, Valdés-Sosa, &amp; Olivares, 1994), environmental noises (Chao, Nielsen-Bohlman, &amp; Knight, 1995; Van Petten &amp; Rheinfelder, 1995), movie clips (Sitnikova, Holcomb, Kiyonaga, &amp; Kuperberg, 2008; Sitnikova, Kuperberg, &amp; Holcomb, 2003) and co-speech gestures (Kelly, Kravitz, &amp; Hopkins, 2004; Wu &amp; Coulson, 2005). Linguistically anomalous stimuli are not always associated with an N400 response. For example, the left anterior negativity (LAN; Friederici, 2002; Neville, Nicol, Barss, Forster, &amp; Garrett, 1991) and P600 (Osterhout &amp; Holcomb, 1992) are well-known ERP components that have been found in syntactic violation contexts in 1 This possibility is bolstered by recent work of Albert Kim and colleagues, who have found that relative to real word controls, N400 amplitude decreases and P600 amplitude increases, parametrically, as orthographic irregularity increases (Kim &amp; Pitkänen, s</context>
<context position="11179" citStr="Kelly et al., 2004" startWordPosition="1640" endWordPosition="1643">phonologically legal pseudo-signs. The ERP response for the non-linguistic gesture condition is a priori more difﬁcult to predict. Previous neuro-imaging studies of deaf signers have reported differences in patterns of activation associated with the perception of signs compared to non-linguistic gestures (Corina et al., 2007; Emmorey et al., 2010; MacSweeney et al., 2004), but the methodologies used in those studies lacked the temporal resolution to determine at what stage of processing these differences may occur. While N400-like responses have been elicited to co-speech gestural mismatches (Kelly et al., 2004; Wu &amp; Coulson, 2005), in our study, gestures occur in place of semantically appropriate sentence-ending items, rather than as a possible accompaniment. It should also be borne in mind that the relationship of signs and grooming gestures is probably not quite akin to that between standard lexical items in spoken language and the orthographically/phonotactically illegal pseudo-words used in earlier ERP studies. Unlike grooming gestures, which are part of everyday life, illegal non-words like ‘‘dkfpst’’ are probably alien to most people’s routine experience. A better spoken-language analogue of </context>
<context position="49922" citStr="Kelly et al., 2004" startWordPosition="7762" endWordPosition="7765">, written and signed language; as well as the positive waveforms seen in response to the non-linguistic gesture stimuli in the present study; all support the prediction that phonologically illegal non-signs would elicit a positive-going waveform. However, conﬁrmation of this must await future research. 20 M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24 We have already alluded to the growing number of studies which have used ERP methodology to examine the contributions of co-speech manual gestures to the interpretation of both linguistic and non-linguistic stimuli (Holle &amp; Gunter, 2007; Kelly et al., 2004; Ozyürek, Willems, Kita, &amp; Hagoort, 2007; Wu &amp; Coulson, 2005, 2007a, 2007b). Many of these studies have used iconic manual gestures that depict a salient visual–spatial property of concrete objects, such as their size and shape or an associated manner of movement (but see also Cornejo et al., 2009). Collectively these studies suggest that co-speech manual gestures inﬂuence semantic representations, and that discrepancies between gestural forms and the semantic contexts in which they occur lead to greater processing costs on the part of language perceivers. This in turn results in increased ne</context>
</contexts>
<marker>Kelly, Kravitz, Hopkins, 2004</marker>
<rawString>Kelly, S. D., Kravitz, C., &amp; Hopkins, M. (2004). Neural correlates of bimodal speech and gesture comprehension. Brain and Language, 89, 243–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kim</author>
<author>L Osterhout</author>
</authors>
<title>The independence of combinatory semantic processing: Evidence from event-related potentials.</title>
<date>2005</date>
<journal>Journal of Memory and Language,</journal>
<volume>52</volume>
<pages>205--225</pages>
<contexts>
<context position="25406" citStr="Kim &amp; Osterhout, 2005" startWordPosition="3949" endWordPosition="3952">elorme &amp; Makeig, 2004). Epochs began 200 ms before stimulus onset and ended 1000 ms after. Inspection of subjects’ EEG data was performed by eye to check rejections suggested by a script run in ERPLAB whose artifact rejection thresholds were set at ±120 lV. For all 16 subjects, in each of the four sentence ending conditions at least 20 of the original 30 trials remained after the rejection procedure just described. The statistical analyses reported below were carried out using the SPSS statistical package. To assess the signiﬁcance of the observed effects, a column analysis was conducted (cf. Kim &amp; Osterhout, 2005) for which a separate ANOVA was run on each of four subsets of the scalp sites, as illustrated in Fig. 2. For the midline scalp sites, colored green in the ﬁgure, the two factors in the ANOVA were Electrode (one level for each of the four electrodes) and sentence Ending (semantically congruent sign, semantically incongruent sign, pseudo-sign and grooming gesture). The other three ANOVAs, corresponding to the inner (colored blue in the ﬁgure), outer (purple), and outermost (orange) sites, included a third factor of hemisphere (left or right); in addition, for these three ANOVAs the factor Elect</context>
</contexts>
<marker>Kim, Osterhout, 2005</marker>
<rawString>Kim, A., &amp; Osterhout, L. (2005). The independence of combinatory semantic processing: Evidence from event-related potentials. Journal of Memory and Language, 52, 205–225. Kim, A., &amp; Pitkänen, I. (submitted for publication). Dissociation of ERPs to structural and semantic processing difﬁculty during sentence-embedded pseudoword processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kutas</author>
<author>S A Hillyard</author>
</authors>
<title>Reading senseless sentences: Brain potentials reﬂect semantic incongruity.</title>
<date>1980</date>
<journal>Science,</journal>
<volume>207</volume>
<pages>203--208</pages>
<contexts>
<context position="6322" citStr="Kutas &amp; Hillyard, 1980" startWordPosition="926" endWordPosition="929">eal-time electrophysiological measures to assess empirically the time course of sentence processing in cases where subjects encountered non-linguistic manual forms (here ‘‘self-grooming’’ behaviors, e.g. scratching the face, rubbing one’s eye, adjusting the sleeves of a shirt, etc.). We sought to compare the processing of these non-linguistic gestural forms within a sentential context to cases in which deaf signers encountered violations of semantic expectancy that have been observed to elicit a well-deﬁned electrophysiological component, the N400. The N400 component (Holcomb &amp; Neville, 1991; Kutas &amp; Hillyard, 1980) has been frequently investigated in previous ERP research on written, spoken and signed language (e.g. Capek et al., 2009; Kutas, Neville, &amp; Holcomb, 1987). The N400 is a broad negative deﬂection generally seen at central and parietal scalp sites that peaks about 400 ms after the visual or auditory presentation of a word. Although all content words elicit an N400 component, the ERP response is larger for words that are semantically anomalous or less expected (Hagoort &amp; Brown, 1994; Kutas &amp; Hillyard, 1984); thus the N400 is often interpreted as an index of ease or difﬁculty in semantic concept</context>
<context position="50701" citStr="Kutas &amp; Hillyard, 1980" startWordPosition="7881" endWordPosition="7884">isual–spatial property of concrete objects, such as their size and shape or an associated manner of movement (but see also Cornejo et al., 2009). Collectively these studies suggest that co-speech manual gestures inﬂuence semantic representations, and that discrepancies between gestural forms and the semantic contexts in which they occur lead to greater processing costs on the part of language perceivers. This in turn results in increased negativities in the time window often associated with the classic N400 effect, observed in response to word meanings that violate the wider semantic context (Kutas &amp; Hillyard, 1980). For example, Kelly et al. (2004) observed modulation of ERP responses for speech tokens that were either accompanied by matching, complementary or mismatched hand gestures. An N400-like component was observed for mismatched gesture-speech tokens relative to the other conditions. Wu and Coulson (2005) examined ERPs for subjects who watched cartoons followed by a gestural depiction that either matched or mismatched the events shown in the cartoons. Gestures elicited an N400-like component (a socalled ‘‘gesture N450’’) that was larger for incongruent than congruent items. Ozyürek et al. (2007) </context>
</contexts>
<marker>Kutas, Hillyard, 1980</marker>
<rawString>Kutas, M., &amp; Hillyard, S. A. (1980). Reading senseless sentences: Brain potentials reﬂect semantic incongruity. Science, 207, 203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kutas</author>
<author>S A Hillyard</author>
</authors>
<title>Brain potentials during reading reﬂect word expectancy and semantic association.</title>
<date>1984</date>
<journal>Nature,</journal>
<volume>307</volume>
<pages>161--163</pages>
<contexts>
<context position="6833" citStr="Kutas &amp; Hillyard, 1984" startWordPosition="1009" endWordPosition="1012">d electrophysiological component, the N400. The N400 component (Holcomb &amp; Neville, 1991; Kutas &amp; Hillyard, 1980) has been frequently investigated in previous ERP research on written, spoken and signed language (e.g. Capek et al., 2009; Kutas, Neville, &amp; Holcomb, 1987). The N400 is a broad negative deﬂection generally seen at central and parietal scalp sites that peaks about 400 ms after the visual or auditory presentation of a word. Although all content words elicit an N400 component, the ERP response is larger for words that are semantically anomalous or less expected (Hagoort &amp; Brown, 1994; Kutas &amp; Hillyard, 1984); thus the N400 is often interpreted as an index of ease or difﬁculty in semantic conceptual integration (Brown &amp; Hagoort, 1993; Hagoort &amp; Van Berkum, 2007). For example, for listeners encountering the two sentences ‘‘I like my coffee with milk and sugar’’ and ‘‘I like my coffee with milk and mud,’’ the N400 response to the last word in the second item is expected to be larger. An N400 or N400-like component can also be found in response to orthographically/phonologically legal but non-occurring ‘‘pseudo-words’’ (e.g. ‘‘blork’’), and it has sometimes been reported that pseudo-words elicit a st</context>
</contexts>
<marker>Kutas, Hillyard, 1984</marker>
<rawString>Kutas, M., &amp; Hillyard, S. A. (1984). Brain potentials during reading reﬂect word expectancy and semantic association. Nature, 307, 161–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kutas</author>
<author>H J Neville</author>
<author>P J Holcomb</author>
</authors>
<title>A preliminary comparison of the N400 response to semantic anomalies during reading, listening, and signing.</title>
<date>1987</date>
<journal>Electroencephalography and Clinical Neurophysiology, Supplement,</journal>
<volume>39</volume>
<pages>325--330</pages>
<contexts>
<context position="6477" citStr="Kutas, Neville, &amp; Holcomb, 1987" startWordPosition="949" endWordPosition="953">tic manual forms (here ‘‘self-grooming’’ behaviors, e.g. scratching the face, rubbing one’s eye, adjusting the sleeves of a shirt, etc.). We sought to compare the processing of these non-linguistic gestural forms within a sentential context to cases in which deaf signers encountered violations of semantic expectancy that have been observed to elicit a well-deﬁned electrophysiological component, the N400. The N400 component (Holcomb &amp; Neville, 1991; Kutas &amp; Hillyard, 1980) has been frequently investigated in previous ERP research on written, spoken and signed language (e.g. Capek et al., 2009; Kutas, Neville, &amp; Holcomb, 1987). The N400 is a broad negative deﬂection generally seen at central and parietal scalp sites that peaks about 400 ms after the visual or auditory presentation of a word. Although all content words elicit an N400 component, the ERP response is larger for words that are semantically anomalous or less expected (Hagoort &amp; Brown, 1994; Kutas &amp; Hillyard, 1984); thus the N400 is often interpreted as an index of ease or difﬁculty in semantic conceptual integration (Brown &amp; Hagoort, 1993; Hagoort &amp; Van Berkum, 2007). For example, for listeners encountering the two sentences ‘‘I like my coffee with milk</context>
<context position="12869" citStr="Kutas et al., 1987" startWordPosition="1898" endWordPosition="1901">s will not elicit an N400 but rather a positive-going component (cf. Hagoort &amp; Kutas, 1995). In summary, to the extent that semantic processing at the sentence level is similar for signed and spoken language, despite the obvious difference in modality, the ERP responses associated with our four sentence ending condition should be predictable. First, the incongruent signs should elicit a negative-going component relative to the baseline (congruent sign) condition, consistent with the classic N400 response seen for English and other spoken languages, as well as some previous ERP studies of ASL (Kutas et al., 1987; Neville et al., 1997). Second, the pseudo-signs should also elicit a negative-going wave, and this response can be expected to be of larger magnitude (i.e. be more negative) than that seen for the incongruent signs. Third, while the likely response to the grooming gesture condition is more difﬁcult to predict, we may expect to see a positive-going component relative to the baseline. 2. Methodology 2.1. Participants The 16 participants (12 female and 4 male; age range = [19, 45], mean = 25.4 and SD = 8.3) were deaf users of ASL; all were students 14 M. Grosvald et al. / Brain &amp; Language 121 (</context>
<context position="40269" citStr="Kutas et al., 1987" startWordPosition="6307" endWordPosition="6310">sings. In addition, we suggested that the non-linguistic gestures might elicit a positive-going wave, relative to baseline. While this last prediction was more speculative than the others, the expected pattern was in fact observed very consistently in our analysis of the data. Together, these ﬁndings lead to a number of important observations. First, the outcome of our incongruent sign vs. congruent sign comparison replicates earlier ﬁndings which have also indicated that the N400 generalizes across modalities, including the visual– manual modality of signed language (e.g. Capek et al., 2009; Kutas et al., 1987). These ﬁndings are broadly consistent with other studies using a variety of methodologies including positron emission tomography (PET; Corina, San Jose-Robertson, Guillemin, High, &amp; Braun, 2003), functional magnetic resonance imaging (fMRI; Neville et al., 1998), and cortical stimulation mapping (Corina et al., 1999), highlighting key neural processing similarities between signed and spoken language, in spite of the obvious physical differences in the linguistic signal. For instance, Neville et al. (1997) also found that deaf signers exhibited an N400 response to semantically incongruent ASL </context>
</contexts>
<marker>Kutas, Neville, Holcomb, 1987</marker>
<rawString>Kutas, M., Neville, H. J., &amp; Holcomb, P. J. (1987). A preliminary comparison of the N400 response to semantic anomalies during reading, listening, and signing. Electroencephalography and Clinical Neurophysiology, Supplement, 39, 325–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K Liddell</author>
</authors>
<title>Grammar, gesture, and meaning in American Sign Language.</title>
<date>2003</date>
<booktitle>In development at the</booktitle>
<publisher>Cambridge University</publisher>
<institution>Center for Mind and Brain, University of California at Davis.</institution>
<location>Cambridge, UK:</location>
<contexts>
<context position="4206" citStr="Liddell, 2003" startWordPosition="618" endWordPosition="619">om idiosyncratic gestural and pantomimic systems used by isolated communities, which in some cases may be limited to individual families who have a need to communicate with a deaf child (Frishberg, 1987; GoldinMeadow, 2003; Kegl, Senghas, &amp; Coppola, 1999; Meir, Sandler, Padden, &amp; Aronoff, 2010; Morford &amp; Kegl, 2000; Senghas, 2005). Even within mature sign languages of Deaf communities, linguistic accounts of sign language structure have also argued that lexical and discourse components of American Sign Language (ASL) and other signed languages may be best understood as being gesturally based (Liddell, 2003). Thus diachronic and synchronic evidence from language research support the contention that signed languages might make use of perceptual systems similar to those through which humans understand or parse human actions and gestures more generally (Corballis, 2009). In contrast, given its linguistic status, sign language perception may require the attunement of specialized systems for recognizing sign forms. A comprehensive theory of sign language recognition will be enhanced by providing an account of when and how the processing of sign forms diverges from the processing of human actions in ge</context>
</contexts>
<marker>Liddell, 2003</marker>
<rawString>Liddell, S. K. (2003). Grammar, gesture, and meaning in American Sign Language. Cambridge, UK: Cambridge University Press. Lopez-Calderon, J., &amp; Luck, S. (in press). ERPLAB. Plug-in for EEGLAB. In development at the Center for Mind and Brain, University of California at Davis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M MacSweeney</author>
<author>R Campbell</author>
<author>B Woll</author>
<author>V Giampietro</author>
<author>A S David</author>
<author>P K McGuire</author>
</authors>
<title>Dissociating linguistic and nonlinguistic gestural communication in the brain.</title>
<date>2004</date>
<journal>NeuroImage,</journal>
<volume>22</volume>
<pages>1605--1618</pages>
<contexts>
<context position="5095" citStr="MacSweeney et al., 2004" startWordPosition="744" endWordPosition="747"> In contrast, given its linguistic status, sign language perception may require the attunement of specialized systems for recognizing sign forms. A comprehensive theory of sign language recognition will be enhanced by providing an account of when and how the processing of sign forms diverges from the processing of human actions in general. Recent behavioral and neuro-imaging studies have reported differences in deaf subjects’ responses to single signs compared to non-linguistic gestures (Corina, Grosvald, &amp; Lachaud, 2011; Corina et al., 2007; Emmorey, Xu, Gannon, Goldin-Meadow, &amp; Braun, 2010; MacSweeney et al., 2004), but no studies to our knowledge have examined the recognition of signs and gestures M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24 under sentence processing constraints. Consider for example the signer, who, in mid-sentence, fulﬁlls the urge to scratch his face, or perhaps swat away a ﬂying insect. What is the fate of this non-linguistic articulation? Does the sign perceiver attempt to incorporate these manual behaviors into accruing sentential representations, or are these actions easily tagged as non-linguistic and thus rejected by the parser? The goal of the present paper was to u</context>
<context position="10935" citStr="MacSweeney et al., 2004" startWordPosition="1604" endWordPosition="1607">linguistic grooming gestures. Based upon previous studies, we expected a gradation of N400-like responses across conditions, with N400 effects of smaller magnitude for semantically incongruent endings and of larger magnitude (i.e. more negative) for phonologically legal pseudo-signs. The ERP response for the non-linguistic gesture condition is a priori more difﬁcult to predict. Previous neuro-imaging studies of deaf signers have reported differences in patterns of activation associated with the perception of signs compared to non-linguistic gestures (Corina et al., 2007; Emmorey et al., 2010; MacSweeney et al., 2004), but the methodologies used in those studies lacked the temporal resolution to determine at what stage of processing these differences may occur. While N400-like responses have been elicited to co-speech gestural mismatches (Kelly et al., 2004; Wu &amp; Coulson, 2005), in our study, gestures occur in place of semantically appropriate sentence-ending items, rather than as a possible accompaniment. It should also be borne in mind that the relationship of signs and grooming gestures is probably not quite akin to that between standard lexical items in spoken language and the orthographically/phonotac</context>
</contexts>
<marker>MacSweeney, Campbell, Woll, Giampietro, David, McGuire, 2004</marker>
<rawString>MacSweeney, M., Campbell, R., Woll, B., Giampietro, V., David, A. S., McGuire, P. K., et al. (2004). Dissociating linguistic and nonlinguistic gestural communication in the brain. NeuroImage, 22, 1605–1618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B McPherson</author>
<author>P J Holcomb</author>
</authors>
<title>An electrophysiological investigation of semantic priming with pictures of real objects.</title>
<date>1999</date>
<journal>Psychophysiology,</journal>
<volume>36</volume>
<pages>53--65</pages>
<contexts>
<context position="42644" citStr="McPherson &amp; Holcomb, 1999" startWordPosition="6669" endWordPosition="6672">oulson’s (2005) study of contextually incongruent gestures, a component described by the researchers as a ‘‘gesture N450’’ was observed. Wu and Coulson noted the similarity of this effect to the N450 reported by Barrett and Rugg (1990) for second items in unrelated picture pairs relative to related picture pairs (e.g. wrench/fork vs. knife/fork), stating (p. 659) that consistent with their own ﬁndings, ‘‘most such ‘picture’ ERP studies report a broadly distributed negativity largest at frontal electrode sites and not evident at occipital sites (Barrett &amp; Rugg, 1990; Holcomb &amp; McPherson, 1994; McPherson &amp; Holcomb, 1999; Sitnikova et al., 2003; West &amp; Holcomb, 2002).’’ In contrast, the negativity reported in the present study was quite evident at occipital sites, as can be seen clearly in Figs. 3 and 4. A second notable ﬁnding in our study concerns deaf subjects’ ERP response in the phonologically legal pseudo-sign condition, which was also consistent with an N400 response but was generally larger (more negative) than the negativity seen for semantically incongruent but fully lexical signs. This provides further evidence for broad processing similarities for different linguistic modalities, in the light of s</context>
</contexts>
<marker>McPherson, Holcomb, 1999</marker>
<rawString>McPherson, W. B., &amp; Holcomb, P. J. (1999). An electrophysiological investigation of semantic priming with pictures of real objects. Psychophysiology, 36, 53–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Meir</author>
<author>W Sandler</author>
<author>C Padden</author>
<author>M Aronoff</author>
</authors>
<title>Emerging sign languages. In</title>
<date>2010</date>
<volume>2</volume>
<publisher>University Press.</publisher>
<location>New York: Oxford</location>
<contexts>
<context position="3886" citStr="Meir, Sandler, Padden, &amp; Aronoff, 2010" startWordPosition="567" endWordPosition="572">ldg. 53, Room 204, Orange, CA 928684280, United States. Fax: +1 714 456 1697. E-mail address: m.grosvald@uci.edu (M. Grosvald). 0093-934X/$ - see front matter Ó 2012 Elsevier Inc. All rights reserved. doi:10.1016/j.bandl.2012.01.005 cently, linguists have documented compelling evidence that the development of nascent sign languages derives from idiosyncratic gestural and pantomimic systems used by isolated communities, which in some cases may be limited to individual families who have a need to communicate with a deaf child (Frishberg, 1987; GoldinMeadow, 2003; Kegl, Senghas, &amp; Coppola, 1999; Meir, Sandler, Padden, &amp; Aronoff, 2010; Morford &amp; Kegl, 2000; Senghas, 2005). Even within mature sign languages of Deaf communities, linguistic accounts of sign language structure have also argued that lexical and discourse components of American Sign Language (ASL) and other signed languages may be best understood as being gesturally based (Liddell, 2003). Thus diachronic and synchronic evidence from language research support the contention that signed languages might make use of perceptual systems similar to those through which humans understand or parse human actions and gestures more generally (Corballis, 2009). In contrast, g</context>
</contexts>
<marker>Meir, Sandler, Padden, Aronoff, 2010</marker>
<rawString>Meir, I., Sandler, W., Padden, C., &amp; Aronoff, M. (2010). Emerging sign languages. In M. Marschark &amp; P. Spencer (Eds.). Oxford handbook of deaf studies, language, and education (Vol. 2). New York: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Morford</author>
<author>J A Kegl</author>
</authors>
<title>Gestural precursors to linguistic constructs: How input shapes the form of language. In</title>
<date>2000</date>
<booktitle>Language and gesture</booktitle>
<pages>358--387</pages>
<editor>D. McNeill (Ed.),</editor>
<publisher>Cambridge University Press.</publisher>
<location>Cambridge, UK:</location>
<contexts>
<context position="3908" citStr="Morford &amp; Kegl, 2000" startWordPosition="573" endWordPosition="576"> United States. Fax: +1 714 456 1697. E-mail address: m.grosvald@uci.edu (M. Grosvald). 0093-934X/$ - see front matter Ó 2012 Elsevier Inc. All rights reserved. doi:10.1016/j.bandl.2012.01.005 cently, linguists have documented compelling evidence that the development of nascent sign languages derives from idiosyncratic gestural and pantomimic systems used by isolated communities, which in some cases may be limited to individual families who have a need to communicate with a deaf child (Frishberg, 1987; GoldinMeadow, 2003; Kegl, Senghas, &amp; Coppola, 1999; Meir, Sandler, Padden, &amp; Aronoff, 2010; Morford &amp; Kegl, 2000; Senghas, 2005). Even within mature sign languages of Deaf communities, linguistic accounts of sign language structure have also argued that lexical and discourse components of American Sign Language (ASL) and other signed languages may be best understood as being gesturally based (Liddell, 2003). Thus diachronic and synchronic evidence from language research support the contention that signed languages might make use of perceptual systems similar to those through which humans understand or parse human actions and gestures more generally (Corballis, 2009). In contrast, given its linguistic st</context>
</contexts>
<marker>Morford, Kegl, 2000</marker>
<rawString>Morford, J. P., &amp; Kegl, J. A. (2000). Gestural precursors to linguistic constructs: How input shapes the form of language. In D. McNeill (Ed.), Language and gesture (pp. 358–387). Cambridge, UK: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H J Neville</author>
<author>D Bavelier</author>
<author>D Corina</author>
<author>J Rauschecker</author>
<author>A Karni</author>
<author>A Lalwani</author>
</authors>
<title>Cerebral organization for language in deaf and hearing subjects: Biological constraints and effects of experience.</title>
<date>1998</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States of America,</booktitle>
<volume>95</volume>
<pages>922--929</pages>
<contexts>
<context position="40532" citStr="Neville et al., 1998" startWordPosition="6342" endWordPosition="6345">sis of the data. Together, these ﬁndings lead to a number of important observations. First, the outcome of our incongruent sign vs. congruent sign comparison replicates earlier ﬁndings which have also indicated that the N400 generalizes across modalities, including the visual– manual modality of signed language (e.g. Capek et al., 2009; Kutas et al., 1987). These ﬁndings are broadly consistent with other studies using a variety of methodologies including positron emission tomography (PET; Corina, San Jose-Robertson, Guillemin, High, &amp; Braun, 2003), functional magnetic resonance imaging (fMRI; Neville et al., 1998), and cortical stimulation mapping (Corina et al., 1999), highlighting key neural processing similarities between signed and spoken language, in spite of the obvious physical differences in the linguistic signal. For instance, Neville et al. (1997) also found that deaf signers exhibited an N400 response to semantically incongruent ASL sentences, relative to congruent sentences. Like the effects in the present study, this response was broadly distributed and had an onset and peak that the researchers noted was somewhat later M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24 than would be e</context>
</contexts>
<marker>Neville, Bavelier, Corina, Rauschecker, Karni, Lalwani, 1998</marker>
<rawString>Neville, H. J., Bavelier, D., Corina, D., Rauschecker, J., Karni, A., Lalwani, A., et al. (1998). Cerebral organization for language in deaf and hearing subjects: Biological constraints and effects of experience. Proceedings of the National Academy of Sciences of the United States of America, 95, 922–929.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H J Neville</author>
<author>S A Coffey</author>
<author>D S Lawson</author>
<author>A Fischer</author>
<author>K Emmorey</author>
<author>U Bellugi</author>
</authors>
<title>Neural systems mediating American Sign Language: Effects of sensory experience and age of acquisition.</title>
<date>1997</date>
<journal>Brain and Language,</journal>
<pages>285--308</pages>
<contexts>
<context position="12892" citStr="Neville et al., 1997" startWordPosition="1902" endWordPosition="1905"> N400 but rather a positive-going component (cf. Hagoort &amp; Kutas, 1995). In summary, to the extent that semantic processing at the sentence level is similar for signed and spoken language, despite the obvious difference in modality, the ERP responses associated with our four sentence ending condition should be predictable. First, the incongruent signs should elicit a negative-going component relative to the baseline (congruent sign) condition, consistent with the classic N400 response seen for English and other spoken languages, as well as some previous ERP studies of ASL (Kutas et al., 1987; Neville et al., 1997). Second, the pseudo-signs should also elicit a negative-going wave, and this response can be expected to be of larger magnitude (i.e. be more negative) than that seen for the incongruent signs. Third, while the likely response to the grooming gesture condition is more difﬁcult to predict, we may expect to see a positive-going component relative to the baseline. 2. Methodology 2.1. Participants The 16 participants (12 female and 4 male; age range = [19, 45], mean = 25.4 and SD = 8.3) were deaf users of ASL; all were students 14 M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24 or staff at</context>
<context position="40780" citStr="Neville et al. (1997)" startWordPosition="6377" endWordPosition="6380">lities, including the visual– manual modality of signed language (e.g. Capek et al., 2009; Kutas et al., 1987). These ﬁndings are broadly consistent with other studies using a variety of methodologies including positron emission tomography (PET; Corina, San Jose-Robertson, Guillemin, High, &amp; Braun, 2003), functional magnetic resonance imaging (fMRI; Neville et al., 1998), and cortical stimulation mapping (Corina et al., 1999), highlighting key neural processing similarities between signed and spoken language, in spite of the obvious physical differences in the linguistic signal. For instance, Neville et al. (1997) also found that deaf signers exhibited an N400 response to semantically incongruent ASL sentences, relative to congruent sentences. Like the effects in the present study, this response was broadly distributed and had an onset and peak that the researchers noted was somewhat later M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24 than would be expected for written language, but consistent with earlier studies on auditory language (Holcomb &amp; Neville, 1990, 1991). Neville et al. suggested that this delay might be due to the fact that the recognition point of different signs will tend to var</context>
</contexts>
<marker>Neville, Coffey, Lawson, Fischer, Emmorey, Bellugi, 1997</marker>
<rawString>Neville, H. J., Coffey, S. A., Lawson, D. S., Fischer, A., Emmorey, K., &amp; Bellugi, U. (1997). Neural systems mediating American Sign Language: Effects of sensory experience and age of acquisition. Brain and Language, 285–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H J Neville</author>
<author>J L Nicol</author>
<author>A Barss</author>
<author>K I Forster</author>
<author>M F Garrett</author>
</authors>
<title>Syntactically based sentence processing classes: Evidence from event-related brain potentials.</title>
<date>1991</date>
<journal>Journal of Cognitive Neuroscience,</journal>
<volume>3</volume>
<pages>151--165</pages>
<contexts>
<context position="8999" citStr="Neville, Nicol, Barss, Forster, &amp; Garrett, 1991" startWordPosition="1327" endWordPosition="1333">ictures (Ganis &amp; Kutas, 2003; Ganis, Kutas, &amp; Sereno, 1996; Nigam, Hoffman, &amp; Simons, 1992; Pratarelli, 1994), faces (Barrett &amp; Rugg, 1989; Bobes, Valdés-Sosa, &amp; Olivares, 1994), environmental noises (Chao, Nielsen-Bohlman, &amp; Knight, 1995; Van Petten &amp; Rheinfelder, 1995), movie clips (Sitnikova, Holcomb, Kiyonaga, &amp; Kuperberg, 2008; Sitnikova, Kuperberg, &amp; Holcomb, 2003) and co-speech gestures (Kelly, Kravitz, &amp; Hopkins, 2004; Wu &amp; Coulson, 2005). Linguistically anomalous stimuli are not always associated with an N400 response. For example, the left anterior negativity (LAN; Friederici, 2002; Neville, Nicol, Barss, Forster, &amp; Garrett, 1991) and P600 (Osterhout &amp; Holcomb, 1992) are well-known ERP components that have been found in syntactic violation contexts in 1 This possibility is bolstered by recent work of Albert Kim and colleagues, who have found that relative to real word controls, N400 amplitude decreases and P600 amplitude increases, parametrically, as orthographic irregularity increases (Kim &amp; Pitkänen, submitted for publication). 13 spoken and written language, and more recent work has shown that these components can be elicited in the visual–manual modality as well. For example, in a recent study Capek et al. (2009) </context>
</contexts>
<marker>Neville, Nicol, Barss, Forster, Garrett, 1991</marker>
<rawString>Neville, H. J., Nicol, J. L., Barss, A., Forster, K. I., &amp; Garrett, M. F. (1991). Syntactically based sentence processing classes: Evidence from event-related brain potentials. Journal of Cognitive Neuroscience, 3, 151–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nigam</author>
<author>J E Hoffman</author>
<author>R F Simons</author>
</authors>
<title>N400 and semantic anomaly with pictures and words.</title>
<date>1992</date>
<journal>Journal of Cognitive Neuroscience,</journal>
<volume>4</volume>
<pages>15--22</pages>
<contexts>
<context position="8442" citStr="Nigam, Hoffman, &amp; Simons, 1992" startWordPosition="1253" endWordPosition="1257">and a positive component is sometimes seen instead (Hagoort &amp; Kutas, 1995; Ziegler, Besson, Jacobs, Nazir, &amp; Carr, 1997). This may reﬂect the operation of some kind of ﬁltering mechanism during online processing, through which language users are able to quickly reject forms that lie beyond a certain point of acceptability, or plausibility, during the ongoing processing of the incoming language stream.1 The N400 (or N400-like responses) can also be observed in numerous contexts involving non-linguistic but meaningful stimuli, such as pictures (Ganis &amp; Kutas, 2003; Ganis, Kutas, &amp; Sereno, 1996; Nigam, Hoffman, &amp; Simons, 1992; Pratarelli, 1994), faces (Barrett &amp; Rugg, 1989; Bobes, Valdés-Sosa, &amp; Olivares, 1994), environmental noises (Chao, Nielsen-Bohlman, &amp; Knight, 1995; Van Petten &amp; Rheinfelder, 1995), movie clips (Sitnikova, Holcomb, Kiyonaga, &amp; Kuperberg, 2008; Sitnikova, Kuperberg, &amp; Holcomb, 2003) and co-speech gestures (Kelly, Kravitz, &amp; Hopkins, 2004; Wu &amp; Coulson, 2005). Linguistically anomalous stimuli are not always associated with an N400 response. For example, the left anterior negativity (LAN; Friederici, 2002; Neville, Nicol, Barss, Forster, &amp; Garrett, 1991) and P600 (Osterhout &amp; Holcomb, 1992) are </context>
</contexts>
<marker>Nigam, Hoffman, Simons, 1992</marker>
<rawString>Nigam, A., Hoffman, J. E., &amp; Simons, R. F. (1992). N400 and semantic anomaly with pictures and words. Journal of Cognitive Neuroscience, 4, 15–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Osterhout</author>
<author>P Holcomb</author>
</authors>
<title>Event-related brain potentials elicited by syntactic anomaly.</title>
<date>1992</date>
<journal>Journal of Memory and Language,</journal>
<volume>31</volume>
<pages>785--806</pages>
<contexts>
<context position="9037" citStr="Osterhout &amp; Holcomb, 1992" startWordPosition="1336" endWordPosition="1339"> Nigam, Hoffman, &amp; Simons, 1992; Pratarelli, 1994), faces (Barrett &amp; Rugg, 1989; Bobes, Valdés-Sosa, &amp; Olivares, 1994), environmental noises (Chao, Nielsen-Bohlman, &amp; Knight, 1995; Van Petten &amp; Rheinfelder, 1995), movie clips (Sitnikova, Holcomb, Kiyonaga, &amp; Kuperberg, 2008; Sitnikova, Kuperberg, &amp; Holcomb, 2003) and co-speech gestures (Kelly, Kravitz, &amp; Hopkins, 2004; Wu &amp; Coulson, 2005). Linguistically anomalous stimuli are not always associated with an N400 response. For example, the left anterior negativity (LAN; Friederici, 2002; Neville, Nicol, Barss, Forster, &amp; Garrett, 1991) and P600 (Osterhout &amp; Holcomb, 1992) are well-known ERP components that have been found in syntactic violation contexts in 1 This possibility is bolstered by recent work of Albert Kim and colleagues, who have found that relative to real word controls, N400 amplitude decreases and P600 amplitude increases, parametrically, as orthographic irregularity increases (Kim &amp; Pitkänen, submitted for publication). 13 spoken and written language, and more recent work has shown that these components can be elicited in the visual–manual modality as well. For example, in a recent study Capek et al. (2009) compared ERP responses to semantically</context>
</contexts>
<marker>Osterhout, Holcomb, 1992</marker>
<rawString>Osterhout, L., &amp; Holcomb, P. (1992). Event-related brain potentials elicited by syntactic anomaly. Journal of Memory and Language, 31, 785–806.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ozyürek</author>
<author>R M Willems</author>
<author>S Kita</author>
<author>P Hagoort</author>
</authors>
<title>On-line integration of semantic information from speech and gesture: Insights from event-related brain potentials.</title>
<date>2007</date>
<journal>Journal of Cognitive Neuroscience,</journal>
<volume>19</volume>
<pages>605--616</pages>
<contexts>
<context position="49963" citStr="Ozyürek, Willems, Kita, &amp; Hagoort, 2007" startWordPosition="7766" endWordPosition="7771"> language; as well as the positive waveforms seen in response to the non-linguistic gesture stimuli in the present study; all support the prediction that phonologically illegal non-signs would elicit a positive-going waveform. However, conﬁrmation of this must await future research. 20 M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24 We have already alluded to the growing number of studies which have used ERP methodology to examine the contributions of co-speech manual gestures to the interpretation of both linguistic and non-linguistic stimuli (Holle &amp; Gunter, 2007; Kelly et al., 2004; Ozyürek, Willems, Kita, &amp; Hagoort, 2007; Wu &amp; Coulson, 2005, 2007a, 2007b). Many of these studies have used iconic manual gestures that depict a salient visual–spatial property of concrete objects, such as their size and shape or an associated manner of movement (but see also Cornejo et al., 2009). Collectively these studies suggest that co-speech manual gestures inﬂuence semantic representations, and that discrepancies between gestural forms and the semantic contexts in which they occur lead to greater processing costs on the part of language perceivers. This in turn results in increased negativities in the time window often assoc</context>
<context position="51300" citStr="Ozyürek et al. (2007)" startWordPosition="7968" endWordPosition="7971">utas &amp; Hillyard, 1980). For example, Kelly et al. (2004) observed modulation of ERP responses for speech tokens that were either accompanied by matching, complementary or mismatched hand gestures. An N400-like component was observed for mismatched gesture-speech tokens relative to the other conditions. Wu and Coulson (2005) examined ERPs for subjects who watched cartoons followed by a gestural depiction that either matched or mismatched the events shown in the cartoons. Gestures elicited an N400-like component (a socalled ‘‘gesture N450’’) that was larger for incongruent than congruent items. Ozyürek et al. (2007) recorded EEG while subjects listened to sentences with a critical verb (e.g. ‘‘knock’’) accompanied by a related co-speech gesture (e.g. KNOCK). Verbal/gestural semantic content either matched or mismatched the earlier part of the sentence. The researchers noted that following the N1–P2 complex, the ERP response to mismatch conditions started to deviate from the response to the correct condition in the latency window of the P2 component, around 225–275 ms post stimulus onset, while at around 350 ms, the mismatch conditions deviated from the congruent condition. This was followed by a similar </context>
</contexts>
<marker>Ozyürek, Willems, Kita, Hagoort, 2007</marker>
<rawString>Ozyürek, A., Willems, R. M., Kita, S., &amp; Hagoort, P. (2007). On-line integration of semantic information from speech and gesture: Insights from event-related brain potentials. Journal of Cognitive Neuroscience, 19, 605–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Pratarelli</author>
</authors>
<title>Semantic processing of pictures and spoken words: Evidence from event-related brain potentials.</title>
<date>1994</date>
<journal>Brain and Cognition,</journal>
<volume>24</volume>
<pages>137--157</pages>
<contexts>
<context position="8461" citStr="Pratarelli, 1994" startWordPosition="1258" endWordPosition="1259">times seen instead (Hagoort &amp; Kutas, 1995; Ziegler, Besson, Jacobs, Nazir, &amp; Carr, 1997). This may reﬂect the operation of some kind of ﬁltering mechanism during online processing, through which language users are able to quickly reject forms that lie beyond a certain point of acceptability, or plausibility, during the ongoing processing of the incoming language stream.1 The N400 (or N400-like responses) can also be observed in numerous contexts involving non-linguistic but meaningful stimuli, such as pictures (Ganis &amp; Kutas, 2003; Ganis, Kutas, &amp; Sereno, 1996; Nigam, Hoffman, &amp; Simons, 1992; Pratarelli, 1994), faces (Barrett &amp; Rugg, 1989; Bobes, Valdés-Sosa, &amp; Olivares, 1994), environmental noises (Chao, Nielsen-Bohlman, &amp; Knight, 1995; Van Petten &amp; Rheinfelder, 1995), movie clips (Sitnikova, Holcomb, Kiyonaga, &amp; Kuperberg, 2008; Sitnikova, Kuperberg, &amp; Holcomb, 2003) and co-speech gestures (Kelly, Kravitz, &amp; Hopkins, 2004; Wu &amp; Coulson, 2005). Linguistically anomalous stimuli are not always associated with an N400 response. For example, the left anterior negativity (LAN; Friederici, 2002; Neville, Nicol, Barss, Forster, &amp; Garrett, 1991) and P600 (Osterhout &amp; Holcomb, 1992) are well-known ERP comp</context>
</contexts>
<marker>Pratarelli, 1994</marker>
<rawString>Pratarelli, M. E. (1994). Semantic processing of pictures and spoken words: Evidence from event-related brain potentials. Brain and Cognition, 24, 137–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Rizzolatti</author>
<author>M A Arbib</author>
</authors>
<title>Language within our grasp.</title>
<date>1998</date>
<journal>Trends in Neurosciences,</journal>
<volume>21</volume>
<pages>188--194</pages>
<contexts>
<context position="3091" citStr="Rizzolatti &amp; Arbib, 1998" startWordPosition="456" endWordPosition="459">of other common everyday behaviors that include non-linguistic actions such a reaching and grasping, waving, and scratching oneself, as well gesticulations that accompany speech (i.e. co-speech gestures) or serve non-sign language deictic functions, such as pointing. The formal relationship between signed languages and human gestural actions is of considerable interest to a range of disciplines. Linguists, psychologists and cognitive scientists have proposed a critical role for manual gesture in the development and evolution of human languages (Arbib, 2005, 2008; Gentilucci &amp; Corballis, 2006; Rizzolatti &amp; Arbib, 1998; Tomasello, 2005; Wilcox, 2004). Re⇑ Corresponding author. Address: Department of Neurology, University of California at Irvine, 101 The City Drive South, Bldg. 53, Room 204, Orange, CA 928684280, United States. Fax: +1 714 456 1697. E-mail address: m.grosvald@uci.edu (M. Grosvald). 0093-934X/$ - see front matter Ó 2012 Elsevier Inc. All rights reserved. doi:10.1016/j.bandl.2012.01.005 cently, linguists have documented compelling evidence that the development of nascent sign languages derives from idiosyncratic gestural and pantomimic systems used by isolated communities, which in some cases </context>
</contexts>
<marker>Rizzolatti, Arbib, 1998</marker>
<rawString>Rizzolatti, G., &amp; Arbib, M. A. (1998). Language within our grasp. Trends in Neurosciences, 21, 188–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Rugg</author>
<author>M E Nagy</author>
</authors>
<title>Lexical contribution to non-word-repetition effects: Evidence from event-related potentials.</title>
<date>1987</date>
<journal>Memory and Cognition,</journal>
<volume>15</volume>
<pages>473--481</pages>
<contexts>
<context position="47850" citStr="Rugg &amp; Nagy, 1987" startWordPosition="7454" endWordPosition="7457">t may be relevant here. First, a P300 response is well-attested in studies making use of stimuli perceived by subjects to be in a low-probability category (e.g. Johnson &amp; Donchin, 1980). In our experiment, grooming gestures occurred 1/4 of the time, rendering these non-linguistic events low-probability with respect to the other three (linguistic) sentence ending conditions. Second, ERP differences between non-words and words have been attributed to the fact that these non-linguistic items have little or nothing in common with lexical entries and therefore do not generate lexical activity (cf. Rugg &amp; Nagy, 1987). Third, the ERP differences observed between pseudo-words and non-words have also been suggested to reﬂect a pre-lexical ﬁltering process that quickly rejects non-linguistic items based upon aberrant physical characteristics (Holcomb &amp; Neville, 1990). For example, Ziegler et al. (1997) suggest such a categorization may be based on a spelling check in the case of non-word consonant-string stimuli. This last interpretation accords well with a possibility we noted in the Introduction, that such an ERP response may be due to the operation of a ﬁltering/rejection mechanism, allowing language users</context>
</contexts>
<marker>Rugg, Nagy, 1987</marker>
<rawString>Rugg, M. D., &amp; Nagy, M. E. (1987). Lexical contribution to non-word-repetition effects: Evidence from event-related potentials. Memory and Cognition, 15, 473–481.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Senghas</author>
</authors>
<title>Language emergence. Clues from a new Bedouin sign language.</title>
<date>2005</date>
<journal>Current Biology,</journal>
<volume>15</volume>
<pages>463--465</pages>
<contexts>
<context position="3924" citStr="Senghas, 2005" startWordPosition="577" endWordPosition="578">1 714 456 1697. E-mail address: m.grosvald@uci.edu (M. Grosvald). 0093-934X/$ - see front matter Ó 2012 Elsevier Inc. All rights reserved. doi:10.1016/j.bandl.2012.01.005 cently, linguists have documented compelling evidence that the development of nascent sign languages derives from idiosyncratic gestural and pantomimic systems used by isolated communities, which in some cases may be limited to individual families who have a need to communicate with a deaf child (Frishberg, 1987; GoldinMeadow, 2003; Kegl, Senghas, &amp; Coppola, 1999; Meir, Sandler, Padden, &amp; Aronoff, 2010; Morford &amp; Kegl, 2000; Senghas, 2005). Even within mature sign languages of Deaf communities, linguistic accounts of sign language structure have also argued that lexical and discourse components of American Sign Language (ASL) and other signed languages may be best understood as being gesturally based (Liddell, 2003). Thus diachronic and synchronic evidence from language research support the contention that signed languages might make use of perceptual systems similar to those through which humans understand or parse human actions and gestures more generally (Corballis, 2009). In contrast, given its linguistic status, sign langu</context>
</contexts>
<marker>Senghas, 2005</marker>
<rawString>Senghas, A. (2005). Language emergence. Clues from a new Bedouin sign language. Current Biology, 15, 463–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Sitnikova</author>
<author>P J Holcomb</author>
<author>K A Kiyonaga</author>
<author>G R Kuperberg</author>
</authors>
<title>Two neurocognitive mechanisms of semantic integration during the comprehension of real-world events.</title>
<date>2008</date>
<journal>Journal of Cognitive Neuroscience,</journal>
<volume>20</volume>
<pages>2037--2057</pages>
<contexts>
<context position="8685" citStr="Sitnikova, Holcomb, Kiyonaga, &amp; Kuperberg, 2008" startWordPosition="1284" endWordPosition="1289"> language users are able to quickly reject forms that lie beyond a certain point of acceptability, or plausibility, during the ongoing processing of the incoming language stream.1 The N400 (or N400-like responses) can also be observed in numerous contexts involving non-linguistic but meaningful stimuli, such as pictures (Ganis &amp; Kutas, 2003; Ganis, Kutas, &amp; Sereno, 1996; Nigam, Hoffman, &amp; Simons, 1992; Pratarelli, 1994), faces (Barrett &amp; Rugg, 1989; Bobes, Valdés-Sosa, &amp; Olivares, 1994), environmental noises (Chao, Nielsen-Bohlman, &amp; Knight, 1995; Van Petten &amp; Rheinfelder, 1995), movie clips (Sitnikova, Holcomb, Kiyonaga, &amp; Kuperberg, 2008; Sitnikova, Kuperberg, &amp; Holcomb, 2003) and co-speech gestures (Kelly, Kravitz, &amp; Hopkins, 2004; Wu &amp; Coulson, 2005). Linguistically anomalous stimuli are not always associated with an N400 response. For example, the left anterior negativity (LAN; Friederici, 2002; Neville, Nicol, Barss, Forster, &amp; Garrett, 1991) and P600 (Osterhout &amp; Holcomb, 1992) are well-known ERP components that have been found in syntactic violation contexts in 1 This possibility is bolstered by recent work of Albert Kim and colleagues, who have found that relative to real word controls, N400 amplitude decreases and P60</context>
</contexts>
<marker>Sitnikova, Holcomb, Kiyonaga, Kuperberg, 2008</marker>
<rawString>Sitnikova, T., Holcomb, P. J., Kiyonaga, K. A., &amp; Kuperberg, G. R. (2008). Two neurocognitive mechanisms of semantic integration during the comprehension of real-world events. Journal of Cognitive Neuroscience, 20, 2037–2057.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Sitnikova</author>
<author>G Kuperberg</author>
<author>P J Holcomb</author>
</authors>
<title>Semantic integration in videos of real-world events: An electrophysiological investigation.</title>
<date>2003</date>
<journal>Psychophysiology,</journal>
<volume>40</volume>
<pages>160--164</pages>
<contexts>
<context position="8724" citStr="Sitnikova, Kuperberg, &amp; Holcomb, 2003" startWordPosition="1290" endWordPosition="1294">that lie beyond a certain point of acceptability, or plausibility, during the ongoing processing of the incoming language stream.1 The N400 (or N400-like responses) can also be observed in numerous contexts involving non-linguistic but meaningful stimuli, such as pictures (Ganis &amp; Kutas, 2003; Ganis, Kutas, &amp; Sereno, 1996; Nigam, Hoffman, &amp; Simons, 1992; Pratarelli, 1994), faces (Barrett &amp; Rugg, 1989; Bobes, Valdés-Sosa, &amp; Olivares, 1994), environmental noises (Chao, Nielsen-Bohlman, &amp; Knight, 1995; Van Petten &amp; Rheinfelder, 1995), movie clips (Sitnikova, Holcomb, Kiyonaga, &amp; Kuperberg, 2008; Sitnikova, Kuperberg, &amp; Holcomb, 2003) and co-speech gestures (Kelly, Kravitz, &amp; Hopkins, 2004; Wu &amp; Coulson, 2005). Linguistically anomalous stimuli are not always associated with an N400 response. For example, the left anterior negativity (LAN; Friederici, 2002; Neville, Nicol, Barss, Forster, &amp; Garrett, 1991) and P600 (Osterhout &amp; Holcomb, 1992) are well-known ERP components that have been found in syntactic violation contexts in 1 This possibility is bolstered by recent work of Albert Kim and colleagues, who have found that relative to real word controls, N400 amplitude decreases and P600 amplitude increases, parametrically, </context>
<context position="42668" citStr="Sitnikova et al., 2003" startWordPosition="6673" endWordPosition="6676">ntextually incongruent gestures, a component described by the researchers as a ‘‘gesture N450’’ was observed. Wu and Coulson noted the similarity of this effect to the N450 reported by Barrett and Rugg (1990) for second items in unrelated picture pairs relative to related picture pairs (e.g. wrench/fork vs. knife/fork), stating (p. 659) that consistent with their own ﬁndings, ‘‘most such ‘picture’ ERP studies report a broadly distributed negativity largest at frontal electrode sites and not evident at occipital sites (Barrett &amp; Rugg, 1990; Holcomb &amp; McPherson, 1994; McPherson &amp; Holcomb, 1999; Sitnikova et al., 2003; West &amp; Holcomb, 2002).’’ In contrast, the negativity reported in the present study was quite evident at occipital sites, as can be seen clearly in Figs. 3 and 4. A second notable ﬁnding in our study concerns deaf subjects’ ERP response in the phonologically legal pseudo-sign condition, which was also consistent with an N400 response but was generally larger (more negative) than the negativity seen for semantically incongruent but fully lexical signs. This provides further evidence for broad processing similarities for different linguistic modalities, in the light of similar ﬁndings for pseud</context>
</contexts>
<marker>Sitnikova, Kuperberg, Holcomb, 2003</marker>
<rawString>Sitnikova, T., Kuperberg, G., &amp; Holcomb, P. J. (2003). Semantic integration in videos of real-world events: An electrophysiological investigation. Psychophysiology, 40, 160–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomasello</author>
</authors>
<title>Constructing a language: A usage-based theory of language acquisition.</title>
<date>2005</date>
<publisher>Harvard University Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="3108" citStr="Tomasello, 2005" startWordPosition="460" endWordPosition="461">ehaviors that include non-linguistic actions such a reaching and grasping, waving, and scratching oneself, as well gesticulations that accompany speech (i.e. co-speech gestures) or serve non-sign language deictic functions, such as pointing. The formal relationship between signed languages and human gestural actions is of considerable interest to a range of disciplines. Linguists, psychologists and cognitive scientists have proposed a critical role for manual gesture in the development and evolution of human languages (Arbib, 2005, 2008; Gentilucci &amp; Corballis, 2006; Rizzolatti &amp; Arbib, 1998; Tomasello, 2005; Wilcox, 2004). Re⇑ Corresponding author. Address: Department of Neurology, University of California at Irvine, 101 The City Drive South, Bldg. 53, Room 204, Orange, CA 928684280, United States. Fax: +1 714 456 1697. E-mail address: m.grosvald@uci.edu (M. Grosvald). 0093-934X/$ - see front matter Ó 2012 Elsevier Inc. All rights reserved. doi:10.1016/j.bandl.2012.01.005 cently, linguists have documented compelling evidence that the development of nascent sign languages derives from idiosyncratic gestural and pantomimic systems used by isolated communities, which in some cases may be limited to</context>
</contexts>
<marker>Tomasello, 2005</marker>
<rawString>Tomasello, M. (2005). Constructing a language: A usage-based theory of language acquisition. Cambridge, MA: Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Van Petten</author>
<author>H Rheinfelder</author>
</authors>
<title>Conceptual relationships between spoken words and environmental sounds: Event-related brain potential measures.</title>
<date>1995</date>
<journal>Neuropsychologia,</journal>
<volume>33</volume>
<pages>485--508</pages>
<marker>Van Petten, Rheinfelder, 1995</marker>
<rawString>Van Petten, C., &amp; Rheinfelder, H. (1995). Conceptual relationships between spoken words and environmental sounds: Event-related brain potential measures. Neuropsychologia, 33, 485–508.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C West</author>
<author>P J Holcomb</author>
</authors>
<title>Event-related potentials during discourse-level semantic integration of complex pictures.</title>
<date>2002</date>
<journal>Cognitive Brain Research,</journal>
<volume>13</volume>
<pages>363--375</pages>
<contexts>
<context position="42691" citStr="West &amp; Holcomb, 2002" startWordPosition="6677" endWordPosition="6680">estures, a component described by the researchers as a ‘‘gesture N450’’ was observed. Wu and Coulson noted the similarity of this effect to the N450 reported by Barrett and Rugg (1990) for second items in unrelated picture pairs relative to related picture pairs (e.g. wrench/fork vs. knife/fork), stating (p. 659) that consistent with their own ﬁndings, ‘‘most such ‘picture’ ERP studies report a broadly distributed negativity largest at frontal electrode sites and not evident at occipital sites (Barrett &amp; Rugg, 1990; Holcomb &amp; McPherson, 1994; McPherson &amp; Holcomb, 1999; Sitnikova et al., 2003; West &amp; Holcomb, 2002).’’ In contrast, the negativity reported in the present study was quite evident at occipital sites, as can be seen clearly in Figs. 3 and 4. A second notable ﬁnding in our study concerns deaf subjects’ ERP response in the phonologically legal pseudo-sign condition, which was also consistent with an N400 response but was generally larger (more negative) than the negativity seen for semantically incongruent but fully lexical signs. This provides further evidence for broad processing similarities for different linguistic modalities, in the light of similar ﬁndings for pseudo-words in earlier stud</context>
</contexts>
<marker>West, Holcomb, 2002</marker>
<rawString>West, W. C., &amp; Holcomb, P. J. (2002). Event-related potentials during discourse-level semantic integration of complex pictures. Cognitive Brain Research, 13, 363–375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Grosvald</author>
</authors>
<title>Gesture and language: Cross-linguistic and historical data from signed languages.</title>
<date>2012</date>
<journal>Brain &amp; Language</journal>
<volume>121</volume>
<pages>12--24</pages>
<marker>Grosvald, 2012</marker>
<rawString>M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24 Wilcox, S. (2004). Gesture and language: Cross-linguistic and historical data from signed languages. Gesture, 4, 43–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y C Wu</author>
<author>S Coulson</author>
</authors>
<title>Meaningful gestures: Electrophysiological indices of iconic gesture comprehension.</title>
<date>2005</date>
<journal>Psychophysiology,</journal>
<volume>42</volume>
<pages>654--667</pages>
<contexts>
<context position="51004" citStr="Wu and Coulson (2005)" startWordPosition="7924" endWordPosition="7927">ntic contexts in which they occur lead to greater processing costs on the part of language perceivers. This in turn results in increased negativities in the time window often associated with the classic N400 effect, observed in response to word meanings that violate the wider semantic context (Kutas &amp; Hillyard, 1980). For example, Kelly et al. (2004) observed modulation of ERP responses for speech tokens that were either accompanied by matching, complementary or mismatched hand gestures. An N400-like component was observed for mismatched gesture-speech tokens relative to the other conditions. Wu and Coulson (2005) examined ERPs for subjects who watched cartoons followed by a gestural depiction that either matched or mismatched the events shown in the cartoons. Gestures elicited an N400-like component (a socalled ‘‘gesture N450’’) that was larger for incongruent than congruent items. Ozyürek et al. (2007) recorded EEG while subjects listened to sentences with a critical verb (e.g. ‘‘knock’’) accompanied by a related co-speech gesture (e.g. KNOCK). Verbal/gestural semantic content either matched or mismatched the earlier part of the sentence. The researchers noted that following the N1–P2 complex, the ER</context>
<context position="8802" citStr="Wu &amp; Coulson, 2005" startWordPosition="1303" endWordPosition="1306"> of the incoming language stream.1 The N400 (or N400-like responses) can also be observed in numerous contexts involving non-linguistic but meaningful stimuli, such as pictures (Ganis &amp; Kutas, 2003; Ganis, Kutas, &amp; Sereno, 1996; Nigam, Hoffman, &amp; Simons, 1992; Pratarelli, 1994), faces (Barrett &amp; Rugg, 1989; Bobes, Valdés-Sosa, &amp; Olivares, 1994), environmental noises (Chao, Nielsen-Bohlman, &amp; Knight, 1995; Van Petten &amp; Rheinfelder, 1995), movie clips (Sitnikova, Holcomb, Kiyonaga, &amp; Kuperberg, 2008; Sitnikova, Kuperberg, &amp; Holcomb, 2003) and co-speech gestures (Kelly, Kravitz, &amp; Hopkins, 2004; Wu &amp; Coulson, 2005). Linguistically anomalous stimuli are not always associated with an N400 response. For example, the left anterior negativity (LAN; Friederici, 2002; Neville, Nicol, Barss, Forster, &amp; Garrett, 1991) and P600 (Osterhout &amp; Holcomb, 1992) are well-known ERP components that have been found in syntactic violation contexts in 1 This possibility is bolstered by recent work of Albert Kim and colleagues, who have found that relative to real word controls, N400 amplitude decreases and P600 amplitude increases, parametrically, as orthographic irregularity increases (Kim &amp; Pitkänen, submitted for publicat</context>
<context position="11200" citStr="Wu &amp; Coulson, 2005" startWordPosition="1644" endWordPosition="1647"> pseudo-signs. The ERP response for the non-linguistic gesture condition is a priori more difﬁcult to predict. Previous neuro-imaging studies of deaf signers have reported differences in patterns of activation associated with the perception of signs compared to non-linguistic gestures (Corina et al., 2007; Emmorey et al., 2010; MacSweeney et al., 2004), but the methodologies used in those studies lacked the temporal resolution to determine at what stage of processing these differences may occur. While N400-like responses have been elicited to co-speech gestural mismatches (Kelly et al., 2004; Wu &amp; Coulson, 2005), in our study, gestures occur in place of semantically appropriate sentence-ending items, rather than as a possible accompaniment. It should also be borne in mind that the relationship of signs and grooming gestures is probably not quite akin to that between standard lexical items in spoken language and the orthographically/phonotactically illegal pseudo-words used in earlier ERP studies. Unlike grooming gestures, which are part of everyday life, illegal non-words like ‘‘dkfpst’’ are probably alien to most people’s routine experience. A better spoken-language analogue of our grooming action c</context>
<context position="49983" citStr="Wu &amp; Coulson, 2005" startWordPosition="7772" endWordPosition="7775">rms seen in response to the non-linguistic gesture stimuli in the present study; all support the prediction that phonologically illegal non-signs would elicit a positive-going waveform. However, conﬁrmation of this must await future research. 20 M. Grosvald et al. / Brain &amp; Language 121 (2012) 12–24 We have already alluded to the growing number of studies which have used ERP methodology to examine the contributions of co-speech manual gestures to the interpretation of both linguistic and non-linguistic stimuli (Holle &amp; Gunter, 2007; Kelly et al., 2004; Ozyürek, Willems, Kita, &amp; Hagoort, 2007; Wu &amp; Coulson, 2005, 2007a, 2007b). Many of these studies have used iconic manual gestures that depict a salient visual–spatial property of concrete objects, such as their size and shape or an associated manner of movement (but see also Cornejo et al., 2009). Collectively these studies suggest that co-speech manual gestures inﬂuence semantic representations, and that discrepancies between gestural forms and the semantic contexts in which they occur lead to greater processing costs on the part of language perceivers. This in turn results in increased negativities in the time window often associated with the class</context>
</contexts>
<marker>Wu, Coulson, 2005</marker>
<rawString>Wu, Y. C., &amp; Coulson, S. (2005). Meaningful gestures: Electrophysiological indices of iconic gesture comprehension. Psychophysiology, 42, 654–667.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y C Wu</author>
<author>S Coulson</author>
</authors>
<title>How iconic gestures enhance communication: An ERP study.</title>
<date>2007</date>
<journal>Brain and Language,</journal>
<volume>101</volume>
<pages>234--245</pages>
<marker>Wu, Coulson, 2007</marker>
<rawString>Wu, Y. C., &amp; Coulson, S. (2007a). How iconic gestures enhance communication: An ERP study. Brain and Language, 101, 234–245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y C Wu</author>
<author>S Coulson</author>
</authors>
<title>Iconic gestures prime related concepts: An ERP study.</title>
<date>2007</date>
<journal>Psychonomic Bulletin &amp; Review,</journal>
<volume>14</volume>
<pages>57--63</pages>
<marker>Wu, Coulson, 2007</marker>
<rawString>Wu, Y. C., &amp; Coulson, S. (2007b). Iconic gestures prime related concepts: An ERP study. Psychonomic Bulletin &amp; Review, 14, 57–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Ziegler</author>
<author>M Besson</author>
<author>A M Jacobs</author>
<author>T A Nazir</author>
<author>T H Carr</author>
</authors>
<title>Word, pseudoword, and nonword processing: A multitask comparison using eventrelated brain potentials.</title>
<date>1997</date>
<journal>Journal of Cognitive Neuroscience,</journal>
<volume>9</volume>
<pages>758--775</pages>
<contexts>
<context position="7931" citStr="Ziegler, Besson, Jacobs, Nazir, &amp; Carr, 1997" startWordPosition="1174" endWordPosition="1180">hically/phonologically legal but non-occurring ‘‘pseudo-words’’ (e.g. ‘‘blork’’), and it has sometimes been reported that pseudo-words elicit a stronger N400 response than semantically incongruent real words (Bentin, 1987; Bentin, McCarthy, &amp; Wood, 1985; Hagoort &amp; Kutas, 1995), consistent with the idea that the magnitude of N400 response is related to the difﬁculty of the ongoing process of semantic-contextual integration. However, orthographically illegal ‘‘non-words’’ (e.g. ‘‘rbsnk’’) do not generally elicit an N400, and a positive component is sometimes seen instead (Hagoort &amp; Kutas, 1995; Ziegler, Besson, Jacobs, Nazir, &amp; Carr, 1997). This may reﬂect the operation of some kind of ﬁltering mechanism during online processing, through which language users are able to quickly reject forms that lie beyond a certain point of acceptability, or plausibility, during the ongoing processing of the incoming language stream.1 The N400 (or N400-like responses) can also be observed in numerous contexts involving non-linguistic but meaningful stimuli, such as pictures (Ganis &amp; Kutas, 2003; Ganis, Kutas, &amp; Sereno, 1996; Nigam, Hoffman, &amp; Simons, 1992; Pratarelli, 1994), faces (Barrett &amp; Rugg, 1989; Bobes, Valdés-Sosa, &amp; Olivares, 1994), </context>
<context position="44896" citStr="Ziegler et al., 1997" startWordPosition="7007" endWordPosition="7010">out useful directions such research might take. A third set of ﬁndings, concerning the outcome related to our non-linguistic grooming actions, is especially provocative. In contrast to the three other kinds of sentence-ﬁnal items, all of which could be considered linguistic (i.e. as actual lexical items in two cases, and phonologically legal lexical gaps in the third), the non-linguistic grooming actions elicited a large positivity. As noted earlier, phonologically illegal words in ERP studies have in some cases elicited a positive-going component rather than an N400 (Holcomb &amp; Neville, 1990; Ziegler et al., 1997). Holcomb and Neville (1990) examined differences between pseudo-words and non-words in the visual and auditory modalities in the context of a lexical decision experiment. Pseudo-words accorded with phonotactic constraints of English; visually presented non-words were composed of consonant strings and auditory non-words were words played backwards. The researchers reported that within an early time window (150– 300 ms), auditory non-words (but not visual non-words) elicited a 19 more negative response than pseudo-words, but only at anterior and right hemisphere sites. In a later time window (3</context>
<context position="48137" citStr="Ziegler et al. (1997)" startWordPosition="7493" endWordPosition="7496">nts low-probability with respect to the other three (linguistic) sentence ending conditions. Second, ERP differences between non-words and words have been attributed to the fact that these non-linguistic items have little or nothing in common with lexical entries and therefore do not generate lexical activity (cf. Rugg &amp; Nagy, 1987). Third, the ERP differences observed between pseudo-words and non-words have also been suggested to reﬂect a pre-lexical ﬁltering process that quickly rejects non-linguistic items based upon aberrant physical characteristics (Holcomb &amp; Neville, 1990). For example, Ziegler et al. (1997) suggest such a categorization may be based on a spelling check in the case of non-word consonant-string stimuli. This last interpretation accords well with a possibility we noted in the Introduction, that such an ERP response may be due to the operation of a ﬁltering/rejection mechanism, allowing language users to efﬁciently reject items in the incoming linguistic signal that do not fall within some limits of linguistic acceptability. The gesture stimuli in the present study, in lacking the semantic appropriateness of semantically congruent signs, the lexicality of incongruent signs, and even</context>
</contexts>
<marker>Ziegler, Besson, Jacobs, Nazir, Carr, 1997</marker>
<rawString>Ziegler, J. C., Besson, M., Jacobs, A. M., Nazir, T. A., &amp; Carr, T. H. (1997). Word, pseudoword, and nonword processing: A multitask comparison using eventrelated brain potentials. Journal of Cognitive Neuroscience, 9, 758–775.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>