<?xml version='1.0' encoding='UTF-8'?><paper>
<algorithm name="Grobid Header Extraction" version="0.1"><title>Interaction between auditory and motor systems in speech perception</title><authors><author><name>Zhe-Meng Wu</name><affiliation>PKU-IDG/McGovern Institute for Brain Research, Peking University, Department of Psychology, Speech and Hearing Research Center, Key Laboratory of Machine Perception (Ministry of Education)</affiliation></author><author><name>Ming-Li Chen</name><affiliation>PKU-IDG/McGovern Institute for Brain Research, Peking University, Department of Psychology, Speech and Hearing Research Center, Key Laboratory of Machine Perception (Ministry of Education)</affiliation></author><author><name>Xi-Hong Wu</name><affiliation>PKU-IDG/McGovern Institute for Brain Research, Peking University, Department of Psychology, Speech and Hearing Research Center, Key Laboratory of Machine Perception (Ministry of Education)</affiliation></author><author><name>Liang Li</name><affiliation>PKU-IDG/McGovern Institute for Brain Research, Peking University, Department of Psychology, Speech and Hearing Research Center, Key Laboratory of Machine Perception (Ministry of Education)</affiliation></author></authors><keywords /></algorithm><TEI>
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interaction between auditory and motor systems in speech perception</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
				<date type="published" when="2014-06-01">June 1, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Zhe-Meng</forename>
								<surname>Wu</surname>
							</persName>
							<affiliation>
								<orgName type="department">Department of Psychology, Speech and Hearing Research Center</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName key="instit1" type="institution">PKU-IDG/McGovern Institute for Brain Research</orgName>
								<orgName key="instit2" type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Ming-Li</forename>
								<surname>Chen</surname>
							</persName>
							<affiliation>
								<orgName type="department">Department of Psychology, Speech and Hearing Research Center</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName key="instit1" type="institution">PKU-IDG/McGovern Institute for Brain Research</orgName>
								<orgName key="instit2" type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Xi-Hong</forename>
								<surname>Wu</surname>
							</persName>
							<affiliation>
								<orgName type="department">Department of Psychology, Speech and Hearing Research Center</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName key="instit1" type="institution">PKU-IDG/McGovern Institute for Brain Research</orgName>
								<orgName key="instit2" type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Liang</forename>
								<surname>Li</surname>
							</persName>
							<affiliation>
								<orgName type="department">Department of Psychology, Speech and Hearing Research Center</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName key="instit1" type="institution">PKU-IDG/McGovern Institute for Brain Research</orgName>
								<orgName key="instit2" type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Interaction between auditory and motor systems in speech perception</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Neurosci Bull</title>
						<imprint>
							<biblScope unit="volume">30</biblScope>
							<biblScope unit="issue">3</biblScope>
							<biblScope from="490" to="496" unit="page" />
							<date type="published" when="2014-06-01">June 1, 2014</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1007/s12264-013-1428-6</idno>
					<note>490</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>auditory-motor interaction</term>
					<term>Motor Theory of speech perception</term>
					<term>motor cortex</term>
					<term>" cocktail-party " problem ·Review·</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Based on the Motor Theory of speech perception, the interaction between the auditory and motor systems plays an essential role in speech perception. Since the Motor Theory was proposed, it has received remarkable attention in the fi eld. However, each of the three hypotheses of the theory still needs further verifi cation. In this review, we focus on how the auditory-motor anatomical and functional associations play a role in speech perception and discuss why previous studies could not reach an agreement and particularly whether the motor system involvement in speech perception is task-load dependent. Finally, we suggest that the function of the auditory-motor link is particularly useful for speech perception under adverse listening conditions and the further revised Motor Theory is a potential solution to the " cocktail-party " problem.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI><algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Schreiner CE</author>
<author>Winer JA</author>
</authors>
<title>Auditory cortex mapmaking: principles, projections, and plasticity. Neuron</title>
<date>2007</date>
<pages>56--356</pages>
<contexts>
<context citStr="[12,13]" endWordPosition="1048" position="7108" startWordPosition="1048">Auditory and Motor Systems To confi rm the involvement of the motor system in speech perception, evidence of both anatomical and functional links between the motor and auditory systems is needed. Indeed, some models emphasize the auditory-motor link in speech perception. For example, the dual-stream processing model suggests that there are two pathways in audition: one is the ventral pathway down to the temporal lobe regulating “what” in acoustic information, and the other is the dorsal pathway from primary sensory areas up to the posterior cortex regulating “how” speech production takes place[12,13]. It is also known that the ventral pathway is involved in analyzing phonetic characters, acoustic features, and speech intelligibility[14,15], and the dorsal pathway is associated with sensorimotor mapping between auditory and motor representations[16,17], speech production[14,18], and silent articulatory organ movement[19]. Although this dual-stream model proposes that each of the pathways plays a specifi c role in speech perception, how the streams interact with each other is still not clear. The other model, the forward-inverse model, proposes that the motor cortical regions predict the co</context>
</contexts>
<marker>[12]</marker>
<rawString>Schreiner CE, Winer JA. Auditory cortex mapmaking: principles, projections, and plasticity. Neuron 2007, 56: 356– 365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Recanzone GH</author>
<author>Sutter ML</author>
</authors>
<title>The biological basis of audition. Annu Rev Psychol</title>
<date>2008</date>
<volume>59</volume>
<pages>119--142</pages>
<contexts>
<context citStr="[12,13]" endWordPosition="1048" position="7108" startWordPosition="1048">Auditory and Motor Systems To confi rm the involvement of the motor system in speech perception, evidence of both anatomical and functional links between the motor and auditory systems is needed. Indeed, some models emphasize the auditory-motor link in speech perception. For example, the dual-stream processing model suggests that there are two pathways in audition: one is the ventral pathway down to the temporal lobe regulating “what” in acoustic information, and the other is the dorsal pathway from primary sensory areas up to the posterior cortex regulating “how” speech production takes place[12,13]. It is also known that the ventral pathway is involved in analyzing phonetic characters, acoustic features, and speech intelligibility[14,15], and the dorsal pathway is associated with sensorimotor mapping between auditory and motor representations[16,17], speech production[14,18], and silent articulatory organ movement[19]. Although this dual-stream model proposes that each of the pathways plays a specifi c role in speech perception, how the streams interact with each other is still not clear. The other model, the forward-inverse model, proposes that the motor cortical regions predict the co</context>
</contexts>
<marker>[13]</marker>
<rawString>Recanzone GH, Sutter ML. The biological basis of audition. Annu Rev Psychol 2008, 59: 119–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Belin</author>
<author>Zatorre RJ</author>
</authors>
<title>Adaptation to speaker’s voice in right anterior temporal lobe. Neuroreport</title>
<date>2003</date>
<volume>14</volume>
<pages>2105--2109</pages>
<contexts>
<context citStr="[14,15]" endWordPosition="1067" position="7250" startWordPosition="1067">nks between the motor and auditory systems is needed. Indeed, some models emphasize the auditory-motor link in speech perception. For example, the dual-stream processing model suggests that there are two pathways in audition: one is the ventral pathway down to the temporal lobe regulating “what” in acoustic information, and the other is the dorsal pathway from primary sensory areas up to the posterior cortex regulating “how” speech production takes place[12,13]. It is also known that the ventral pathway is involved in analyzing phonetic characters, acoustic features, and speech intelligibility[14,15], and the dorsal pathway is associated with sensorimotor mapping between auditory and motor representations[16,17], speech production[14,18], and silent articulatory organ movement[19]. Although this dual-stream model proposes that each of the pathways plays a specifi c role in speech perception, how the streams interact with each other is still not clear. The other model, the forward-inverse model, proposes that the motor cortical regions predict the consequences of motor commands and revise the signals with the changing environment[20,21]. In more detail, before motor commands reach the effe</context>
</contexts>
<marker>[14]</marker>
<rawString>Belin P, Zatorre RJ. Adaptation to speaker’s voice in right anterior temporal lobe. Neuroreport 2003, 14: 2105–2109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott SK</author>
<author>Blank CC</author>
<author>S Rosen</author>
<author>Wise RJ</author>
</authors>
<title>Identification of a pathway for intelligible speech in the left temporal lobe.</title>
<date>2000</date>
<pages>123--2400</pages>
<location>Brain</location>
<contexts>
<context citStr="[14,15]" endWordPosition="1067" position="7250" startWordPosition="1067">nks between the motor and auditory systems is needed. Indeed, some models emphasize the auditory-motor link in speech perception. For example, the dual-stream processing model suggests that there are two pathways in audition: one is the ventral pathway down to the temporal lobe regulating “what” in acoustic information, and the other is the dorsal pathway from primary sensory areas up to the posterior cortex regulating “how” speech production takes place[12,13]. It is also known that the ventral pathway is involved in analyzing phonetic characters, acoustic features, and speech intelligibility[14,15], and the dorsal pathway is associated with sensorimotor mapping between auditory and motor representations[16,17], speech production[14,18], and silent articulatory organ movement[19]. Although this dual-stream model proposes that each of the pathways plays a specifi c role in speech perception, how the streams interact with each other is still not clear. The other model, the forward-inverse model, proposes that the motor cortical regions predict the consequences of motor commands and revise the signals with the changing environment[20,21]. In more detail, before motor commands reach the effe</context>
</contexts>
<marker>[15]</marker>
<rawString>Scott SK, Blank CC, Rosen S, Wise RJ. Identification of a pathway for intelligible speech in the left temporal lobe. Brain 2000, 123: 2400–2406.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hickok</author>
<author>D Poeppel</author>
</authors>
<title>Towards a functional neuroanatomy of speech perception. Trends Cogn Sci</title>
<date>2000</date>
<volume>4</volume>
<pages>131--138</pages>
<contexts>
<context citStr="[16,17]" endWordPosition="1081" position="7364" startWordPosition="1081">ech perception. For example, the dual-stream processing model suggests that there are two pathways in audition: one is the ventral pathway down to the temporal lobe regulating “what” in acoustic information, and the other is the dorsal pathway from primary sensory areas up to the posterior cortex regulating “how” speech production takes place[12,13]. It is also known that the ventral pathway is involved in analyzing phonetic characters, acoustic features, and speech intelligibility[14,15], and the dorsal pathway is associated with sensorimotor mapping between auditory and motor representations[16,17], speech production[14,18], and silent articulatory organ movement[19]. Although this dual-stream model proposes that each of the pathways plays a specifi c role in speech perception, how the streams interact with each other is still not clear. The other model, the forward-inverse model, proposes that the motor cortical regions predict the consequences of motor commands and revise the signals with the changing environment[20,21]. In more detail, before motor commands reach the effectors, the forward-inverse model produces predicted sensory consequences of the motor commands, and then compares </context>
<context citStr="[16,17]" endWordPosition="1910" position="13322" startWordPosition="1910"> impairment of speech perception and comprehension, they can speak fl uently[36,37]. The dissociations in expressive and receptive aphasia support another view that speech perception and production are two distinct processes. Moreover, patients with lesions in Broca’s area perform well in both word-comprehension and syllable-identification tests, but patients with temporal lobule damage perform poorly in these tests[38]. These studies also negate the role of motor regions in speech perception, but support the view that temporal regions rather than motor areas are important in speech perception[16,17]. Research in child development has also shown dissociations between speech perception and speech production. Children born with hearing loss can learn to speak if they gain enough positive somatosensory feedback, even though the learning process is much harder than in healthy children[39]. Also, children with severe dysarthria are unable to produce meaningful sentences, but they can accurately understand spoken content[40]. Furthermore, infants usually learn to understand speech first and then begin to learn how to produce their own words[41,42]. These studies indicate that dissociations betw</context>
</contexts>
<marker>[16]</marker>
<rawString>Hickok G, Poeppel D. Towards a functional neuroanatomy of speech perception. Trends Cogn Sci 2000, 4: 131–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hickok</author>
<author>D Poeppel</author>
</authors>
<title>The cortical organization of speech processing.</title>
<date>2007</date>
<journal>Nat Neurosci</journal>
<volume>8</volume>
<pages>393--402</pages>
<contexts>
<context citStr="[16,17]" endWordPosition="1081" position="7364" startWordPosition="1081">ech perception. For example, the dual-stream processing model suggests that there are two pathways in audition: one is the ventral pathway down to the temporal lobe regulating “what” in acoustic information, and the other is the dorsal pathway from primary sensory areas up to the posterior cortex regulating “how” speech production takes place[12,13]. It is also known that the ventral pathway is involved in analyzing phonetic characters, acoustic features, and speech intelligibility[14,15], and the dorsal pathway is associated with sensorimotor mapping between auditory and motor representations[16,17], speech production[14,18], and silent articulatory organ movement[19]. Although this dual-stream model proposes that each of the pathways plays a specifi c role in speech perception, how the streams interact with each other is still not clear. The other model, the forward-inverse model, proposes that the motor cortical regions predict the consequences of motor commands and revise the signals with the changing environment[20,21]. In more detail, before motor commands reach the effectors, the forward-inverse model produces predicted sensory consequences of the motor commands, and then compares </context>
<context citStr="[16,17]" endWordPosition="1910" position="13322" startWordPosition="1910"> impairment of speech perception and comprehension, they can speak fl uently[36,37]. The dissociations in expressive and receptive aphasia support another view that speech perception and production are two distinct processes. Moreover, patients with lesions in Broca’s area perform well in both word-comprehension and syllable-identification tests, but patients with temporal lobule damage perform poorly in these tests[38]. These studies also negate the role of motor regions in speech perception, but support the view that temporal regions rather than motor areas are important in speech perception[16,17]. Research in child development has also shown dissociations between speech perception and speech production. Children born with hearing loss can learn to speak if they gain enough positive somatosensory feedback, even though the learning process is much harder than in healthy children[39]. Also, children with severe dysarthria are unable to produce meaningful sentences, but they can accurately understand spoken content[40]. Furthermore, infants usually learn to understand speech first and then begin to learn how to produce their own words[41,42]. These studies indicate that dissociations betw</context>
<context citStr="[17]" endWordPosition="2193" position="15290" startWordPosition="2193">ion of the central sulcus (adapted from Meister et al. [33] with permission). Neurosci Bull June 1, 2014, 30(3): 490–496494 speech perception and speech production. Also note that some fMRI studies did not reveal an increased activation in motor cortical regions during speech perception and comprehension[19,43,44]. The different results in the studies described above may be due to different task demands. The Hickok and Poeppel study showed that when the task load is high, requiring both speech identification and speech categorization, the activated frontal region extends to the premotor cortex[17]. Also, when speech signals are distorted, the motor cortex is markedly activated[45,46]. Interestingly, non-verbal signals can activate motor areas[31,47], and there is no difference in activation magnitude in the motor cortex between perceiving speech and perceiving nonverbal sounds[31]. Some studies have further shown that blurred speech causes even stronger activation in the bilateral premotor cortex, compared to clear speech[45] and perceiving a foreign language causes larger activation in the motor cortex than the native language[25]. Also, lowfrequency words induce higher activation in </context>
</contexts>
<marker>[17]</marker>
<rawString>Hickok G, Poeppel D. The cortical organization of speech processing. Nat Neurosci 2007, 8: 393–402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Baddeley</author>
<author>V Lewis</author>
<author>G Vallar</author>
</authors>
<title>Exploring the articulatory loop.</title>
<date>1984</date>
<journal>Q J Exp Psychol</journal>
<volume>36</volume>
<pages>233--252</pages>
<contexts>
<context citStr="[14,18]" endWordPosition="1083" position="7390" startWordPosition="1083">e, the dual-stream processing model suggests that there are two pathways in audition: one is the ventral pathway down to the temporal lobe regulating “what” in acoustic information, and the other is the dorsal pathway from primary sensory areas up to the posterior cortex regulating “how” speech production takes place[12,13]. It is also known that the ventral pathway is involved in analyzing phonetic characters, acoustic features, and speech intelligibility[14,15], and the dorsal pathway is associated with sensorimotor mapping between auditory and motor representations[16,17], speech production[14,18], and silent articulatory organ movement[19]. Although this dual-stream model proposes that each of the pathways plays a specifi c role in speech perception, how the streams interact with each other is still not clear. The other model, the forward-inverse model, proposes that the motor cortical regions predict the consequences of motor commands and revise the signals with the changing environment[20,21]. In more detail, before motor commands reach the effectors, the forward-inverse model produces predicted sensory consequences of the motor commands, and then compares the predicted results with</context>
</contexts>
<marker>[18]</marker>
<rawString>Baddeley A, Lewis V, Vallar G. Exploring the articulatory loop. Q J Exp Psychol 1984, 36: 233–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wise RJ</author>
<author>Scott SK</author>
<author>Blank SC</author>
<author>Mummery CJ</author>
<author>K Murphy</author>
<author>Warburton EA</author>
</authors>
<date>2001</date>
<booktitle>Separate neural subsystems within Wernicke’s area. Brain</booktitle>
<pages>124--83</pages>
<contexts>
<context citStr="[19]" endWordPosition="1088" position="7434" startWordPosition="1088">at there are two pathways in audition: one is the ventral pathway down to the temporal lobe regulating “what” in acoustic information, and the other is the dorsal pathway from primary sensory areas up to the posterior cortex regulating “how” speech production takes place[12,13]. It is also known that the ventral pathway is involved in analyzing phonetic characters, acoustic features, and speech intelligibility[14,15], and the dorsal pathway is associated with sensorimotor mapping between auditory and motor representations[16,17], speech production[14,18], and silent articulatory organ movement[19]. Although this dual-stream model proposes that each of the pathways plays a specifi c role in speech perception, how the streams interact with each other is still not clear. The other model, the forward-inverse model, proposes that the motor cortical regions predict the consequences of motor commands and revise the signals with the changing environment[20,21]. In more detail, before motor commands reach the effectors, the forward-inverse model produces predicted sensory consequences of the motor commands, and then compares the predicted results with the real sensory information. This comparis</context>
<context citStr="[19,43,44]" endWordPosition="2149" position="15001" startWordPosition="2149">n Fig. 2. Representative fMRI activation in the premotor cortex (PMC) associated with discriminating voiceless stop consonants in single syllables masked by white noise in two representative participants. Regions selected for stimulation are shown in bright colors. Arrowheads indicate the location of the central sulcus (adapted from Meister et al. [33] with permission). Neurosci Bull June 1, 2014, 30(3): 490–496494 speech perception and speech production. Also note that some fMRI studies did not reveal an increased activation in motor cortical regions during speech perception and comprehension[19,43,44]. The different results in the studies described above may be due to different task demands. The Hickok and Poeppel study showed that when the task load is high, requiring both speech identification and speech categorization, the activated frontal region extends to the premotor cortex[17]. Also, when speech signals are distorted, the motor cortex is markedly activated[45,46]. Interestingly, non-verbal signals can activate motor areas[31,47], and there is no difference in activation magnitude in the motor cortex between perceiving speech and perceiving nonverbal sounds[31]. Some studies have fu</context>
</contexts>
<marker>[19]</marker>
<rawString>Wise RJ, Scott SK, Blank SC, Mummery CJ, Murphy K, Warburton EA. Separate neural subsystems within Wernicke’s area. Brain 2001, 124: 83–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andersen RA</author>
<author>Buneo CA</author>
</authors>
<title>Intentional maps in posterior parietal cortex. Annu Rev Neurosci</title>
<date>2002</date>
<volume>25</volume>
<pages>189--220</pages>
<contexts>
<context citStr="[20,21]" endWordPosition="1143" position="7796" startWordPosition="1143">racters, acoustic features, and speech intelligibility[14,15], and the dorsal pathway is associated with sensorimotor mapping between auditory and motor representations[16,17], speech production[14,18], and silent articulatory organ movement[19]. Although this dual-stream model proposes that each of the pathways plays a specifi c role in speech perception, how the streams interact with each other is still not clear. The other model, the forward-inverse model, proposes that the motor cortical regions predict the consequences of motor commands and revise the signals with the changing environment[20,21]. In more detail, before motor commands reach the effectors, the forward-inverse model produces predicted sensory consequences of the motor commands, and then compares the predicted results with the real sensory information. This comparison provides more information for the central system to produce a more appropriate performance. With time delays and interruptions from the surroundings, the motor commands need to be up-dated from time to time in order to produce the desired outcome. Thus, when a speech signal is distorted by environmental noise and/or time delays, the motor representation of </context>
</contexts>
<marker>[20]</marker>
<rawString>Andersen RA, Buneo CA. Intentional maps in posterior parietal cortex. Annu Rev Neurosci 2002, 25: 189–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolpert DM</author>
<author>K Doya</author>
<author>M Kawato</author>
</authors>
<title>A unifying computational framework for motor control and social interaction.</title>
<date>2003</date>
<journal>Philos Trans R Soc Lond B Biol Sci</journal>
<pages>358--593</pages>
<contexts>
<context citStr="[20,21]" endWordPosition="1143" position="7796" startWordPosition="1143">racters, acoustic features, and speech intelligibility[14,15], and the dorsal pathway is associated with sensorimotor mapping between auditory and motor representations[16,17], speech production[14,18], and silent articulatory organ movement[19]. Although this dual-stream model proposes that each of the pathways plays a specifi c role in speech perception, how the streams interact with each other is still not clear. The other model, the forward-inverse model, proposes that the motor cortical regions predict the consequences of motor commands and revise the signals with the changing environment[20,21]. In more detail, before motor commands reach the effectors, the forward-inverse model produces predicted sensory consequences of the motor commands, and then compares the predicted results with the real sensory information. This comparison provides more information for the central system to produce a more appropriate performance. With time delays and interruptions from the surroundings, the motor commands need to be up-dated from time to time in order to produce the desired outcome. Thus, when a speech signal is distorted by environmental noise and/or time delays, the motor representation of </context>
</contexts>
<marker>[21]</marker>
<rawString>Wolpert DM, Doya K, Kawato M. A unifying computational framework for motor control and social interaction. Philos Trans R Soc Lond B Biol Sci 2003, 358: 593–602.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Adank</author>
<author>Devlin JT</author>
</authors>
<title>On-line plasticity in spoken sentence comprehension: Adapting to time-compressed speech. Neuroimage</title>
<date>2010</date>
<volume>49</volume>
<pages>1124</pages>
<contexts>
<context citStr="[22]" endWordPosition="1395" position="9587" startWordPosition="1395"> we propose that further-developed models should emphasize how the auditory-motor interaction is modulated by both processing load (due to complex inputs) and prediction/ estimation (due to task goals and feedback) (Fig. 1). So far, co-activations between auditory and motor regions in speech perception have been clearly demonstrated. When exposed to novel speech distortions, such as timecompressed sentences, listeners can rapidly distinguish distorted sentences from normal-speed sentences, with increased activation associations between the auditory cortices and the left ventral premotor cortex[22]. Moreover, compared to listening to pseudo-words and reversedwords, listening to normal words induces broad activation connectivity in the auditory-motor network, which may be useful for facilitating semantic processing[23]. Further investigation is needed to verify whether this enhanced dynamic auditory-motor network promotes the transition from a sound stream into a series of meaningful motorbased units and results in speech comprehension. In addition to the well-known fact that speech production is tightly related to the motor cortex, some studies have shown that the motor cortex is activa</context>
</contexts>
<marker>[22]</marker>
<rawString>Adank P, Devlin JT. On-line plasticity in spoken sentence comprehension: Adapting to time-compressed speech. Neuroimage 2010, 49: 1124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Londei</author>
<author>A D’Ausilio</author>
<author>D Basso</author>
<author>C Sestieri</author>
<author>Gratta CD</author>
<author>Romani GL</author>
</authors>
<title>Sensory-motor brain network connectivity for speech comprehension. Hum Brain Mapp</title>
<date>2010</date>
<pages>31--567</pages>
<contexts>
<context citStr="[23]" endWordPosition="1424" position="9811" startWordPosition="1424"> So far, co-activations between auditory and motor regions in speech perception have been clearly demonstrated. When exposed to novel speech distortions, such as timecompressed sentences, listeners can rapidly distinguish distorted sentences from normal-speed sentences, with increased activation associations between the auditory cortices and the left ventral premotor cortex[22]. Moreover, compared to listening to pseudo-words and reversedwords, listening to normal words induces broad activation connectivity in the auditory-motor network, which may be useful for facilitating semantic processing[23]. Further investigation is needed to verify whether this enhanced dynamic auditory-motor network promotes the transition from a sound stream into a series of meaningful motorbased units and results in speech comprehension. In addition to the well-known fact that speech production is tightly related to the motor cortex, some studies have shown that the motor cortex is activated in speech perception tasks[24, 25, 26, 27, 28, 29, 30]. For example, when listeners Fig. 1. Modifi ed Auditory-Motor Interaction model based on the Dual-stream Processing model in combination with the Forward-inverse mod</context>
</contexts>
<marker>[23]</marker>
<rawString>Londei A, D’Ausilio A, Basso D, Sestieri C, Gratta CD, Romani GL, et al. Sensory-motor brain network connectivity for speech comprehension. Hum Brain Mapp 2010, 31: 567– 580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilson SM</author>
</authors>
<title>Listening to speech activates motor areas involved in speech production.</title>
<date>2004</date>
<journal>Nat Neurosci</journal>
<volume>7</volume>
<pages>701--702</pages>
<contexts>
<context citStr="[24, 25, 26, 27, 28, 29, 30]" endWordPosition="1486" position="10245" startWordPosition="1486">ing to pseudo-words and reversedwords, listening to normal words induces broad activation connectivity in the auditory-motor network, which may be useful for facilitating semantic processing[23]. Further investigation is needed to verify whether this enhanced dynamic auditory-motor network promotes the transition from a sound stream into a series of meaningful motorbased units and results in speech comprehension. In addition to the well-known fact that speech production is tightly related to the motor cortex, some studies have shown that the motor cortex is activated in speech perception tasks[24, 25, 26, 27, 28, 29, 30]. For example, when listeners Fig. 1. Modifi ed Auditory-Motor Interaction model based on the Dual-stream Processing model in combination with the Forward-inverse model. This model emphasizes that both processing load and prediction/estimation affect the auditory-motor interaction (white two-headed arrow). Red: the auditory processing system including the ventral stream pathway; blue: the motor processing system including the dorsal stream pathway; green: the systems that mediate the processing load, prediction, and estimation that modulate the auditory-motor interaction (arrows with dashed li</context>
<context citStr="[24,31]" endWordPosition="1699" position="11792" startWordPosition="1699">various verbal stimuli may cause differential automatic activations of cortical regions involved in speech production. Also, it has been suggested that the activation of the motor cortex may refl ect a mediating role of the motor cortex in speech perception[28, 29, 30]. Moreover, studies using either functional magnetic resonance imaging (fMRI) or transcranial magnetic stimulation (TMS) have confirmed the role of the motor cortex in speech perception. For example, in a speech perception task, strong activation in the motor cortex can be induced only when participants perceive the target speech[24,31]. Some studies using TMS of the motor cortex have demonstrated that stimulation of speech-related regions affects speech perception[32, 33, 34]. For example, using TMS to suppress the left premotor cortex, which is activated both during speech production and speech perception (Fig. 2), Meister et al. found that participants with a suppressed premotor cortex were impaired in discriminating voiceless stop consonants under white-noise masking conditions. Thus, they suggested that the premotor cortex is essentially involved in speech perception[33]. However, the results of some clinical studies ap</context>
</contexts>
<marker>[24]</marker>
<rawString>Wilson SM. Listening to speech activates motor areas involved in speech production. Nat Neurosci 2004, 7: 701– 702.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilson SM</author>
<author>M Iacoboni</author>
</authors>
<title>Neural responses to non-native phonemes varying in producibil i ty: evidence for the sensorimotor nature of speech perception. Neuroimage</title>
<date>2006</date>
<volume>33</volume>
<pages>316--325</pages>
<contexts>
<context citStr="[24, 25, 26, 27, 28, 29, 30]" endWordPosition="1486" position="10245" startWordPosition="1486">ing to pseudo-words and reversedwords, listening to normal words induces broad activation connectivity in the auditory-motor network, which may be useful for facilitating semantic processing[23]. Further investigation is needed to verify whether this enhanced dynamic auditory-motor network promotes the transition from a sound stream into a series of meaningful motorbased units and results in speech comprehension. In addition to the well-known fact that speech production is tightly related to the motor cortex, some studies have shown that the motor cortex is activated in speech perception tasks[24, 25, 26, 27, 28, 29, 30]. For example, when listeners Fig. 1. Modifi ed Auditory-Motor Interaction model based on the Dual-stream Processing model in combination with the Forward-inverse model. This model emphasizes that both processing load and prediction/estimation affect the auditory-motor interaction (white two-headed arrow). Red: the auditory processing system including the ventral stream pathway; blue: the motor processing system including the dorsal stream pathway; green: the systems that mediate the processing load, prediction, and estimation that modulate the auditory-motor interaction (arrows with dashed li</context>
<context citStr="[25]" endWordPosition="2269" position="15835" startWordPosition="2269">activated frontal region extends to the premotor cortex[17]. Also, when speech signals are distorted, the motor cortex is markedly activated[45,46]. Interestingly, non-verbal signals can activate motor areas[31,47], and there is no difference in activation magnitude in the motor cortex between perceiving speech and perceiving nonverbal sounds[31]. Some studies have further shown that blurred speech causes even stronger activation in the bilateral premotor cortex, compared to clear speech[45] and perceiving a foreign language causes larger activation in the motor cortex than the native language[25]. Also, lowfrequency words induce higher activation in the motor cortex than high-frequency words[48]. Thus, facing unfamiliar stimuli (such as distorted speech, a foreign language, or low-frequency words), the motor cortex may play a role in facilitating the association with the auditory system to improve speech perception. More interestingly, in a mixed visual and auditory task, weaker visual stimuli evoke stronger activation in the motor cortex than clear pictures of speakers[49], suggesting that a heavy-load task, such as analyzing distorted auditory or visual signals, requires involvement</context>
</contexts>
<marker>[25]</marker>
<rawString>Wilson SM, Iacoboni M. Neural responses to non-native phonemes varying in producibil i ty: evidence for the sensorimotor nature of speech perception. Neuroimage 2006, 33: 316–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Fadiga</author>
<author>L Craighero</author>
<author>G Buccino</author>
<author>G Rizzolatti</author>
</authors>
<title>Speech listening specifically modulates the excitability of tongue muscles: a TMS study.</title>
<date>2002</date>
<journal>Eur J Neurosci</journal>
<volume>15</volume>
<pages>399--402</pages>
<contexts>
<context citStr="[24, 25, 26, 27, 28, 29, 30]" endWordPosition="1486" position="10245" startWordPosition="1486">ing to pseudo-words and reversedwords, listening to normal words induces broad activation connectivity in the auditory-motor network, which may be useful for facilitating semantic processing[23]. Further investigation is needed to verify whether this enhanced dynamic auditory-motor network promotes the transition from a sound stream into a series of meaningful motorbased units and results in speech comprehension. In addition to the well-known fact that speech production is tightly related to the motor cortex, some studies have shown that the motor cortex is activated in speech perception tasks[24, 25, 26, 27, 28, 29, 30]. For example, when listeners Fig. 1. Modifi ed Auditory-Motor Interaction model based on the Dual-stream Processing model in combination with the Forward-inverse model. This model emphasizes that both processing load and prediction/estimation affect the auditory-motor interaction (white two-headed arrow). Red: the auditory processing system including the ventral stream pathway; blue: the motor processing system including the dorsal stream pathway; green: the systems that mediate the processing load, prediction, and estimation that modulate the auditory-motor interaction (arrows with dashed li</context>
</contexts>
<marker>[26]</marker>
<rawString>Fadiga L, Craighero L, Buccino G, Rizzolatti G. Speech listening specifically modulates the excitability of tongue muscles: a TMS study. Eur J Neurosci 2002, 15: 399–402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pulvermüller</author>
<author>M Huss</author>
<author>F Kherif</author>
<author>del Prado Martin FM</author>
</authors>
<title>Motor cortex maps articulatory features of speech sounds.</title>
<date>2006</date>
<journal>Hauk Neurosci Bull June</journal>
<booktitle>Proc Natl Acad Sci U S A</booktitle>
<volume>30</volume>
<issue>3</issue>
<pages>490--496496</pages>
<contexts>
<context citStr="[24, 25, 26, 27, 28, 29, 30]" endWordPosition="1486" position="10245" startWordPosition="1486">ing to pseudo-words and reversedwords, listening to normal words induces broad activation connectivity in the auditory-motor network, which may be useful for facilitating semantic processing[23]. Further investigation is needed to verify whether this enhanced dynamic auditory-motor network promotes the transition from a sound stream into a series of meaningful motorbased units and results in speech comprehension. In addition to the well-known fact that speech production is tightly related to the motor cortex, some studies have shown that the motor cortex is activated in speech perception tasks[24, 25, 26, 27, 28, 29, 30]. For example, when listeners Fig. 1. Modifi ed Auditory-Motor Interaction model based on the Dual-stream Processing model in combination with the Forward-inverse model. This model emphasizes that both processing load and prediction/estimation affect the auditory-motor interaction (white two-headed arrow). Red: the auditory processing system including the ventral stream pathway; blue: the motor processing system including the dorsal stream pathway; green: the systems that mediate the processing load, prediction, and estimation that modulate the auditory-motor interaction (arrows with dashed li</context>
</contexts>
<marker>[27]</marker>
<rawString>Pulvermüller F, Huss M, Kherif F, del Prado Martin FM, Hauk Neurosci Bull June 1, 2014, 30(3): 490–496496 O, Shtyrov Y. Motor cortex maps articulatory features of speech sounds. Proc Natl Acad Sci U S A 2006, 103: 7865– 7870.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bever TG</author>
<author>D Poeppel</author>
</authors>
<title>Analysis by synthesis: a (re-) emerging program of research for language and vision. Biolinguistics</title>
<date>2010</date>
<volume>4</volume>
<pages>174--200</pages>
<contexts>
<context citStr="[24, 25, 26, 27, 28, 29, 30]" endWordPosition="1486" position="10245" startWordPosition="1486">ing to pseudo-words and reversedwords, listening to normal words induces broad activation connectivity in the auditory-motor network, which may be useful for facilitating semantic processing[23]. Further investigation is needed to verify whether this enhanced dynamic auditory-motor network promotes the transition from a sound stream into a series of meaningful motorbased units and results in speech comprehension. In addition to the well-known fact that speech production is tightly related to the motor cortex, some studies have shown that the motor cortex is activated in speech perception tasks[24, 25, 26, 27, 28, 29, 30]. For example, when listeners Fig. 1. Modifi ed Auditory-Motor Interaction model based on the Dual-stream Processing model in combination with the Forward-inverse model. This model emphasizes that both processing load and prediction/estimation affect the auditory-motor interaction (white two-headed arrow). Red: the auditory processing system including the ventral stream pathway; blue: the motor processing system including the dorsal stream pathway; green: the systems that mediate the processing load, prediction, and estimation that modulate the auditory-motor interaction (arrows with dashed li</context>
</contexts>
<marker>[28]</marker>
<rawString>Bever TG, Poeppel D. Analysis by synthesis: a (re-) emerging program of research for language and vision. Biolinguistics 2010, 4: 174–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Callan DE</author>
<author>Jones JA</author>
<author>Callan AM</author>
<author>R Akahane-Yamada</author>
</authors>
<title>Phonetic perceptual identification by native-and secondlanguage speakers differentially activates brain regions involved with acoustic phonetic processing and those involved with articulatory-auditory/orosensory internal models. Neuroimage</title>
<date>2004</date>
<volume>22</volume>
<pages>1182--1194</pages>
<contexts>
<context citStr="[24, 25, 26, 27, 28, 29, 30]" endWordPosition="1486" position="10245" startWordPosition="1486">ing to pseudo-words and reversedwords, listening to normal words induces broad activation connectivity in the auditory-motor network, which may be useful for facilitating semantic processing[23]. Further investigation is needed to verify whether this enhanced dynamic auditory-motor network promotes the transition from a sound stream into a series of meaningful motorbased units and results in speech comprehension. In addition to the well-known fact that speech production is tightly related to the motor cortex, some studies have shown that the motor cortex is activated in speech perception tasks[24, 25, 26, 27, 28, 29, 30]. For example, when listeners Fig. 1. Modifi ed Auditory-Motor Interaction model based on the Dual-stream Processing model in combination with the Forward-inverse model. This model emphasizes that both processing load and prediction/estimation affect the auditory-motor interaction (white two-headed arrow). Red: the auditory processing system including the ventral stream pathway; blue: the motor processing system including the dorsal stream pathway; green: the systems that mediate the processing load, prediction, and estimation that modulate the auditory-motor interaction (arrows with dashed li</context>
</contexts>
<marker>[29]</marker>
<rawString>Callan DE, Jones JA, Callan AM, Akahane-Yamada R. Phonetic perceptual identification by native-and secondlanguage speakers differentially activates brain regions involved with acoustic phonetic processing and those involved with articulatory-auditory/orosensory internal models. Neuroimage 2004, 22: 1182–1194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hickok</author>
<author>J Houde</author>
<author>F Rong</author>
</authors>
<title>Sensorimotor integration in speech processing: computational basis and neural organization. Neuron</title>
<date>2011</date>
<volume>69</volume>
<pages>407--422</pages>
<contexts>
<context citStr="[24, 25, 26, 27, 28, 29, 30]" endWordPosition="1486" position="10245" startWordPosition="1486">ing to pseudo-words and reversedwords, listening to normal words induces broad activation connectivity in the auditory-motor network, which may be useful for facilitating semantic processing[23]. Further investigation is needed to verify whether this enhanced dynamic auditory-motor network promotes the transition from a sound stream into a series of meaningful motorbased units and results in speech comprehension. In addition to the well-known fact that speech production is tightly related to the motor cortex, some studies have shown that the motor cortex is activated in speech perception tasks[24, 25, 26, 27, 28, 29, 30]. For example, when listeners Fig. 1. Modifi ed Auditory-Motor Interaction model based on the Dual-stream Processing model in combination with the Forward-inverse model. This model emphasizes that both processing load and prediction/estimation affect the auditory-motor interaction (white two-headed arrow). Red: the auditory processing system including the ventral stream pathway; blue: the motor processing system including the dorsal stream pathway; green: the systems that mediate the processing load, prediction, and estimation that modulate the auditory-motor interaction (arrows with dashed li</context>
</contexts>
<marker>[30]</marker>
<rawString>Hickok G, Houde J, Rong F. Sensorimotor integration in speech processing: computational basis and neural organization. Neuron 2011, 69: 407–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Watkins KE</author>
<author>Strafella AP</author>
<author>T Paus</author>
</authors>
<title>Seeing and hearing speech excites the motor system involved in speech production. Neuropsychologia</title>
<date>2003</date>
<volume>41</volume>
<pages>989--994</pages>
<contexts>
<context citStr="[24,31]" endWordPosition="1699" position="11792" startWordPosition="1699">various verbal stimuli may cause differential automatic activations of cortical regions involved in speech production. Also, it has been suggested that the activation of the motor cortex may refl ect a mediating role of the motor cortex in speech perception[28, 29, 30]. Moreover, studies using either functional magnetic resonance imaging (fMRI) or transcranial magnetic stimulation (TMS) have confirmed the role of the motor cortex in speech perception. For example, in a speech perception task, strong activation in the motor cortex can be induced only when participants perceive the target speech[24,31]. Some studies using TMS of the motor cortex have demonstrated that stimulation of speech-related regions affects speech perception[32, 33, 34]. For example, using TMS to suppress the left premotor cortex, which is activated both during speech production and speech perception (Fig. 2), Meister et al. found that participants with a suppressed premotor cortex were impaired in discriminating voiceless stop consonants under white-noise masking conditions. Thus, they suggested that the premotor cortex is essentially involved in speech perception[33]. However, the results of some clinical studies ap</context>
<context citStr="[31,47]" endWordPosition="2212" position="15445" startWordPosition="2212"> production. Also note that some fMRI studies did not reveal an increased activation in motor cortical regions during speech perception and comprehension[19,43,44]. The different results in the studies described above may be due to different task demands. The Hickok and Poeppel study showed that when the task load is high, requiring both speech identification and speech categorization, the activated frontal region extends to the premotor cortex[17]. Also, when speech signals are distorted, the motor cortex is markedly activated[45,46]. Interestingly, non-verbal signals can activate motor areas[31,47], and there is no difference in activation magnitude in the motor cortex between perceiving speech and perceiving nonverbal sounds[31]. Some studies have further shown that blurred speech causes even stronger activation in the bilateral premotor cortex, compared to clear speech[45] and perceiving a foreign language causes larger activation in the motor cortex than the native language[25]. Also, lowfrequency words induce higher activation in the motor cortex than high-frequency words[48]. Thus, facing unfamiliar stimuli (such as distorted speech, a foreign language, or low-frequency words), the</context>
</contexts>
<marker>[31]</marker>
<rawString>Watkins KE, Strafella AP, Paus T. Seeing and hearing speech excites the motor system involved in speech production. Neuropsychologia 2003, 41: 989–994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A D’Ausilio</author>
<author>F Pulvermüller</author>
<author>P Salmas</author>
<author>I Bufalari</author>
<author>C Begliomini</author>
<author>L Fadiga</author>
</authors>
<title>The motor somatotopy of speech perception.</title>
<date>2009</date>
<journal>Curr Biol</journal>
<volume>19</volume>
<pages>381--385</pages>
<contexts>
<context citStr="[32, 33, 34]" endWordPosition="1717" position="11935" startWordPosition="1717">uggested that the activation of the motor cortex may refl ect a mediating role of the motor cortex in speech perception[28, 29, 30]. Moreover, studies using either functional magnetic resonance imaging (fMRI) or transcranial magnetic stimulation (TMS) have confirmed the role of the motor cortex in speech perception. For example, in a speech perception task, strong activation in the motor cortex can be induced only when participants perceive the target speech[24,31]. Some studies using TMS of the motor cortex have demonstrated that stimulation of speech-related regions affects speech perception[32, 33, 34]. For example, using TMS to suppress the left premotor cortex, which is activated both during speech production and speech perception (Fig. 2), Meister et al. found that participants with a suppressed premotor cortex were impaired in discriminating voiceless stop consonants under white-noise masking conditions. Thus, they suggested that the premotor cortex is essentially involved in speech perception[33]. However, the results of some clinical studies appear not to support the view that there is an association between impairment of speech perception and impairment of speech production. For exam</context>
</contexts>
<marker>[32]</marker>
<rawString>D’Ausilio A, Pulvermüller F, Salmas P, Bufalari I, Begliomini C, Fadiga L. The motor somatotopy of speech perception. Curr Biol 2009, 19: 381–385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meister IG</author>
<author>Wilson SM</author>
<author>C Deblieck</author>
<author>Wu AD</author>
<author>M Iacoboni</author>
</authors>
<title>The essential role of premotor cortex in speech perception. Curr Biol</title>
<date>2007</date>
<volume>17</volume>
<pages>1692--1696</pages>
<contexts>
<context citStr="[32, 33, 34]" endWordPosition="1717" position="11935" startWordPosition="1717">uggested that the activation of the motor cortex may refl ect a mediating role of the motor cortex in speech perception[28, 29, 30]. Moreover, studies using either functional magnetic resonance imaging (fMRI) or transcranial magnetic stimulation (TMS) have confirmed the role of the motor cortex in speech perception. For example, in a speech perception task, strong activation in the motor cortex can be induced only when participants perceive the target speech[24,31]. Some studies using TMS of the motor cortex have demonstrated that stimulation of speech-related regions affects speech perception[32, 33, 34]. For example, using TMS to suppress the left premotor cortex, which is activated both during speech production and speech perception (Fig. 2), Meister et al. found that participants with a suppressed premotor cortex were impaired in discriminating voiceless stop consonants under white-noise masking conditions. Thus, they suggested that the premotor cortex is essentially involved in speech perception[33]. However, the results of some clinical studies appear not to support the view that there is an association between impairment of speech perception and impairment of speech production. For exam</context>
<context citStr="[33]" endWordPosition="2114" position="14745" startWordPosition="2114"> perception as the Motor Theory proposes. As reviewed above, some fMRI and TMS studies support the view that motor cortical areas are important in speech perception, while clinical and developmental studies have shown significant dissociations between Fig. 2. Representative fMRI activation in the premotor cortex (PMC) associated with discriminating voiceless stop consonants in single syllables masked by white noise in two representative participants. Regions selected for stimulation are shown in bright colors. Arrowheads indicate the location of the central sulcus (adapted from Meister et al. [33] with permission). Neurosci Bull June 1, 2014, 30(3): 490–496494 speech perception and speech production. Also note that some fMRI studies did not reveal an increased activation in motor cortical regions during speech perception and comprehension[19,43,44]. The different results in the studies described above may be due to different task demands. The Hickok and Poeppel study showed that when the task load is high, requiring both speech identification and speech categorization, the activated frontal region extends to the premotor cortex[17]. Also, when speech signals are distorted, the motor co</context>
</contexts>
<marker>[33]</marker>
<rawString>Meister IG, Wilson SM, Deblieck C, Wu AD, Iacoboni M. The essential role of premotor cortex in speech perception. Curr Biol 2007, 17: 1692–1696.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Watkins</author>
<author>T Paus</author>
</authors>
<title>Modulation of motor excitability during speech perception: the role of Broca’s area.</title>
<date>2004</date>
<journal>J Cogn Neurosci</journal>
<volume>16</volume>
<pages>978--987</pages>
<contexts>
<context citStr="[32, 33, 34]" endWordPosition="1717" position="11935" startWordPosition="1717">uggested that the activation of the motor cortex may refl ect a mediating role of the motor cortex in speech perception[28, 29, 30]. Moreover, studies using either functional magnetic resonance imaging (fMRI) or transcranial magnetic stimulation (TMS) have confirmed the role of the motor cortex in speech perception. For example, in a speech perception task, strong activation in the motor cortex can be induced only when participants perceive the target speech[24,31]. Some studies using TMS of the motor cortex have demonstrated that stimulation of speech-related regions affects speech perception[32, 33, 34]. For example, using TMS to suppress the left premotor cortex, which is activated both during speech production and speech perception (Fig. 2), Meister et al. found that participants with a suppressed premotor cortex were impaired in discriminating voiceless stop consonants under white-noise masking conditions. Thus, they suggested that the premotor cortex is essentially involved in speech perception[33]. However, the results of some clinical studies appear not to support the view that there is an association between impairment of speech perception and impairment of speech production. For exam</context>
</contexts>
<marker>[34]</marker>
<rawString>Watkins K, Paus T. Modulation of motor excitability during speech perception: the role of Broca’s area. J Cogn Neurosci 2004, 16: 978–987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohr JP</author>
<author>Pessin MS</author>
<author>S Finkelstein</author>
<author>Funkenstein HH</author>
<author>Duncan GW</author>
<author>Davis KR</author>
</authors>
<date>1978</date>
<booktitle>Broca aphasia Pathologic and clinical. Neurology</booktitle>
<volume>28</volume>
<pages>311--311</pages>
<contexts>
<context citStr="[35]" endWordPosition="1819" position="12659" startWordPosition="1819">ch perception (Fig. 2), Meister et al. found that participants with a suppressed premotor cortex were impaired in discriminating voiceless stop consonants under white-noise masking conditions. Thus, they suggested that the premotor cortex is essentially involved in speech perception[33]. However, the results of some clinical studies appear not to support the view that there is an association between impairment of speech perception and impairment of speech production. For example, patients with expressive aphasia exhibit impairment of speech production but not speech perception or comprehension[35]. Also, although patients with receptive aphasia exhibit impairment of speech perception and comprehension, they can speak fl uently[36,37]. The dissociations in expressive and receptive aphasia support another view that speech perception and production are two distinct processes. Moreover, patients with lesions in Broca’s area perform well in both word-comprehension and syllable-identification tests, but patients with temporal lobule damage perform poorly in these tests[38]. These studies also negate the role of motor regions in speech perception, but support the view that temporal regions ra</context>
</contexts>
<marker>[35]</marker>
<rawString>Mohr JP, Pessin MS, Finkelstein S, Funkenstein HH, Duncan GW, Davis KR. Broca aphasia Pathologic and clinical. Neurology 1978, 28: 311–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Crinion JT</author>
<author>Warburton EA</author>
<author>Lambon-Ralph MA</author>
<author>D Howard</author>
<author>Wise RJ</author>
</authors>
<title>Listening to narrative speech after aphasic stroke: the role of the left anterior temporal lobe. Cereb Cortex</title>
<date>2006</date>
<volume>16</volume>
<pages>1116--1125</pages>
<contexts>
<context citStr="[36,37]" endWordPosition="1837" position="12798" startWordPosition="1837">ss stop consonants under white-noise masking conditions. Thus, they suggested that the premotor cortex is essentially involved in speech perception[33]. However, the results of some clinical studies appear not to support the view that there is an association between impairment of speech perception and impairment of speech production. For example, patients with expressive aphasia exhibit impairment of speech production but not speech perception or comprehension[35]. Also, although patients with receptive aphasia exhibit impairment of speech perception and comprehension, they can speak fl uently[36,37]. The dissociations in expressive and receptive aphasia support another view that speech perception and production are two distinct processes. Moreover, patients with lesions in Broca’s area perform well in both word-comprehension and syllable-identification tests, but patients with temporal lobule damage perform poorly in these tests[38]. These studies also negate the role of motor regions in speech perception, but support the view that temporal regions rather than motor areas are important in speech perception[16,17]. Research in child development has also shown dissociations between speech </context>
</contexts>
<marker>[36]</marker>
<rawString>Crinion JT, Warburton EA, Lambon-Ralph MA, Howard D, Wise RJ. Listening to narrative speech after aphasic stroke: the role of the left anterior temporal lobe. Cereb Cortex 2006, 16: 1116–1125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bogen JE</author>
<author>Bogen GM</author>
</authors>
<title>Wernick&amp;apos;s region-Where is it?</title>
<date>1976</date>
<journal>Ann NY Acad Sci,</journal>
<pages>280--834</pages>
<contexts>
<context citStr="[36,37]" endWordPosition="1837" position="12798" startWordPosition="1837">ss stop consonants under white-noise masking conditions. Thus, they suggested that the premotor cortex is essentially involved in speech perception[33]. However, the results of some clinical studies appear not to support the view that there is an association between impairment of speech perception and impairment of speech production. For example, patients with expressive aphasia exhibit impairment of speech production but not speech perception or comprehension[35]. Also, although patients with receptive aphasia exhibit impairment of speech perception and comprehension, they can speak fl uently[36,37]. The dissociations in expressive and receptive aphasia support another view that speech perception and production are two distinct processes. Moreover, patients with lesions in Broca’s area perform well in both word-comprehension and syllable-identification tests, but patients with temporal lobule damage perform poorly in these tests[38]. These studies also negate the role of motor regions in speech perception, but support the view that temporal regions rather than motor areas are important in speech perception[16,17]. Research in child development has also shown dissociations between speech </context>
</contexts>
<marker>[37]</marker>
<rawString>Bogen JE, Bogen GM. Wernick&amp;apos;s region-Where is it? Ann NY Acad Sci, 1976, 280: 834–843.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Baker</author>
<author>Blumstein SE</author>
<author>H Goodglass</author>
</authors>
<title>Interact ion between phonological and semantic factors in auditory comprehension. Neuropsychologia</title>
<date>1981</date>
<volume>19</volume>
<pages>1--15</pages>
<contexts>
<context citStr="[38]" endWordPosition="1882" position="13138" startWordPosition="1882">mple, patients with expressive aphasia exhibit impairment of speech production but not speech perception or comprehension[35]. Also, although patients with receptive aphasia exhibit impairment of speech perception and comprehension, they can speak fl uently[36,37]. The dissociations in expressive and receptive aphasia support another view that speech perception and production are two distinct processes. Moreover, patients with lesions in Broca’s area perform well in both word-comprehension and syllable-identification tests, but patients with temporal lobule damage perform poorly in these tests[38]. These studies also negate the role of motor regions in speech perception, but support the view that temporal regions rather than motor areas are important in speech perception[16,17]. Research in child development has also shown dissociations between speech perception and speech production. Children born with hearing loss can learn to speak if they gain enough positive somatosensory feedback, even though the learning process is much harder than in healthy children[39]. Also, children with severe dysarthria are unable to produce meaningful sentences, but they can accurately understand spoken </context>
</contexts>
<marker>[38]</marker>
<rawString>Baker E, Blumstein SE, Goodglass H. Interact ion between phonological and semantic factors in auditory comprehension. Neuropsychologia 1981, 19: 1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bishop</author>
<author>K Mogford-Bevan</author>
</authors>
<title>Language Development in Exceptional Circumstances.</title>
<date>1993</date>
<publisher>Psychology Press</publisher>
<contexts>
<context citStr="[39]" endWordPosition="1952" position="13612" startWordPosition="1952">h word-comprehension and syllable-identification tests, but patients with temporal lobule damage perform poorly in these tests[38]. These studies also negate the role of motor regions in speech perception, but support the view that temporal regions rather than motor areas are important in speech perception[16,17]. Research in child development has also shown dissociations between speech perception and speech production. Children born with hearing loss can learn to speak if they gain enough positive somatosensory feedback, even though the learning process is much harder than in healthy children[39]. Also, children with severe dysarthria are unable to produce meaningful sentences, but they can accurately understand spoken content[40]. Furthermore, infants usually learn to understand speech first and then begin to learn how to produce their own words[41,42]. These studies indicate that dissociations between speech perception and speech production occur during development. As speech perception and speech production do not appear at the same time during development, motor cortical areas may not be as important for speech perception as the Motor Theory proposes. As reviewed above, some fMRI </context>
</contexts>
<marker>[39]</marker>
<rawString>Bishop D, Mogford-Bevan K. Language Development in Exceptional Circumstances. Psychology Press 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bishop CW</author>
<author>Miller LM</author>
</authors>
<title>A multisensory cortical network for understanding speech in noise.</title>
<date>2009</date>
<journal>J Cogn Neurosci</journal>
<volume>21</volume>
<pages>1790--1804</pages>
<contexts>
<context citStr="[40]" endWordPosition="1970" position="13749" startWordPosition="1970"> studies also negate the role of motor regions in speech perception, but support the view that temporal regions rather than motor areas are important in speech perception[16,17]. Research in child development has also shown dissociations between speech perception and speech production. Children born with hearing loss can learn to speak if they gain enough positive somatosensory feedback, even though the learning process is much harder than in healthy children[39]. Also, children with severe dysarthria are unable to produce meaningful sentences, but they can accurately understand spoken content[40]. Furthermore, infants usually learn to understand speech first and then begin to learn how to produce their own words[41,42]. These studies indicate that dissociations between speech perception and speech production occur during development. As speech perception and speech production do not appear at the same time during development, motor cortical areas may not be as important for speech perception as the Motor Theory proposes. As reviewed above, some fMRI and TMS studies support the view that motor cortical areas are important in speech perception, while clinical and developmental studies h</context>
<context citStr="[40]" endWordPosition="2450" position="17112" startWordPosition="2450"> is involved in speech perception is task-load dependent. In future, this assumption will be tested to confi rm whether the dissociations between speech perception and speech production under either clinical or developmental conditions are task-load related. Speech Perception under “Cocktail Party” Conditions Speech perception is not just for hearing speech sounds, but more essentially, for recognizing and understanding speech signals, requiring that multisensory modalities interact. In fact, speech understanding and speech hearing do not share the same brain network, including the motor areas[40]. In a noisy environment (like a cocktail party), although there are many acoustic sources from various directions, listeners are still able to identify and follow target speech sounds in this high perceptual-load situation. How can listeners separate various speakers’ signals and understand target sentences? Although this “cocktail party” problem advanced by Cherry[50] has not been fully solved, several lines of evidence suggest that the motor system plays a role in solving this problem when the perceptual load is high. First, observing a speaker’s articulator movements can induce better unde</context>
</contexts>
<marker>[40]</marker>
<rawString>Bishop CW, Miller LM. A multisensory cortical network for understanding speech in noise. J Cogn Neurosci 2009, 21: 1790–1804.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Werker JF</author>
<author>Yeung HH</author>
</authors>
<title>Infant speech perception bootstraps word learning. Trends Cogn Sci</title>
<date>2005</date>
<volume>9</volume>
<pages>519--527</pages>
<contexts>
<context citStr="[41,42]" endWordPosition="1989" position="13874" startWordPosition="1989">n motor areas are important in speech perception[16,17]. Research in child development has also shown dissociations between speech perception and speech production. Children born with hearing loss can learn to speak if they gain enough positive somatosensory feedback, even though the learning process is much harder than in healthy children[39]. Also, children with severe dysarthria are unable to produce meaningful sentences, but they can accurately understand spoken content[40]. Furthermore, infants usually learn to understand speech first and then begin to learn how to produce their own words[41,42]. These studies indicate that dissociations between speech perception and speech production occur during development. As speech perception and speech production do not appear at the same time during development, motor cortical areas may not be as important for speech perception as the Motor Theory proposes. As reviewed above, some fMRI and TMS studies support the view that motor cortical areas are important in speech perception, while clinical and developmental studies have shown significant dissociations between Fig. 2. Representative fMRI activation in the premotor cortex (PMC) associated wi</context>
</contexts>
<marker>[41]</marker>
<rawString>Werker JF, Yeung HH. Infant speech perception bootstraps word learning. Trends Cogn Sci 2005, 9: 519–527.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsao FM</author>
<author>Liu HM</author>
<author>Kuhl PK</author>
</authors>
<title>Speech perception in infancy predicts language development in the second year of life: a longitudinal study. Child Dev</title>
<date>2004</date>
<volume>75</volume>
<pages>1067--1084</pages>
<contexts>
<context citStr="[41,42]" endWordPosition="1989" position="13874" startWordPosition="1989">n motor areas are important in speech perception[16,17]. Research in child development has also shown dissociations between speech perception and speech production. Children born with hearing loss can learn to speak if they gain enough positive somatosensory feedback, even though the learning process is much harder than in healthy children[39]. Also, children with severe dysarthria are unable to produce meaningful sentences, but they can accurately understand spoken content[40]. Furthermore, infants usually learn to understand speech first and then begin to learn how to produce their own words[41,42]. These studies indicate that dissociations between speech perception and speech production occur during development. As speech perception and speech production do not appear at the same time during development, motor cortical areas may not be as important for speech perception as the Motor Theory proposes. As reviewed above, some fMRI and TMS studies support the view that motor cortical areas are important in speech perception, while clinical and developmental studies have shown significant dissociations between Fig. 2. Representative fMRI activation in the premotor cortex (PMC) associated wi</context>
</contexts>
<marker>[42]</marker>
<rawString>Tsao FM, Liu HM, Kuhl PK. Speech perception in infancy predicts language development in the second year of life: a longitudinal study. Child Dev 2004, 75: 1067–1084.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Basso</author>
<author>G Casati</author>
<author>Vignolo LA</author>
</authors>
<title>Phonemic identification defect in aphasia. Cortex</title>
<date>1977</date>
<pages>13--85</pages>
<contexts>
<context citStr="[19,43,44]" endWordPosition="2149" position="15001" startWordPosition="2149">n Fig. 2. Representative fMRI activation in the premotor cortex (PMC) associated with discriminating voiceless stop consonants in single syllables masked by white noise in two representative participants. Regions selected for stimulation are shown in bright colors. Arrowheads indicate the location of the central sulcus (adapted from Meister et al. [33] with permission). Neurosci Bull June 1, 2014, 30(3): 490–496494 speech perception and speech production. Also note that some fMRI studies did not reveal an increased activation in motor cortical regions during speech perception and comprehension[19,43,44]. The different results in the studies described above may be due to different task demands. The Hickok and Poeppel study showed that when the task load is high, requiring both speech identification and speech categorization, the activated frontal region extends to the premotor cortex[17]. Also, when speech signals are distorted, the motor cortex is markedly activated[45,46]. Interestingly, non-verbal signals can activate motor areas[31,47], and there is no difference in activation magnitude in the motor cortex between perceiving speech and perceiving nonverbal sounds[31]. Some studies have fu</context>
</contexts>
<marker>[43]</marker>
<rawString>Basso A, Casati G, Vignolo LA. Phonemic identification defect in aphasia. Cortex 1977, 13: 85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott SK</author>
<author>S Rosen</author>
<author>H Lang</author>
<author>Wise RJ</author>
</authors>
<title>Neural correlates of intelligibility in speech investigated with noise vocoded speech-a positron emission tomography study.</title>
<date>2006</date>
<journal>J Acoust Soc Am</journal>
<pages>120--1075</pages>
<contexts>
<context citStr="[19,43,44]" endWordPosition="2149" position="15001" startWordPosition="2149">n Fig. 2. Representative fMRI activation in the premotor cortex (PMC) associated with discriminating voiceless stop consonants in single syllables masked by white noise in two representative participants. Regions selected for stimulation are shown in bright colors. Arrowheads indicate the location of the central sulcus (adapted from Meister et al. [33] with permission). Neurosci Bull June 1, 2014, 30(3): 490–496494 speech perception and speech production. Also note that some fMRI studies did not reveal an increased activation in motor cortical regions during speech perception and comprehension[19,43,44]. The different results in the studies described above may be due to different task demands. The Hickok and Poeppel study showed that when the task load is high, requiring both speech identification and speech categorization, the activated frontal region extends to the premotor cortex[17]. Also, when speech signals are distorted, the motor cortex is markedly activated[45,46]. Interestingly, non-verbal signals can activate motor areas[31,47], and there is no difference in activation magnitude in the motor cortex between perceiving speech and perceiving nonverbal sounds[31]. Some studies have fu</context>
</contexts>
<marker>[44]</marker>
<rawString>Scott SK, Rosen S, Lang H, Wise RJ. Neural correlates of intelligibility in speech investigated with noise vocoded speech-a positron emission tomography study. J Acoust Soc Am 2006, 120: 1075.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davis MH</author>
<author>Johnsrude IS</author>
</authors>
<title>Hierarchical processing in spoken language comprehension.</title>
<date>2003</date>
<journal>J Neurosci</journal>
<volume>23</volume>
<pages>3423--3431</pages>
<contexts>
<context citStr="[45,46]" endWordPosition="2205" position="15378" startWordPosition="2205">i Bull June 1, 2014, 30(3): 490–496494 speech perception and speech production. Also note that some fMRI studies did not reveal an increased activation in motor cortical regions during speech perception and comprehension[19,43,44]. The different results in the studies described above may be due to different task demands. The Hickok and Poeppel study showed that when the task load is high, requiring both speech identification and speech categorization, the activated frontal region extends to the premotor cortex[17]. Also, when speech signals are distorted, the motor cortex is markedly activated[45,46]. Interestingly, non-verbal signals can activate motor areas[31,47], and there is no difference in activation magnitude in the motor cortex between perceiving speech and perceiving nonverbal sounds[31]. Some studies have further shown that blurred speech causes even stronger activation in the bilateral premotor cortex, compared to clear speech[45] and perceiving a foreign language causes larger activation in the motor cortex than the native language[25]. Also, lowfrequency words induce higher activation in the motor cortex than high-frequency words[48]. Thus, facing unfamiliar stimuli (such as</context>
</contexts>
<marker>[45]</marker>
<rawString>Davis MH, Johnsrude IS. Hierarchical processing in spoken language comprehension. J Neurosci 2003, 23: 3423–3431.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Uppenkamp</author>
<author>Johnsrude IS</author>
<author>D Norris</author>
<author>W Marslen-Wilson</author>
<author>Patterson RD</author>
</authors>
<title>Locating the initial stages of speech? Sound processing in human temporal cortex. Neuroimage</title>
<date>2006</date>
<volume>31</volume>
<pages>1284--1296</pages>
<contexts>
<context citStr="[45,46]" endWordPosition="2205" position="15378" startWordPosition="2205">i Bull June 1, 2014, 30(3): 490–496494 speech perception and speech production. Also note that some fMRI studies did not reveal an increased activation in motor cortical regions during speech perception and comprehension[19,43,44]. The different results in the studies described above may be due to different task demands. The Hickok and Poeppel study showed that when the task load is high, requiring both speech identification and speech categorization, the activated frontal region extends to the premotor cortex[17]. Also, when speech signals are distorted, the motor cortex is markedly activated[45,46]. Interestingly, non-verbal signals can activate motor areas[31,47], and there is no difference in activation magnitude in the motor cortex between perceiving speech and perceiving nonverbal sounds[31]. Some studies have further shown that blurred speech causes even stronger activation in the bilateral premotor cortex, compared to clear speech[45] and perceiving a foreign language causes larger activation in the motor cortex than the native language[25]. Also, lowfrequency words induce higher activation in the motor cortex than high-frequency words[48]. Thus, facing unfamiliar stimuli (such as</context>
</contexts>
<marker>[46]</marker>
<rawString>Uppenkamp S, Johnsrude IS, Norris D, Marslen-Wilson W, Patterson RD. Locating the initial stages of speech? Sound processing in human temporal cortex. Neuroimage 2006, 31: 1284–1296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Warren JE</author>
<author>Sauter DA</author>
<author>F Eisner</author>
<author>J Wiland</author>
<author>M Dresner</author>
<author>Wise RJ</author>
</authors>
<title>Positive emotions preferentially engage an auditory–motor “mirror” system.</title>
<date>2006</date>
<journal>J Neurosci</journal>
<volume>26</volume>
<pages>13067--13075</pages>
<contexts>
<context citStr="[31,47]" endWordPosition="2212" position="15445" startWordPosition="2212"> production. Also note that some fMRI studies did not reveal an increased activation in motor cortical regions during speech perception and comprehension[19,43,44]. The different results in the studies described above may be due to different task demands. The Hickok and Poeppel study showed that when the task load is high, requiring both speech identification and speech categorization, the activated frontal region extends to the premotor cortex[17]. Also, when speech signals are distorted, the motor cortex is markedly activated[45,46]. Interestingly, non-verbal signals can activate motor areas[31,47], and there is no difference in activation magnitude in the motor cortex between perceiving speech and perceiving nonverbal sounds[31]. Some studies have further shown that blurred speech causes even stronger activation in the bilateral premotor cortex, compared to clear speech[45] and perceiving a foreign language causes larger activation in the motor cortex than the native language[25]. Also, lowfrequency words induce higher activation in the motor cortex than high-frequency words[48]. Thus, facing unfamiliar stimuli (such as distorted speech, a foreign language, or low-frequency words), the</context>
</contexts>
<marker>[47]</marker>
<rawString>Warren JE, Sauter DA, Eisner F, Wiland J, Dresner M, Wise RJ, et al. Positive emotions preferentially engage an auditory–motor “mirror” system. J Neurosci 2006, 26: 13067– 13075.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy AC</author>
<author>L Craighero</author>
<author>Fabbr i -Destro M</author>
<author>L Fadiga</author>
</authors>
<title>Phonological and lexical motor facilitation during speech listening: A transcranial magnetic stimulation study.</title>
<date>2008</date>
<journal>J Physiol</journal>
<pages>102--101</pages>
<location>Paris</location>
<contexts>
<context citStr="[48]" endWordPosition="2283" position="15936" startWordPosition="2283"> the motor cortex is markedly activated[45,46]. Interestingly, non-verbal signals can activate motor areas[31,47], and there is no difference in activation magnitude in the motor cortex between perceiving speech and perceiving nonverbal sounds[31]. Some studies have further shown that blurred speech causes even stronger activation in the bilateral premotor cortex, compared to clear speech[45] and perceiving a foreign language causes larger activation in the motor cortex than the native language[25]. Also, lowfrequency words induce higher activation in the motor cortex than high-frequency words[48]. Thus, facing unfamiliar stimuli (such as distorted speech, a foreign language, or low-frequency words), the motor cortex may play a role in facilitating the association with the auditory system to improve speech perception. More interestingly, in a mixed visual and auditory task, weaker visual stimuli evoke stronger activation in the motor cortex than clear pictures of speakers[49], suggesting that a heavy-load task, such as analyzing distorted auditory or visual signals, requires involvement of the motor cortex. These studies suggest that whether the motor system is involved in speech perce</context>
</contexts>
<marker>[48]</marker>
<rawString>Roy AC, Craighero L, Fabbr i -Destro M, Fadiga L. Phonological and lexical motor facilitation during speech listening: A transcranial magnetic stimulation study. J Physiol Paris 2008, 102: 101–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fridriksson</author>
<author>J Moss</author>
<author>B Davis</author>
<author>Baylis GC</author>
<author>L Bonilha</author>
<author>C Rorden</author>
</authors>
<title>Motor speech perception modulates the cortical language areas. Neuroimage</title>
<date>2008</date>
<volume>41</volume>
<pages>605--613</pages>
<contexts>
<context citStr="[49]" endWordPosition="2340" position="16322" startWordPosition="2340">peech[45] and perceiving a foreign language causes larger activation in the motor cortex than the native language[25]. Also, lowfrequency words induce higher activation in the motor cortex than high-frequency words[48]. Thus, facing unfamiliar stimuli (such as distorted speech, a foreign language, or low-frequency words), the motor cortex may play a role in facilitating the association with the auditory system to improve speech perception. More interestingly, in a mixed visual and auditory task, weaker visual stimuli evoke stronger activation in the motor cortex than clear pictures of speakers[49], suggesting that a heavy-load task, such as analyzing distorted auditory or visual signals, requires involvement of the motor cortex. These studies suggest that whether the motor system is involved in speech perception is task-load dependent. In future, this assumption will be tested to confi rm whether the dissociations between speech perception and speech production under either clinical or developmental conditions are task-load related. Speech Perception under “Cocktail Party” Conditions Speech perception is not just for hearing speech sounds, but more essentially, for recognizing and unde</context>
</contexts>
<marker>[49]</marker>
<rawString>Fridriksson J, Moss J, Davis B, Baylis GC, Bonilha L, Rorden C. Motor speech perception modulates the cortical language areas. Neuroimage 2008, 41: 605–613.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cherry EC</author>
</authors>
<title>Some experiments on the recognition of speech, with one and with two ears.</title>
<date>1953</date>
<journal>J Acoust Soc Am</journal>
<pages>25--975</pages>
<contexts>
<context citStr="[50]" endWordPosition="2502" position="17484" startWordPosition="2502">more essentially, for recognizing and understanding speech signals, requiring that multisensory modalities interact. In fact, speech understanding and speech hearing do not share the same brain network, including the motor areas[40]. In a noisy environment (like a cocktail party), although there are many acoustic sources from various directions, listeners are still able to identify and follow target speech sounds in this high perceptual-load situation. How can listeners separate various speakers’ signals and understand target sentences? Although this “cocktail party” problem advanced by Cherry[50] has not been fully solved, several lines of evidence suggest that the motor system plays a role in solving this problem when the perceptual load is high. First, observing a speaker’s articulator movements can induce better understanding of speech. Listeners perceive speech in noise-masking or speech-masking environments more accurately when they can see speaker’s articulatory organ movement than when they cannot[7,9]. In addition, signals from the motor system help a listener to track a speaker talking over time[51,52]. It has been suggested that one of the functions of motor activation is tr</context>
</contexts>
<marker>[50]</marker>
<rawString>Cherry EC. Some experiments on the recognition of speech, with one and with two ears. J Acoust Soc Am 1953, 25: 975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>McFarland DH</author>
</authors>
<title>Respiratory markers of conversational interaction.</title>
<date>2001</date>
<journal>J Speech Lang Hear Res</journal>
<pages>44--128</pages>
<contexts>
<context citStr="[51,52]" endWordPosition="2580" position="18009" startWordPosition="2580">nd target sentences? Although this “cocktail party” problem advanced by Cherry[50] has not been fully solved, several lines of evidence suggest that the motor system plays a role in solving this problem when the perceptual load is high. First, observing a speaker’s articulator movements can induce better understanding of speech. Listeners perceive speech in noise-masking or speech-masking environments more accurately when they can see speaker’s articulatory organ movement than when they cannot[7,9]. In addition, signals from the motor system help a listener to track a speaker talking over time[51,52]. It has been suggested that one of the functions of motor activation is tracking the talker’s speed and rhythm over time, and provides the timing signals to the auditory cortex. Particularly in a conversation, the monitoring role of the motor system in interacting with the auditory system over time can induce fl uent conversation[53]. Under “cocktail-party” conditions, listeners are able to take advantage of certain perceptual cues to facilitate their selective attention to target speech. Selective attention allocates more cognitive resources to the motor representation of speech so that a li</context>
</contexts>
<marker>[51]</marker>
<rawString>McFarland DH. Respiratory markers of conversational interaction. J Speech Lang Hear Res 2001, 44: 128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pickering MJ</author>
<author>S Garrod</author>
</authors>
<title>Do people use language production to make predictions during comprehension? Trends Cogn Sci</title>
<date>2007</date>
<volume>11</volume>
<pages>105--110</pages>
<contexts>
<context citStr="[51,52]" endWordPosition="2580" position="18009" startWordPosition="2580">nd target sentences? Although this “cocktail party” problem advanced by Cherry[50] has not been fully solved, several lines of evidence suggest that the motor system plays a role in solving this problem when the perceptual load is high. First, observing a speaker’s articulator movements can induce better understanding of speech. Listeners perceive speech in noise-masking or speech-masking environments more accurately when they can see speaker’s articulatory organ movement than when they cannot[7,9]. In addition, signals from the motor system help a listener to track a speaker talking over time[51,52]. It has been suggested that one of the functions of motor activation is tracking the talker’s speed and rhythm over time, and provides the timing signals to the auditory cortex. Particularly in a conversation, the monitoring role of the motor system in interacting with the auditory system over time can induce fl uent conversation[53]. Under “cocktail-party” conditions, listeners are able to take advantage of certain perceptual cues to facilitate their selective attention to target speech. Selective attention allocates more cognitive resources to the motor representation of speech so that a li</context>
</contexts>
<marker>[52]</marker>
<rawString>Pickering MJ, Garrod S. Do people use language production to make predictions during comprehension? Trends Cogn Sci 2007, 11: 105–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott SK</author>
<author>C McGettigan</author>
<author>F Eisner</author>
</authors>
<title>A little more conversation, a little less action-candidate roles for the motor cortex in speech perception. Nat Rev Neurosci</title>
<date>2009</date>
<volume>10</volume>
<pages>295--302</pages>
<contexts>
<context citStr="[53]" endWordPosition="2634" position="18345" startWordPosition="2634">s perceive speech in noise-masking or speech-masking environments more accurately when they can see speaker’s articulatory organ movement than when they cannot[7,9]. In addition, signals from the motor system help a listener to track a speaker talking over time[51,52]. It has been suggested that one of the functions of motor activation is tracking the talker’s speed and rhythm over time, and provides the timing signals to the auditory cortex. Particularly in a conversation, the monitoring role of the motor system in interacting with the auditory system over time can induce fl uent conversation[53]. Under “cocktail-party” conditions, listeners are able to take advantage of certain perceptual cues to facilitate their selective attention to target speech. Selective attention allocates more cognitive resources to the motor representation of speech so that a listener can capture a speaker’s intention and improve speech recognition. Under noise-masking conditions, selective attention affects both active and passive listening. As Alho et al. have reported, attention modulates the magnitude of activation of the left premotor cortex, which infl uences the performance of phonetic categorization[</context>
</contexts>
<marker>[53]</marker>
<rawString>Scott SK, McGettigan C, Eisner F. A little more conversation, a little less action-candidate roles for the motor cortex in speech perception. Nat Rev Neurosci 2009, 10: 295–302.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C Wu</author>
<author>S Cao</author>
<author>F Zhou</author>
<author>C Wang</author>
<author>X Wu</author>
<author>L Li</author>
</authors>
<title>Masking of speech in people with fi rst-episode schizophrenia and people with chronic schizophrenia.</title>
<booktitle>Schizophr Res 2012a,</booktitle>
<pages>134--33</pages>
<contexts>
<context citStr="[54]" endWordPosition="2758" position="19280" startWordPosition="2758">tion. Under noise-masking conditions, selective attention affects both active and passive listening. As Alho et al. have reported, attention modulates the magnitude of activation of the left premotor cortex, which infl uences the performance of phonetic categorization[10]. In patients with schizophrenia, both speech-perception deficits and increased vulnerability to masking stimuli generally occur. More specifically, speech recognition in both fi rst-episode and chronic patients with schizophrenia is more vulnerable to masking stimuli, particularly speechmasking stimuli, than in healthy people[54]. Thus, whether functional impairments of motor cortical regions contribute to the enhanced vulnerability to speech-masking stimuli in patients with schizophrenia will be an important research issue in the future. Zhe-Meng Wu, et al. Interaction between auditory and motor systems in speech perception 495 Conclusion This review summarizes the studies showing that interactions between the auditory system and the motor system are related to speech perception. The anatomical and functional connections between the auditory and motor systems are important for improving speech recognition, particular</context>
</contexts>
<marker>[54]</marker>
<rawString>Wu C, Cao S, Zhou F, Wang C, Wu X, Li L. Masking of speech in people with fi rst-episode schizophrenia and people with chronic schizophrenia. Schizophr Res 2012a, 134: 33– 41.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms></paper>