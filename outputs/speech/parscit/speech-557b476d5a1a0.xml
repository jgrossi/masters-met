<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<author confidence="0.501932">
Neurosci Bull
</author>
<page confidence="0.97196">
490
</page>
<bodyText confidence="0.3117272">
June 1, 2014, 30(3): 490–496. http://www.neurosci.cn
DOI: 10.1007/s12264-013-1428-6
·Review·
Interaction between auditory and motor systems in speech
perception
</bodyText>
<author confidence="0.965967">
Zhe-Meng Wu, Ming-Li Chen, Xi-Hong Wu, Liang Li
</author>
<affiliation confidence="0.979577">
Department of Psychology, Speech and Hearing Research Center, Key Laboratory of Machine Perception (Ministry of
</affiliation>
<title confidence="0.407945">
Education), PKU-IDG/McGovern Institute for Brain Research, Peking University, Beijing 100871, China
</title>
<author confidence="0.280993">
Corresponding author: Liang Li. E-mail: liangli@pku.edu.cn
</author>
<affiliation confidence="0.250176">
© Shanghai Institutes for Biological Sciences, CAS and Springer-Verlag Berlin Heidelberg 2014
</affiliation>
<bodyText confidence="0.992580269230769">
Based on the Motor Theory of speech perception, the interaction between the auditory and motor systems
plays an essential role in speech perception. Since the Motor Theory was proposed, it has received remarkable
attention in the field. However, each of the three hypotheses of the theory still needs further verification. In
this review, we focus on how the auditory-motor anatomical and functional associations play a role in speech
perception and discuss why previous studies could not reach an agreement and particularly whether the motor
system involvement in speech perception is task-load dependent. Finally, we suggest that the function of the
auditory-motor link is particularly useful for speech perception under adverse listening conditions and the
further revised Motor Theory is a potential solution to the “cocktail-party” problem.
Keywords: auditory-motor interaction; Motor Theory of speech perception; motor cortex; “cocktail-party”
problem.
Introduction
in a reading-machine study. In their experiments, although
How listeners process the acoustic signals of speech is a
hot question. Traditionally, studies of this question have
mainly focused on the functions of the auditory system.
However, speech processing is not the pure and simple
analysis of speech sound signals, but a quite complicated
integrated process involving multisensory modalities and
even the motor system. In this review, we focus on the
interaction between the auditory system and the motor
system in speech perception and emphasize that the motor
processing component plays an essential role: activation
of the perceptual-motor loop enables listeners to both track
the speaker over time and form the intention to speak,
especially under adverse listening conditions, such as a
noisy and reverberating environment.
The Motor Theory of Speech Perception
The Motor Theory of speech perception was first proposed
by Liberman and colleagues
[1,2]
after an unexpected failure
blind people could recognize independent linguistic
units, they could not perceive alphabetic sequences and
understand synthesized speech, because the linguistic units
they perceived tended to merge into a blur[3]. This problem
related to speech perception is called coarticulation, in
which speech acoustic signals are highly context-sensitive;
a single phoneme can be influenced by its surrounding
phonemes[2]. However, normal listeners are able to conquer
coarticulation and perceive the original phonemes well[4].
Based on the results, Liberman and colleagues assumed
that what we really perceive when hearing speech signals
is not only sound waves, but also body “gestures” that
reflect the speaker’s intention. Liberman proposed three
hypotheses in both a weak[1] and a strong version of the
Motor Theory[5]. (1) The object of speech perception is the
“gesture”; (2) speech processing is special and requires a
specific phonetic module; and (3) activation of the motor
cortex is involved in speech perception.
When Liberman advanced the Motor Theory, he asked
Zhe-Meng Wu, et al.
Interaction between auditory and motor systems in speech perception
</bodyText>
<page confidence="0.996541">
491
</page>
<bodyText confidence="0.986658221052632">
a critical question, “when articulation and sound wave go
clear whether the enhanced activity of the cortical areas is
their separate ways, which way does perception go?”.
specific to speech perceptual performance or just reflects
The answer he provided was that perception goes with
an increase in general processing load.
articulation[1]. In more detail, the theory suggests that the
speech sound wave that we perceive carries the speaker’s
Anatomical and Functional Associations between
information indirectly, and there is a direct way to transmit
the Auditory and Motor Systems
information, that is through the “gesture” bearing the
speaker’s intention. In other words, perceiving the “gesture”
To confirm the involvement of the motor system in speech
is just perceiving the actual movement of the speaker’s
perception, evidence of both anatomical and functional
vocal tract, including motion of the larynx, tongue, and
links between the motor and auditory systems is needed.
[1]
lips . Our understanding of this theory is that although
Indeed, some models emphasize the auditory-motor
the listener may not be aware of tracking movement of
link in speech perception. For example, the dual-stream
the vocal tract, he/she automatically uses this motor cue
processing model suggests that there are two pathways
to recognize the speaker’s intention, just as in imitative
in audition: one is the ventral pathway down to the
behavior. When a speaker talks, the listener tries to follow
temporal lobe regulating “what” in acoustic information,
his speech style in mind and make a prediction before the
and the other is the dorsal pathway from primary sensory
speaker says the next word. Thus, the speaker and the
areas up to the posterior cortex regulating “how” speech
listener must converge on the same “linguistic currency”,
production takes place[12,13]. It is also known that the ventral
the “gesture”, to communicate.
pathway is involved in analyzing phonetic characters,
It is well known that “gesture” information can affect
acoustic features, and speech intelligibility[14,15], and the
speech perception in different ways, such as the McGurk
dorsal pathway is associated with sensorimotor mapping
effect. When the listener sees the speaker producing the
between auditory and motor representations[16,17], speech
syllable (/ga/) while listening to another syllable
production[14,18], and silent articulatory organ movement[19].
(/ba/), the mouth movement may mislead the listener into
Although this dual-stream model proposes that each of the
[6]
hearing a different syllable (/da/) . This visuomotor cue
pathways plays a specific role in speech perception, how
strongly influences what we actually hear. Also, another
the streams interact with each other is still not clear.
study focused on the role of articulatory organ movement
The other model, the forward-inverse model, proposes
in a noisy environment[7]. Listeners perceive speech more
that the motor cortical regions predict the consequences
accurately when they can see the speaker’s articulatory
of motor commands and revise the signals with the
organ movement than when they cannot. Also, under
changing environment[20,21]. In more detail, before motor
adverse listening conditions, lip-reading associated with
commands reach the effectors, the forward-inverse model
the target sentence can act as a cue to improve listener
produces predicted sensory consequences of the motor
[8,9]
. Thus, perceiving “gesture”
commands, and then compares the predicted results with
signals provides visuomotor cues, which help listeners take
the real sensory information. This comparison provides
advantage of the speaker’s motor actions during speech in
more information for the central system to produce a
an adverse noisy environment and facilitate the perceptual
more appropriate performance. With time delays and
performance. In other words, listeners actively, rather than
interruptions from the surroundings, the motor commands
passively, receive speech information. Supporting this
need to be up-dated from time to time in order to produce
recognition of speech
view, Alho et al. reported that stronger activation in the
the desired outcome. Thus, when a speech signal is
left premotor cortex is associated with better identification
distorted by environmental noise and/or time delays, the
of syllables that are embedded in noise, and the cortical
motor representation of the previous signals modifies the
activation is quite different between active and passive
current auditory representation through inverse mapping.
[10]
. Also, Calla et al. reported stronger activity in
Due to the role of motor representation in revising distorted
correct trials over incorrect trials within both the ventral
signals, listeners can recognize the speaker’s intention and
listening
[11]
premotor cortex and Broca’s area . However, it is still not
predict the outcome of motor commands before making
</bodyText>
<page confidence="0.995704">
492
</page>
<bodyText confidence="0.972558270270271">
Neurosci Bull
June 1, 2014, 30(3): 490–496
responses. In other words, anticipation of a motor signal
increased activation associations between the auditory
can be combined with both signal characteristics and
cortices and the left ventral premotor cortex[22]. Moreover,
the speaker’s articulatory information, producing a more
compared to listening to pseudo-words and reversed-
desirable response. Based on the basic principles of the
words, listening to normal words induces broad activation
dual-stream processing and forward-inverse models, we
connectivity in the auditory-motor network, which may
propose that further-developed models should emphasize
be useful for facilitating semantic processing[23]. Further
how the auditory-motor interaction is modulated by both
investigation is needed to verify whether this enhanced
processing load (due to complex inputs) and prediction/
dynamic auditory-motor network promotes the transition
estimation (due to task goals and feedback) (Fig. 1).
from a sound stream into a series of meaningful motor-
So far, co-activations between auditory and motor
based units and results in speech comprehension.
regions in speech perception have been clearly demonstrated.
In addition to the well-known fact that speech
When exposed to novel speech distortions, such as time-
production is tightly related to the motor cortex, some
compressed sentences, listeners can rapidly distinguish
studies have shown that the motor cortex is activated in
distorted sentences from normal-speed sentences, with
speech perception tasks[24-30]. For example, when listeners
Fig. 1. Modified Auditory-Motor Interaction model based on the Dual-stream Processing model in combination with the Forward-inverse
model. This model emphasizes that both processing load and prediction/estimation affect the auditory-motor interaction (white
two-headed arrow). Red: the auditory processing system including the ventral stream pathway; blue: the motor processing
system including the dorsal stream pathway; green: the systems that mediate the processing load, prediction, and estimation that
modulate the auditory-motor interaction (arrows with dashed lines).
Zhe-Meng Wu, et al.
Interaction between auditory and motor systems in speech perception
</bodyText>
<page confidence="0.997782">
493
</page>
<bodyText confidence="0.975881368421053">
hear a lip-related phoneme [p] or a tongue-related phoneme
[t], motor regions are activated differentially[26,27], suggesting
that different speech stimuli activate motor cortical regions
with different patterns. In other words, listening to various
verbal stimuli may cause differential automatic activations
of cortical regions involved in speech production. Also, it
has been suggested that the activation of the motor cortex
may reflect a mediating role of the motor cortex in speech
perception[28-30].
Moreover, studies using either functional magnetic
resonance imaging (fMRI) or transcranial magnetic
stimulation (TMS) have confirmed the role of the motor
cortex in speech perception. For example, in a speech
perception task, strong activation in the motor cortex can
be induced only when participants perceive the target
speech[24,31]. Some studies using TMS of the motor cortex
have demonstrated that stimulation of speech-related
regions affects speech perception[32-34]. For example, using
TMS to suppress the left premotor cortex, which is activated
both during speech production and speech perception (Fig.
Fig. 2. Representative fMRI activation in the premotor cortex
2), Meister et al. found that participants with a suppressed
(PMC) associated with discriminating voiceless stop
premotor cortex were impaired in discriminating voiceless
consonants in single syllables masked by white noise
stop consonants under white-noise masking conditions.
in two representative participants. Regions selected
for stimulation are shown in bright colors. Arrowheads
Thus, they suggested that the premotor cortex is essentially
indicate the location of the central sulcus (adapted from
involved in speech perception[33].
Meister et al. [33] with permission).
However, the results of some clinical studies appear
not to support the view that there is an association between
impairment of speech perception and impairment of
production. Children born with hearing loss can learn
speech production. For example, patients with expressive
to speak if they gain enough positive somatosensory
aphasia exhibit impairment of speech production but not
feedback, even though the learning process is much
. Also, although
harder than in healthy children[39]. Also, children with severe
patients with receptive aphasia exhibit impairment of
dysarthria are unable to produce meaningful sentences,
speech perception and comprehension, they can speak
but they can accurately understand spoken content [40].
speech perception or comprehension
[36,37]
[35]
. The dissociations in expressive and receptive
Furthermore, infants usually learn to understand speech
aphasia support another view that speech perception
first and then begin to learn how to produce their own
and production are two distinct processes. Moreover,
words [41,42] . These studies indicate that dissociations
patients with lesions in Broca’s area perform well in both
between speech perception and speech production
word-comprehension and syllable-identification tests, but
occur during development. As speech perception and
patients with temporal lobule damage perform poorly in
speech production do not appear at the same time during
these tests[38]. These studies also negate the role of motor
development, motor cortical areas may not be as important
regions in speech perception, but support the view that
for speech perception as the Motor Theory proposes.
fluently
temporal regions rather than motor areas are important in
speech perception
[16,17]
.
As reviewed above, some fMRI and TMS studies
support the view that motor cortical areas are important
Research in child development has also shown
in speech perception, while clinical and developmental
dissociations between speech perception and speech
studies have shown significant dissociations between
</bodyText>
<page confidence="0.997285">
494
</page>
<table confidence="0.374469">
Neurosci Bull
June 1, 2014, 30(3): 490–496
</table>
<bodyText confidence="0.975752888888889">
speech perception and speech production. Also note that
In a noisy environment (like a cocktail party), although
some fMRI studies did not reveal an increased activation
there are many acoustic sources from various directions,
in motor cortical regions during speech perception and
listeners are still able to identify and follow target speech
comprehension[19,43,44].
sounds in this high perceptual-load situation. How
The different results in the studies described above
can listeners separate various speakers’ signals and
may be due to different task demands. The Hickok
understand target sentences? Although this “cocktail party”
and Poeppel study showed that when the task load is
problem advanced by Cherry[50] has not been fully solved,
high, requiring both speech identification and speech
several lines of evidence suggest that the motor system
categorization, the activated frontal region extends to the
plays a role in solving this problem when the perceptual
premotor cortex[17]. Also, when speech signals are distorted,
load is high.
the motor cortex is markedly activated[45,46]. Interestingly,
First, observing a speaker’s articulator movements can
non-verbal signals can activate motor areas [31,47], and
induce better understanding of speech. Listeners perceive
there is no difference in activation magnitude in the motor
speech in noise-masking or speech-masking environments
cortex between perceiving speech and perceiving non-
more accurately when they can see speaker’s articulatory
verbal sounds[31]. Some studies have further shown that
blurred speech causes even stronger activation in the
bilateral premotor cortex, compared to clear speech[45] and
perceiving a foreign language causes larger activation in
the motor cortex than the native language[25]. Also, lowfrequency words induce higher activation in the motor
cortex than high-frequency words[48]. Thus, facing unfamiliar
stimuli (such as distorted speech, a foreign language, or
low-frequency words), the motor cortex may play a role
in facilitating the association with the auditory system to
improve speech perception. More interestingly, in a mixed
visual and auditory task, weaker visual stimuli evoke
stronger activation in the motor cortex than clear pictures
of speakers[49], suggesting that a heavy-load task, such
as analyzing distorted auditory or visual signals, requires
involvement of the motor cortex. These studies suggest that
whether the motor system is involved in speech perception
is task-load dependent. In future, this assumption will be
tested to confirm whether the dissociations between speech
perception and speech production under either clinical or
developmental conditions are task-load related.
organ movement than when they cannot[7,9]. In addition,
signals from the motor system help a listener to track
a speaker talking over time[51,52]. It has been suggested
that one of the functions of motor activation is tracking
the talker’s speed and rhythm over time, and provides
the timing signals to the auditory cortex. Particularly in a
conversation, the monitoring role of the motor system in
interacting with the auditory system over time can induce
fluent conversation[53].
Under “cocktail-party” conditions, listeners are able
to take advantage of certain perceptual cues to facilitate
their selective attention to target speech. Selective
attention allocates more cognitive resources to the motor
representation of speech so that a listener can capture a
speaker’s intention and improve speech recognition. Under
noise-masking conditions, selective attention affects both
active and passive listening. As Alho et al. have reported,
attention modulates the magnitude of activation of the
left premotor cortex, which influences the performance of
phonetic categorization[10].
In patients with schizophrenia, both speech-perception
Speech Perception under “Cocktail Party”
Conditions
deficits and increased vulnerability to masking stimuli
generally occur. More specifically, speech recognition in
both first-episode and chronic patients with schizophrenia
Speech perception is not just for hearing speech sounds,
is more vulnerable to masking stimuli, particularly speech-
but more essentially, for recognizing and understanding
masking stimuli, than in healthy people[54]. Thus, whether
speech signals, requiring that multisensory modalities
functional impairments of motor cortical regions contribute
interact. In fact, speech understanding and speech hearing
to the enhanced vulnerability to speech-masking stimuli in
do not share the same brain network, including the motor
patients with schizophrenia will be an important research
[40]
areas .
issue in the future.
Zhe-Meng Wu, et al.
Interaction between auditory and motor systems in speech perception
Conclusion
</bodyText>
<page confidence="0.994907">
495
</page>
<bodyText confidence="0.984267714285714">
[10] Alho J, Sato M, Sams M, Schwartz JL, Tiitinen H,
Jääskeläinen IP. Enhanced early-latency electromagnetic
This review summarizes the studies showing that
activity in the left premotor cortex is associated with
interactions between the auditory system and the motor
successful phonetic categorization. Neuroimage 2012, 60:
system are related to speech perception. The anatomical
and functional connections between the auditory and motor
systems are important for improving speech recognition,
1937–1946.
[11] Callan D, Callan A, Gamez M, Sato MA, Kawato M. Premotor
cortex mediates perceptual performance. Neuroimage 2010,
51: 844–858.
particularly under difficult listening conditions (such as the
[12] Schreiner CE, Winer JA. Auditory cortex mapmaking:
cocktail-party environment). With the involvement of the
principles, projections, and plasticity. Neuron 2007, 56: 356–
motor system, the listener can better identify the speaker’s
intention and follow the target stream. Thus, investigation
of the auditory-motor association in speech perception is
important for solving the “cocktail party” problem.
</bodyText>
<sectionHeader confidence="0.984426" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.953623">
This review was supported by the National Basic Research
</bodyText>
<subsectionHeader confidence="0.955391">
Development Program of China (2009CB320901, 2011CB707805,
</subsectionHeader>
<reference confidence="0.77935485">
2013CB329304), the National Natural Science Foundation of
China (31170985, 91120001, 61121002), and “985” project grants
from Peking University.
Received date: 2013-05-06; Accepted date: 2013-08-20
365.
[13] Recanzone GH, Sutter ML. The biological basis of audition.
Annu Rev Psychol 2008, 59: 119–142.
[14] Belin P, Zatorre RJ. Adaptation to speaker’s voice in right
anterior temporal lobe. Neuroreport 2003, 14: 2105–2109.
[15] Scott SK, Blank CC, Rosen S, Wise RJ. Identification of a
pathway for intelligible speech in the left temporal lobe. Brain
2000, 123: 2400–2406.
[16] Hickok G, Poeppel D. Towards a functional neuroanatomy of
speech perception. Trends Cogn Sci 2000, 4: 131–138.
[17] Hickok G, Poeppel D. The cortical organization of speech
processing. Nat Neurosci 2007, 8: 393–402.
[18] Baddeley A, Lewis V, Vallar G. Exploring the articulatory loop.
Q J Exp Psychol 1984, 36: 233–252.
[19] Wise RJ, Scott SK, Blank SC, Mummery CJ, Murphy
K, Warburton EA. Separate neural subsystems within
</reference>
<sectionHeader confidence="0.69255" genericHeader="references">
REFERENCES
</sectionHeader>
<bodyText confidence="0.296836833333333">
Wernicke’s area. Brain 2001, 124: 83–95.
[20] Andersen RA, Buneo CA. Intentional maps in posterior
[1]
Liberman AM, Delattre P, Cooper FS. The role of selected
parietal cortex. Annu Rev Neurosci 2002, 25: 189–220.
stimulus-variables in the perception of the unvoiced stop
</bodyText>
<figure confidence="0.777837454545455">
[21] Wolpert DM, Doya K, Kawato M. A unifying computational
consonants. Am J Psychol 1952, 65: 497–516.
[2]
Liberman AM, Cooper FS, Shankweiler DP, Studdert KM.
Perception of the speech code. Psychol Rev 1967, 74: 431–
461.
[3]
Liberman AM. Speech: A Special Code. Cambridge, MA: The
MIT Press 1996.
[4]
[5]
</figure>
<reference confidence="0.781380842105263">
comprehension: Adapting to time-compressed speech.
Neuroimage 2010, 49: 1124.
[23] Londei A, D’Ausilio A, Basso D, Sestieri C, Gratta CD,
Romani GL, et al. Sensory-motor brain network connectivity
production models. J Phon 1977, 5: 115–133.
for speech comprehension. Hum Brain Mapp 2010, 31: 567–
Liberman AM, Mattingly IG. The motor theory of speech
[6]
McGurk H, MacDonald J. Hearing lips and seeing voices.
[7]
Sumby WH, Pollack I. Visual contribution to speech
Nature 1976, 264: 746–748.
580.
[24] Wilson SM. Listening to speech activates motor areas
involved in speech production. Nat Neurosci 2004, 7: 701–
702.
[25] Wilson SM, Iacoboni M. Neural responses to non-native
intelligibility in noise. J Acoust Soc Am 1954, 26: 212.
phonemes varying in producibility: evidence for the
Rudmann DS, McCarley JS, Kramer AF. Bimodal displays
sensorimotor nature of speech perception. Neuroimage
improve speech comprehension in environments with
multiple speakers. Hum Fac Erg Soc P 2003, 45: 329–336.
[9]
Trans R Soc Lond B Biol Sci 2003, 358: 593–602.
[22] Adank P, Devlin JT. On-line plasticity in spoken sentence
Kent RD, Minifie FD. Coarticulation in recent speech
perception revised. Cognition 1985, 21: 1–36.
[8]
framework for motor control and social interaction. Philos
2006, 33: 316–325.
[26] Fadiga L, Craighero L, Buccino G, Rizzolatti G. Speech
Wu C, Cao S, Wu X, Li L. Temporally pre-presented
listening specifically modulates the excitability of tongue
lipreading cues release speech from informational masking. J
muscles: a TMS study. Eur J Neurosci 2002, 15: 399–402.
Acoust Soc Am 2013, 133: 281–285.
[27] Pulvermüller F, Huss M, Kherif F, del Prado Martin FM, Hauk
</reference>
<page confidence="0.990363">
496
</page>
<reference confidence="0.986646863636363">
Neurosci Bull
O, Shtyrov Y. Motor cortex maps articulatory features of
speech sounds. Proc Natl Acad Sci U S A 2006, 103: 7865–
7870.
June 1, 2014, 30(3): 490–496
1790–1804.
[41] Werker JF, Yeung HH. Infant speech perception bootstraps
word learning. Trends Cogn Sci 2005, 9: 519–527.
[28] Bever TG, Poeppel D. Analysis by synthesis: a (re-) emerging
[42] Tsao FM, Liu HM, Kuhl PK. Speech perception in infancy
program of research for language and vision. Biolinguistics
predicts language development in the second year of life: a
2010, 4: 174–200.
[29] Callan DE, Jones JA, Callan AM, Akahane-Yamada R.
Phonetic perceptual identification by native-and second-
longitudinal study. Child Dev 2004, 75: 1067–1084.
[43] Basso A, Casati G, Vignolo LA. Phonemic identification
defect in aphasia. Cortex 1977, 13: 85.
language speakers differentially activates brain regions
[44] Scott SK, Rosen S, Lang H, Wise RJ. Neural correlates
involved with acoustic phonetic processing and those
of intelligibility in speech investigated with noise vocoded
involved with articulatory-auditory/orosensory internal
speech-a positron emission tomography study. J Acoust Soc
models. Neuroimage 2004, 22: 1182–1194.
[30] Hickok G, Houde J, Rong F. Sensorimotor integration
in speech processing: computational basis and neural
organization. Neuron 2011, 69: 407–422.
Am 2006, 120: 1075.
[45] Davis MH, Johnsrude IS. Hierarchical processing in spoken
language comprehension. J Neurosci 2003, 23: 3423–3431.
[46] Uppenkamp S, Johnsrude IS, Norris D, Marslen-Wilson W,
[31] Watkins KE, Strafella AP, Paus T. Seeing and hearing speech
Patterson RD. Locating the initial stages of speech? Sound
excites the motor system involved in speech production.
processing in human temporal cortex. Neuroimage 2006, 31:
Neuropsychologia 2003, 41: 989–994.
1284–1296.
[32] D’Ausilio A, Pulvermüller F, Salmas P, Bufalari I, Begliomini C,
[47] Warren JE, Sauter DA, Eisner F, Wiland J, Dresner M,
Fadiga L. The motor somatotopy of speech perception. Curr
Wise RJ, et al. Positive emotions preferentially engage an
Biol 2009, 19: 381–385.
auditory–motor “mirror” system. J Neurosci 2006, 26: 13067–
[33] Meister IG, Wilson SM, Deblieck C, Wu AD, Iacoboni M. The
essential role of premotor cortex in speech perception. Curr
Biol 2007, 17: 1692–1696.
[34] Watkins K, Paus T. Modulation of motor excitability during
speech perception: the role of Broca’s area. J Cogn Neurosci
2004, 16: 978–987.
[35] Mohr JP, Pessin MS, Finkelstein S, Funkenstein HH, Duncan
GW, Davis KR. Broca aphasia Pathologic and clinical.
Neurology 1978, 28: 311–311.
[36] Crinion JT, Warburton EA, Lambon-Ralph MA, Howard D,
Wise RJ. Listening to narrative speech after aphasic stroke:
the role of the left anterior temporal lobe. Cereb Cortex 2006,
16: 1116–1125.
[37] Bogen JE, Bogen GM. Wernick&amp;apos;s region-Where is it? Ann NY
Acad Sci, 1976, 280: 834–843.
13075.
[48] R o y A C , C r a i g h e r o L , F a b b r i - D e s t r o M , F a d i g a L .
Phonological and lexical motor facilitation during speech
listening: A transcranial magnetic stimulation study. J Physiol
Paris 2008, 102: 101–105.
[49] Fridriksson J, Moss J, Davis B, Baylis GC, Bonilha L, Rorden
C. Motor speech perception modulates the cortical language
areas. Neuroimage 2008, 41: 605–613.
[50] Cherry EC. Some experiments on the recognition of speech,
with one and with two ears. J Acoust Soc Am 1953, 25: 975.
[51] McFarland DH. Respiratory markers of conversational
interaction. J Speech Lang Hear Res 2001, 44: 128.
[52] Pickering MJ, Garrod S. Do people use language production
to make predictions during comprehension? Trends Cogn Sci
2007, 11: 105–110.
[38] B a k e r E , B l u m s t e i n S E , G o o d g l a s s H . I n t e r a c t i o n
[53] Scott SK, McGettigan C, Eisner F. A little more conversation,
between phonological and semantic factors in auditory
a little less action-candidate roles for the motor cortex in
comprehension. Neuropsychologia 1981, 19: 1–15.
[39] Bishop D, Mogford-Bevan K. Language Development in
Exceptional Circumstances. Psychology Press 1993.
[40] Bishop CW, Miller LM. A multisensory cortical network for
understanding speech in noise. J Cogn Neurosci 2009, 21:
speech perception. Nat Rev Neurosci 2009, 10: 295–302.
[54] Wu C, Cao S, Zhou F, Wang C, Wu X, Li L. Masking of
speech in people with first-episode schizophrenia and people
with chronic schizophrenia. Schizophr Res 2012a, 134: 33–
41.
</reference>
</variant>
</algorithm>

<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>2013CB329304), the National Natural Science Foundation of China (31170985, 91120001, 61121002), and “985” project grants from Peking University. Received date: 2013-05-06; Accepted date:</title>
<pages>2013--08</pages>
<marker></marker>
<rawString> 2013CB329304), the National Natural Science Foundation of China (31170985, 91120001, 61121002), and “985” project grants from Peking University. Received date: 2013-05-06; Accepted date: 2013-08-20 365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Recanzone GH</author>
<author>Sutter ML</author>
</authors>
<title>The biological basis of audition. Annu Rev Psychol</title>
<date>2008</date>
<volume>59</volume>
<pages>119--142</pages>
<contexts>
<context position="5567" citStr="[12,13]" startWordPosition="818" endWordPosition="818"> motor cue processing model suggests that there are two pathways to recognize the speaker’s intention, just as in imitative in audition: one is the ventral pathway down to the behavior. When a speaker talks, the listener tries to follow temporal lobe regulating “what” in acoustic information, his speech style in mind and make a prediction before the and the other is the dorsal pathway from primary sensory speaker says the next word. Thus, the speaker and the areas up to the posterior cortex regulating “how” speech listener must converge on the same “linguistic currency”, production takes place[12,13]. It is also known that the ventral the “gesture”, to communicate. pathway is involved in analyzing phonetic characters, It is well known that “gesture” information can affect acoustic features, and speech intelligibility[14,15], and the speech perception in different ways, such as the McGurk dorsal pathway is associated with sensorimotor mapping effect. When the listener sees the speaker producing the between auditory and motor representations[16,17], speech syllable (/ga/) while listening to another syllable production[14,18], and silent articulatory organ movement[19]. (/ba/), the mouth mov</context>
</contexts>
<marker>[13]</marker>
<rawString>Recanzone GH, Sutter ML. The biological basis of audition. Annu Rev Psychol 2008, 59: 119–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Belin</author>
<author>Zatorre RJ</author>
</authors>
<title>Adaptation to speaker’s voice in right anterior temporal lobe. Neuroreport</title>
<date>2003</date>
<volume>14</volume>
<pages>2105--2109</pages>
<contexts>
<context position="5795" citStr="[14,15]" startWordPosition="850" endWordPosition="850">o follow temporal lobe regulating “what” in acoustic information, his speech style in mind and make a prediction before the and the other is the dorsal pathway from primary sensory speaker says the next word. Thus, the speaker and the areas up to the posterior cortex regulating “how” speech listener must converge on the same “linguistic currency”, production takes place[12,13]. It is also known that the ventral the “gesture”, to communicate. pathway is involved in analyzing phonetic characters, It is well known that “gesture” information can affect acoustic features, and speech intelligibility[14,15], and the speech perception in different ways, such as the McGurk dorsal pathway is associated with sensorimotor mapping effect. When the listener sees the speaker producing the between auditory and motor representations[16,17], speech syllable (/ga/) while listening to another syllable production[14,18], and silent articulatory organ movement[19]. (/ba/), the mouth movement may mislead the listener into Although this dual-stream model proposes that each of the [6] hearing a different syllable (/da/) . This visuomotor cue pathways plays a specific role in speech perception, how strongly influe</context>
</contexts>
<marker>[14]</marker>
<rawString>Belin P, Zatorre RJ. Adaptation to speaker’s voice in right anterior temporal lobe. Neuroreport 2003, 14: 2105–2109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott SK</author>
<author>Blank CC</author>
<author>S Rosen</author>
<author>Wise RJ</author>
</authors>
<title>Identification of a pathway for intelligible speech in the left temporal lobe.</title>
<date>2000</date>
<pages>123--2400</pages>
<location>Brain</location>
<contexts>
<context position="5795" citStr="[14,15]" startWordPosition="850" endWordPosition="850">o follow temporal lobe regulating “what” in acoustic information, his speech style in mind and make a prediction before the and the other is the dorsal pathway from primary sensory speaker says the next word. Thus, the speaker and the areas up to the posterior cortex regulating “how” speech listener must converge on the same “linguistic currency”, production takes place[12,13]. It is also known that the ventral the “gesture”, to communicate. pathway is involved in analyzing phonetic characters, It is well known that “gesture” information can affect acoustic features, and speech intelligibility[14,15], and the speech perception in different ways, such as the McGurk dorsal pathway is associated with sensorimotor mapping effect. When the listener sees the speaker producing the between auditory and motor representations[16,17], speech syllable (/ga/) while listening to another syllable production[14,18], and silent articulatory organ movement[19]. (/ba/), the mouth movement may mislead the listener into Although this dual-stream model proposes that each of the [6] hearing a different syllable (/da/) . This visuomotor cue pathways plays a specific role in speech perception, how strongly influe</context>
</contexts>
<marker>[15]</marker>
<rawString>Scott SK, Blank CC, Rosen S, Wise RJ. Identification of a pathway for intelligible speech in the left temporal lobe. Brain 2000, 123: 2400–2406.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hickok</author>
<author>D Poeppel</author>
</authors>
<title>Towards a functional neuroanatomy of speech perception. Trends Cogn Sci</title>
<date>2000</date>
<volume>4</volume>
<pages>131--138</pages>
<contexts>
<context position="6022" citStr="[16,17]" startWordPosition="882" endWordPosition="882">and the areas up to the posterior cortex regulating “how” speech listener must converge on the same “linguistic currency”, production takes place[12,13]. It is also known that the ventral the “gesture”, to communicate. pathway is involved in analyzing phonetic characters, It is well known that “gesture” information can affect acoustic features, and speech intelligibility[14,15], and the speech perception in different ways, such as the McGurk dorsal pathway is associated with sensorimotor mapping effect. When the listener sees the speaker producing the between auditory and motor representations[16,17], speech syllable (/ga/) while listening to another syllable production[14,18], and silent articulatory organ movement[19]. (/ba/), the mouth movement may mislead the listener into Although this dual-stream model proposes that each of the [6] hearing a different syllable (/da/) . This visuomotor cue pathways plays a specific role in speech perception, how strongly influences what we actually hear. Also, another the streams interact with each other is still not clear. study focused on the role of articulatory organ movement The other model, the forward-inverse model, proposes in a noisy environ</context>
<context position="14461" citStr="[16,17]" startWordPosition="2078" endWordPosition="2078">erform well in both between speech perception and speech production word-comprehension and syllable-identification tests, but occur during development. As speech perception and patients with temporal lobule damage perform poorly in speech production do not appear at the same time during these tests[38]. These studies also negate the role of motor development, motor cortical areas may not be as important regions in speech perception, but support the view that for speech perception as the Motor Theory proposes. fluently temporal regions rather than motor areas are important in speech perception [16,17] . As reviewed above, some fMRI and TMS studies support the view that motor cortical areas are important Research in child development has also shown in speech perception, while clinical and developmental dissociations between speech perception and speech studies have shown significant dissociations between 494 Neurosci Bull June 1, 2014, 30(3): 490–496 speech perception and speech production. Also note that In a noisy environment (like a cocktail party), although some fMRI studies did not reveal an increased activation there are many acoustic sources from various directions, in motor cortical</context>
</contexts>
<marker>[16]</marker>
<rawString>Hickok G, Poeppel D. Towards a functional neuroanatomy of speech perception. Trends Cogn Sci 2000, 4: 131–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hickok</author>
<author>D Poeppel</author>
</authors>
<title>The cortical organization of speech processing.</title>
<date>2007</date>
<journal>Nat Neurosci</journal>
<volume>8</volume>
<pages>393--402</pages>
<contexts>
<context position="6022" citStr="[16,17]" startWordPosition="882" endWordPosition="882">and the areas up to the posterior cortex regulating “how” speech listener must converge on the same “linguistic currency”, production takes place[12,13]. It is also known that the ventral the “gesture”, to communicate. pathway is involved in analyzing phonetic characters, It is well known that “gesture” information can affect acoustic features, and speech intelligibility[14,15], and the speech perception in different ways, such as the McGurk dorsal pathway is associated with sensorimotor mapping effect. When the listener sees the speaker producing the between auditory and motor representations[16,17], speech syllable (/ga/) while listening to another syllable production[14,18], and silent articulatory organ movement[19]. (/ba/), the mouth movement may mislead the listener into Although this dual-stream model proposes that each of the [6] hearing a different syllable (/da/) . This visuomotor cue pathways plays a specific role in speech perception, how strongly influences what we actually hear. Also, another the streams interact with each other is still not clear. study focused on the role of articulatory organ movement The other model, the forward-inverse model, proposes in a noisy environ</context>
<context position="14461" citStr="[16,17]" startWordPosition="2078" endWordPosition="2078">erform well in both between speech perception and speech production word-comprehension and syllable-identification tests, but occur during development. As speech perception and patients with temporal lobule damage perform poorly in speech production do not appear at the same time during these tests[38]. These studies also negate the role of motor development, motor cortical areas may not be as important regions in speech perception, but support the view that for speech perception as the Motor Theory proposes. fluently temporal regions rather than motor areas are important in speech perception [16,17] . As reviewed above, some fMRI and TMS studies support the view that motor cortical areas are important Research in child development has also shown in speech perception, while clinical and developmental dissociations between speech perception and speech studies have shown significant dissociations between 494 Neurosci Bull June 1, 2014, 30(3): 490–496 speech perception and speech production. Also note that In a noisy environment (like a cocktail party), although some fMRI studies did not reveal an increased activation there are many acoustic sources from various directions, in motor cortical</context>
<context position="15808" citStr="[17]" startWordPosition="2276" endWordPosition="2276">h perceptual-load situation. How The different results in the studies described above can listeners separate various speakers’ signals and may be due to different task demands. The Hickok understand target sentences? Although this “cocktail party” and Poeppel study showed that when the task load is problem advanced by Cherry[50] has not been fully solved, high, requiring both speech identification and speech several lines of evidence suggest that the motor system categorization, the activated frontal region extends to the plays a role in solving this problem when the perceptual premotor cortex[17]. Also, when speech signals are distorted, load is high. the motor cortex is markedly activated[45,46]. Interestingly, First, observing a speaker’s articulator movements can non-verbal signals can activate motor areas [31,47], and induce better understanding of speech. Listeners perceive there is no difference in activation magnitude in the motor speech in noise-masking or speech-masking environments cortex between perceiving speech and perceiving nonmore accurately when they can see speaker’s articulatory verbal sounds[31]. Some studies have further shown that blurred speech causes even stron</context>
</contexts>
<marker>[17]</marker>
<rawString>Hickok G, Poeppel D. The cortical organization of speech processing. Nat Neurosci 2007, 8: 393–402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Baddeley</author>
<author>V Lewis</author>
<author>G Vallar</author>
</authors>
<title>Exploring the articulatory loop.</title>
<date>1984</date>
<journal>Q J Exp Psychol</journal>
<volume>36</volume>
<pages>233--252</pages>
<contexts>
<context position="6100" citStr="[14,18]" startWordPosition="891" endWordPosition="891"> converge on the same “linguistic currency”, production takes place[12,13]. It is also known that the ventral the “gesture”, to communicate. pathway is involved in analyzing phonetic characters, It is well known that “gesture” information can affect acoustic features, and speech intelligibility[14,15], and the speech perception in different ways, such as the McGurk dorsal pathway is associated with sensorimotor mapping effect. When the listener sees the speaker producing the between auditory and motor representations[16,17], speech syllable (/ga/) while listening to another syllable production[14,18], and silent articulatory organ movement[19]. (/ba/), the mouth movement may mislead the listener into Although this dual-stream model proposes that each of the [6] hearing a different syllable (/da/) . This visuomotor cue pathways plays a specific role in speech perception, how strongly influences what we actually hear. Also, another the streams interact with each other is still not clear. study focused on the role of articulatory organ movement The other model, the forward-inverse model, proposes in a noisy environment[7]. Listeners perceive speech more that the motor cortical regions predic</context>
</contexts>
<marker>[18]</marker>
<rawString>Baddeley A, Lewis V, Vallar G. Exploring the articulatory loop. Q J Exp Psychol 1984, 36: 233–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wise RJ</author>
<author>Scott SK</author>
<author>Blank SC</author>
<author>Mummery CJ</author>
<author>K Murphy</author>
<author>Warburton EA</author>
</authors>
<title>Separate neural subsystems within comprehension: Adapting to time-compressed speech. Neuroimage</title>
<date>2010</date>
<volume>49</volume>
<pages>1124</pages>
<contexts>
<context position="6144" citStr="[19]" startWordPosition="896" endWordPosition="896">oduction takes place[12,13]. It is also known that the ventral the “gesture”, to communicate. pathway is involved in analyzing phonetic characters, It is well known that “gesture” information can affect acoustic features, and speech intelligibility[14,15], and the speech perception in different ways, such as the McGurk dorsal pathway is associated with sensorimotor mapping effect. When the listener sees the speaker producing the between auditory and motor representations[16,17], speech syllable (/ga/) while listening to another syllable production[14,18], and silent articulatory organ movement[19]. (/ba/), the mouth movement may mislead the listener into Although this dual-stream model proposes that each of the [6] hearing a different syllable (/da/) . This visuomotor cue pathways plays a specific role in speech perception, how strongly influences what we actually hear. Also, another the streams interact with each other is still not clear. study focused on the role of articulatory organ movement The other model, the forward-inverse model, proposes in a noisy environment[7]. Listeners perceive speech more that the motor cortical regions predict the consequences accurately when they can </context>
<context position="15184" citStr="[19,43,44]" startWordPosition="2183" endWordPosition="2183"> in child development has also shown in speech perception, while clinical and developmental dissociations between speech perception and speech studies have shown significant dissociations between 494 Neurosci Bull June 1, 2014, 30(3): 490–496 speech perception and speech production. Also note that In a noisy environment (like a cocktail party), although some fMRI studies did not reveal an increased activation there are many acoustic sources from various directions, in motor cortical regions during speech perception and listeners are still able to identify and follow target speech comprehension[19,43,44]. sounds in this high perceptual-load situation. How The different results in the studies described above can listeners separate various speakers’ signals and may be due to different task demands. The Hickok understand target sentences? Although this “cocktail party” and Poeppel study showed that when the task load is problem advanced by Cherry[50] has not been fully solved, high, requiring both speech identification and speech several lines of evidence suggest that the motor system categorization, the activated frontal region extends to the plays a role in solving this problem when the percep</context>
</contexts>
<marker>[19]</marker>
<rawString>Wise RJ, Scott SK, Blank SC, Mummery CJ, Murphy K, Warburton EA. Separate neural subsystems within comprehension: Adapting to time-compressed speech. Neuroimage 2010, 49: 1124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Londei</author>
<author>A D’Ausilio</author>
<author>D Basso</author>
<author>C Sestieri</author>
<author>Gratta CD</author>
<author>Romani GL</author>
</authors>
<title>Sensory-motor brain network connectivity production models.</title>
<date>1977</date>
<journal>J Phon</journal>
<volume>5</volume>
<pages>115--133</pages>
<contexts>
<context position="9370" citStr="[23]" startWordPosition="1363" endWordPosition="1363">increased activation associations between the auditory can be combined with both signal characteristics and cortices and the left ventral premotor cortex[22]. Moreover, the speaker’s articulatory information, producing a more compared to listening to pseudo-words and reverseddesirable response. Based on the basic principles of the words, listening to normal words induces broad activation dual-stream processing and forward-inverse models, we connectivity in the auditory-motor network, which may propose that further-developed models should emphasize be useful for facilitating semantic processing[23]. Further how the auditory-motor interaction is modulated by both investigation is needed to verify whether this enhanced processing load (due to complex inputs) and prediction/ dynamic auditory-motor network promotes the transition estimation (due to task goals and feedback) (Fig. 1). from a sound stream into a series of meaningful motorSo far, co-activations between auditory and motor based units and results in speech comprehension. regions in speech perception have been clearly demonstrated. In addition to the well-known fact that speech When exposed to novel speech distortions, such as tim</context>
</contexts>
<marker>[23]</marker>
<rawString>Londei A, D’Ausilio A, Basso D, Sestieri C, Gratta CD, Romani GL, et al. Sensory-motor brain network connectivity production models. J Phon 1977, 5: 115–133. for speech comprehension. Hum Brain Mapp 2010, 31: 567– Liberman AM, Mattingly IG. The motor theory of speech</rawString>
</citation>
<citation valid="false">
<authors>
<author>H McGurk</author>
<author>J MacDonald</author>
</authors>
<title>Hearing lips and seeing voices.</title>
<contexts>
<context position="6264" citStr="[6]" startWordPosition="915" endWordPosition="915">yzing phonetic characters, It is well known that “gesture” information can affect acoustic features, and speech intelligibility[14,15], and the speech perception in different ways, such as the McGurk dorsal pathway is associated with sensorimotor mapping effect. When the listener sees the speaker producing the between auditory and motor representations[16,17], speech syllable (/ga/) while listening to another syllable production[14,18], and silent articulatory organ movement[19]. (/ba/), the mouth movement may mislead the listener into Although this dual-stream model proposes that each of the [6] hearing a different syllable (/da/) . This visuomotor cue pathways plays a specific role in speech perception, how strongly influences what we actually hear. Also, another the streams interact with each other is still not clear. study focused on the role of articulatory organ movement The other model, the forward-inverse model, proposes in a noisy environment[7]. Listeners perceive speech more that the motor cortical regions predict the consequences accurately when they can see the speaker’s articulatory of motor commands and revise the signals with the organ movement than when they cannot. A</context>
</contexts>
<marker>[6]</marker>
<rawString> McGurk H, MacDonald J. Hearing lips and seeing voices.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sumby WH</author>
<author>I Pollack</author>
</authors>
<title>Visual contribution to speech Nature</title>
<date>1976</date>
<pages>264--746</pages>
<contexts>
<context position="6629" citStr="[7]" startWordPosition="971" endWordPosition="971">eech syllable (/ga/) while listening to another syllable production[14,18], and silent articulatory organ movement[19]. (/ba/), the mouth movement may mislead the listener into Although this dual-stream model proposes that each of the [6] hearing a different syllable (/da/) . This visuomotor cue pathways plays a specific role in speech perception, how strongly influences what we actually hear. Also, another the streams interact with each other is still not clear. study focused on the role of articulatory organ movement The other model, the forward-inverse model, proposes in a noisy environment[7]. Listeners perceive speech more that the motor cortical regions predict the consequences accurately when they can see the speaker’s articulatory of motor commands and revise the signals with the organ movement than when they cannot. Also, under changing environment[20,21]. In more detail, before motor adverse listening conditions, lip-reading associated with commands reach the effectors, the forward-inverse model the target sentence can act as a cue to improve listener produces predicted sensory consequences of the motor [8,9] . Thus, perceiving “gesture” commands, and then compares the predi</context>
<context position="17565" citStr="[7,9]" startWordPosition="2522" endWordPosition="2522">d auditory task, weaker visual stimuli evoke stronger activation in the motor cortex than clear pictures of speakers[49], suggesting that a heavy-load task, such as analyzing distorted auditory or visual signals, requires involvement of the motor cortex. These studies suggest that whether the motor system is involved in speech perception is task-load dependent. In future, this assumption will be tested to confirm whether the dissociations between speech perception and speech production under either clinical or developmental conditions are task-load related. organ movement than when they cannot[7,9]. In addition, signals from the motor system help a listener to track a speaker talking over time[51,52]. It has been suggested that one of the functions of motor activation is tracking the talker’s speed and rhythm over time, and provides the timing signals to the auditory cortex. Particularly in a conversation, the monitoring role of the motor system in interacting with the auditory system over time can induce fluent conversation[53]. Under “cocktail-party” conditions, listeners are able to take advantage of certain perceptual cues to facilitate their selective attention to target speech. Se</context>
</contexts>
<marker>[7]</marker>
<rawString> Sumby WH, Pollack I. Visual contribution to speech Nature 1976, 264: 746–748. 580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilson SM</author>
</authors>
<title>Listening to speech activates motor areas involved in speech production.</title>
<date>2004</date>
<journal>Nat Neurosci</journal>
<volume>7</volume>
<pages>701--702</pages>
<contexts>
<context position="10245" citStr="[24, 25, 26, 27, 28, 29, 30]" startWordPosition="1486" endWordPosition="1486">e to task goals and feedback) (Fig. 1). from a sound stream into a series of meaningful motorSo far, co-activations between auditory and motor based units and results in speech comprehension. regions in speech perception have been clearly demonstrated. In addition to the well-known fact that speech When exposed to novel speech distortions, such as timeproduction is tightly related to the motor cortex, some compressed sentences, listeners can rapidly distinguish studies have shown that the motor cortex is activated in distorted sentences from normal-speed sentences, with speech perception tasks[24, 25, 26, 27, 28, 29, 30]. For example, when listeners Fig. 1. Modified Auditory-Motor Interaction model based on the Dual-stream Processing model in combination with the Forward-inverse model. This model emphasizes that both processing load and prediction/estimation affect the auditory-motor interaction (white two-headed arrow). Red: the auditory processing system including the ventral stream pathway; blue: the motor processing system including the dorsal stream pathway; green: the systems that mediate the processing load, prediction, and estimation that modulate the auditory-motor interaction (arrows with dashed lin</context>
<context position="11790" citStr="[24,31]" startWordPosition="1697" endWordPosition="1697"> various verbal stimuli may cause differential automatic activations of cortical regions involved in speech production. Also, it has been suggested that the activation of the motor cortex may reflect a mediating role of the motor cortex in speech perception[28, 29, 30]. Moreover, studies using either functional magnetic resonance imaging (fMRI) or transcranial magnetic stimulation (TMS) have confirmed the role of the motor cortex in speech perception. For example, in a speech perception task, strong activation in the motor cortex can be induced only when participants perceive the target speech[24,31]. Some studies using TMS of the motor cortex have demonstrated that stimulation of speech-related regions affects speech perception[32, 33, 34]. For example, using TMS to suppress the left premotor cortex, which is activated both during speech production and speech perception (Fig. Fig. 2. Representative fMRI activation in the premotor cortex 2), Meister et al. found that participants with a suppressed (PMC) associated with discriminating voiceless stop premotor cortex were impaired in discriminating voiceless consonants in single syllables masked by white noise stop consonants under white-noi</context>
</contexts>
<marker>[24]</marker>
<rawString>Wilson SM. Listening to speech activates motor areas involved in speech production. Nat Neurosci 2004, 7: 701– 702.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilson SM</author>
<author>M Iacoboni</author>
</authors>
<title>Neural responses to non-native intelligibility in noise. J Acoust Soc Am 1954, 26: 212. phonemes varying in producibility: evidence for the Rudmann DS, McCarley JS, Kramer AF. Bimodal displays sensorimotor nature of speech perception. Neuroimage improve speech comprehension in environments with multiple speakers. Hum Fac Erg Soc P</title>
<date>2003</date>
<volume>45</volume>
<pages>329--336</pages>
<contexts>
<context position="10245" citStr="[24, 25, 26, 27, 28, 29, 30]" startWordPosition="1486" endWordPosition="1486">e to task goals and feedback) (Fig. 1). from a sound stream into a series of meaningful motorSo far, co-activations between auditory and motor based units and results in speech comprehension. regions in speech perception have been clearly demonstrated. In addition to the well-known fact that speech When exposed to novel speech distortions, such as timeproduction is tightly related to the motor cortex, some compressed sentences, listeners can rapidly distinguish studies have shown that the motor cortex is activated in distorted sentences from normal-speed sentences, with speech perception tasks[24, 25, 26, 27, 28, 29, 30]. For example, when listeners Fig. 1. Modified Auditory-Motor Interaction model based on the Dual-stream Processing model in combination with the Forward-inverse model. This model emphasizes that both processing load and prediction/estimation affect the auditory-motor interaction (white two-headed arrow). Red: the auditory processing system including the ventral stream pathway; blue: the motor processing system including the dorsal stream pathway; green: the systems that mediate the processing load, prediction, and estimation that modulate the auditory-motor interaction (arrows with dashed lin</context>
<context position="16593" citStr="[25]" startWordPosition="2384" endWordPosition="2384">bal signals can activate motor areas [31,47], and induce better understanding of speech. Listeners perceive there is no difference in activation magnitude in the motor speech in noise-masking or speech-masking environments cortex between perceiving speech and perceiving nonmore accurately when they can see speaker’s articulatory verbal sounds[31]. Some studies have further shown that blurred speech causes even stronger activation in the bilateral premotor cortex, compared to clear speech[45] and perceiving a foreign language causes larger activation in the motor cortex than the native language[25]. Also, lowfrequency words induce higher activation in the motor cortex than high-frequency words[48]. Thus, facing unfamiliar stimuli (such as distorted speech, a foreign language, or low-frequency words), the motor cortex may play a role in facilitating the association with the auditory system to improve speech perception. More interestingly, in a mixed visual and auditory task, weaker visual stimuli evoke stronger activation in the motor cortex than clear pictures of speakers[49], suggesting that a heavy-load task, such as analyzing distorted auditory or visual signals, requires involvement</context>
</contexts>
<marker>[25]</marker>
<rawString>Wilson SM, Iacoboni M. Neural responses to non-native intelligibility in noise. J Acoust Soc Am 1954, 26: 212. phonemes varying in producibility: evidence for the Rudmann DS, McCarley JS, Kramer AF. Bimodal displays sensorimotor nature of speech perception. Neuroimage improve speech comprehension in environments with multiple speakers. Hum Fac Erg Soc P 2003, 45: 329–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Trans</author>
</authors>
<title>Soc Lond B Biol Sci</title>
<date>2003</date>
<pages>358--593</pages>
<contexts>
<context position="7162" citStr="[8,9]" startWordPosition="1047" endWordPosition="1047">her model, the forward-inverse model, proposes in a noisy environment[7]. Listeners perceive speech more that the motor cortical regions predict the consequences accurately when they can see the speaker’s articulatory of motor commands and revise the signals with the organ movement than when they cannot. Also, under changing environment[20,21]. In more detail, before motor adverse listening conditions, lip-reading associated with commands reach the effectors, the forward-inverse model the target sentence can act as a cue to improve listener produces predicted sensory consequences of the motor [8,9] . Thus, perceiving “gesture” commands, and then compares the predicted results with signals provides visuomotor cues, which help listeners take the real sensory information. This comparison provides advantage of the speaker’s motor actions during speech in more information for the central system to produce a an adverse noisy environment and facilitate the perceptual more appropriate performance. With time delays and performance. In other words, listeners actively, rather than interruptions from the surroundings, the motor commands passively, receive speech information. Supporting this need to</context>
<context position="17565" citStr="[7,9]" startWordPosition="2522" endWordPosition="2522">d auditory task, weaker visual stimuli evoke stronger activation in the motor cortex than clear pictures of speakers[49], suggesting that a heavy-load task, such as analyzing distorted auditory or visual signals, requires involvement of the motor cortex. These studies suggest that whether the motor system is involved in speech perception is task-load dependent. In future, this assumption will be tested to confirm whether the dissociations between speech perception and speech production under either clinical or developmental conditions are task-load related. organ movement than when they cannot[7,9]. In addition, signals from the motor system help a listener to track a speaker talking over time[51,52]. It has been suggested that one of the functions of motor activation is tracking the talker’s speed and rhythm over time, and provides the timing signals to the auditory cortex. Particularly in a conversation, the monitoring role of the motor system in interacting with the auditory system over time can induce fluent conversation[53]. Under “cocktail-party” conditions, listeners are able to take advantage of certain perceptual cues to facilitate their selective attention to target speech. Se</context>
</contexts>
<marker>[9]</marker>
<rawString> Trans R Soc Lond B Biol Sci 2003, 358: 593–602.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Adank</author>
<author>Devlin JT</author>
</authors>
<title>On-line plasticity in spoken sentence Kent RD, Minifie FD. Coarticulation in recent speech perception revised. Cognition</title>
<date>1985</date>
<pages>21--1</pages>
<contexts>
<context position="8923" citStr="[22]" startWordPosition="1306" endWordPosition="1306"> reported stronger activity in Due to the role of motor representation in revising distorted correct trials over incorrect trials within both the ventral signals, listeners can recognize the speaker’s intention and listening [11] premotor cortex and Broca’s area . However, it is still not predict the outcome of motor commands before making 492 Neurosci Bull June 1, 2014, 30(3): 490–496 responses. In other words, anticipation of a motor signal increased activation associations between the auditory can be combined with both signal characteristics and cortices and the left ventral premotor cortex[22]. Moreover, the speaker’s articulatory information, producing a more compared to listening to pseudo-words and reverseddesirable response. Based on the basic principles of the words, listening to normal words induces broad activation dual-stream processing and forward-inverse models, we connectivity in the auditory-motor network, which may propose that further-developed models should emphasize be useful for facilitating semantic processing[23]. Further how the auditory-motor interaction is modulated by both investigation is needed to verify whether this enhanced processing load (due to complex</context>
</contexts>
<marker>[22]</marker>
<rawString>Adank P, Devlin JT. On-line plasticity in spoken sentence Kent RD, Minifie FD. Coarticulation in recent speech perception revised. Cognition 1985, 21: 1–36.</rawString>
</citation>
<citation valid="true">
<title>framework for motor control and social interaction. Philos</title>
<date>2006</date>
<volume>33</volume>
<pages>316--325</pages>
<contexts>
<context position="7162" citStr="[8,9]" startWordPosition="1047" endWordPosition="1047">her model, the forward-inverse model, proposes in a noisy environment[7]. Listeners perceive speech more that the motor cortical regions predict the consequences accurately when they can see the speaker’s articulatory of motor commands and revise the signals with the organ movement than when they cannot. Also, under changing environment[20,21]. In more detail, before motor adverse listening conditions, lip-reading associated with commands reach the effectors, the forward-inverse model the target sentence can act as a cue to improve listener produces predicted sensory consequences of the motor [8,9] . Thus, perceiving “gesture” commands, and then compares the predicted results with signals provides visuomotor cues, which help listeners take the real sensory information. This comparison provides advantage of the speaker’s motor actions during speech in more information for the central system to produce a an adverse noisy environment and facilitate the perceptual more appropriate performance. With time delays and performance. In other words, listeners actively, rather than interruptions from the surroundings, the motor commands passively, receive speech information. Supporting this need to</context>
</contexts>
<marker>[8]</marker>
<rawString> framework for motor control and social interaction. Philos 2006, 33: 316–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Fadiga</author>
<author>L Craighero</author>
<author>G Buccino</author>
<author>Rizzolatti G Speech Wu C</author>
<author>S Cao</author>
<author>X Wu</author>
<author>L Li</author>
</authors>
<title>Temporally pre-presented listening specifically modulates the excitability of tongue lipreading cues release speech from informational masking. J muscles: a TMS study.</title>
<date>2002</date>
<journal>Eur J Neurosci</journal>
<pages>133--281</pages>
<contexts>
<context position="10245" citStr="[24, 25, 26, 27, 28, 29, 30]" startWordPosition="1486" endWordPosition="1486">e to task goals and feedback) (Fig. 1). from a sound stream into a series of meaningful motorSo far, co-activations between auditory and motor based units and results in speech comprehension. regions in speech perception have been clearly demonstrated. In addition to the well-known fact that speech When exposed to novel speech distortions, such as timeproduction is tightly related to the motor cortex, some compressed sentences, listeners can rapidly distinguish studies have shown that the motor cortex is activated in distorted sentences from normal-speed sentences, with speech perception tasks[24, 25, 26, 27, 28, 29, 30]. For example, when listeners Fig. 1. Modified Auditory-Motor Interaction model based on the Dual-stream Processing model in combination with the Forward-inverse model. This model emphasizes that both processing load and prediction/estimation affect the auditory-motor interaction (white two-headed arrow). Red: the auditory processing system including the ventral stream pathway; blue: the motor processing system including the dorsal stream pathway; green: the systems that mediate the processing load, prediction, and estimation that modulate the auditory-motor interaction (arrows with dashed lin</context>
</contexts>
<marker>[26]</marker>
<rawString>Fadiga L, Craighero L, Buccino G, Rizzolatti G. Speech Wu C, Cao S, Wu X, Li L. Temporally pre-presented listening specifically modulates the excitability of tongue lipreading cues release speech from informational masking. J muscles: a TMS study. Eur J Neurosci 2002, 15: 399–402. Acoust Soc Am 2013, 133: 281–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pulvermüller</author>
<author>M Huss</author>
<author>F Kherif</author>
<author>del Prado Martin FM</author>
<author>Hauk Neurosci Bull O</author>
<author>Y Shtyrov</author>
</authors>
<title>Motor cortex maps articulatory features of speech sounds.</title>
<date>2006</date>
<journal>Proc Natl Acad Sci U S A</journal>
<volume>103</volume>
<pages>7870</pages>
<contexts>
<context position="10245" citStr="[24, 25, 26, 27, 28, 29, 30]" startWordPosition="1486" endWordPosition="1486">e to task goals and feedback) (Fig. 1). from a sound stream into a series of meaningful motorSo far, co-activations between auditory and motor based units and results in speech comprehension. regions in speech perception have been clearly demonstrated. In addition to the well-known fact that speech When exposed to novel speech distortions, such as timeproduction is tightly related to the motor cortex, some compressed sentences, listeners can rapidly distinguish studies have shown that the motor cortex is activated in distorted sentences from normal-speed sentences, with speech perception tasks[24, 25, 26, 27, 28, 29, 30]. For example, when listeners Fig. 1. Modified Auditory-Motor Interaction model based on the Dual-stream Processing model in combination with the Forward-inverse model. This model emphasizes that both processing load and prediction/estimation affect the auditory-motor interaction (white two-headed arrow). Red: the auditory processing system including the ventral stream pathway; blue: the motor processing system including the dorsal stream pathway; green: the systems that mediate the processing load, prediction, and estimation that modulate the auditory-motor interaction (arrows with dashed lin</context>
</contexts>
<marker>[27]</marker>
<rawString>Pulvermüller F, Huss M, Kherif F, del Prado Martin FM, Hauk Neurosci Bull O, Shtyrov Y. Motor cortex maps articulatory features of speech sounds. Proc Natl Acad Sci U S A 2006, 103: 7865– 7870. June 1, 2014, 30(3): 490–496 1790–1804.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Werker JF</author>
<author>Yeung HH</author>
</authors>
<title>Infant speech perception bootstraps word learning. Trends Cogn Sci</title>
<date>2005</date>
<volume>9</volume>
<pages>519--527</pages>
<contexts>
<context position="13770" citStr="[41,42]" startWordPosition="1977" endWordPosition="1977"> harder than in healthy children[39]. Also, children with severe patients with receptive aphasia exhibit impairment of dysarthria are unable to produce meaningful sentences, speech perception and comprehension, they can speak but they can accurately understand spoken content [40]. speech perception or comprehension [36,37] [35] . The dissociations in expressive and receptive Furthermore, infants usually learn to understand speech aphasia support another view that speech perception first and then begin to learn how to produce their own and production are two distinct processes. Moreover, words [41,42] . These studies indicate that dissociations patients with lesions in Broca’s area perform well in both between speech perception and speech production word-comprehension and syllable-identification tests, but occur during development. As speech perception and patients with temporal lobule damage perform poorly in speech production do not appear at the same time during these tests[38]. These studies also negate the role of motor development, motor cortical areas may not be as important regions in speech perception, but support the view that for speech perception as the Motor Theory proposes. f</context>
</contexts>
<marker>[41]</marker>
<rawString>Werker JF, Yeung HH. Infant speech perception bootstraps word learning. Trends Cogn Sci 2005, 9: 519–527.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Bever TG</author>
<author>D Poeppel</author>
</authors>
<title>Analysis by synthesis: a (re-) emerging</title>
<contexts>
<context position="10245" citStr="[24, 25, 26, 27, 28, 29, 30]" startWordPosition="1486" endWordPosition="1486">e to task goals and feedback) (Fig. 1). from a sound stream into a series of meaningful motorSo far, co-activations between auditory and motor based units and results in speech comprehension. regions in speech perception have been clearly demonstrated. In addition to the well-known fact that speech When exposed to novel speech distortions, such as timeproduction is tightly related to the motor cortex, some compressed sentences, listeners can rapidly distinguish studies have shown that the motor cortex is activated in distorted sentences from normal-speed sentences, with speech perception tasks[24, 25, 26, 27, 28, 29, 30]. For example, when listeners Fig. 1. Modified Auditory-Motor Interaction model based on the Dual-stream Processing model in combination with the Forward-inverse model. This model emphasizes that both processing load and prediction/estimation affect the auditory-motor interaction (white two-headed arrow). Red: the auditory processing system including the ventral stream pathway; blue: the motor processing system including the dorsal stream pathway; green: the systems that mediate the processing load, prediction, and estimation that modulate the auditory-motor interaction (arrows with dashed lin</context>
</contexts>
<marker>[28]</marker>
<rawString>Bever TG, Poeppel D. Analysis by synthesis: a (re-) emerging</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsao FM</author>
<author>Liu HM</author>
<author>Kuhl PK</author>
</authors>
<title>Speech perception in infancy program of research for language and vision. Biolinguistics predicts language development in the second year of life: a</title>
<date>2010</date>
<volume>4</volume>
<pages>174--200</pages>
<contexts>
<context position="13770" citStr="[41,42]" startWordPosition="1977" endWordPosition="1977"> harder than in healthy children[39]. Also, children with severe patients with receptive aphasia exhibit impairment of dysarthria are unable to produce meaningful sentences, speech perception and comprehension, they can speak but they can accurately understand spoken content [40]. speech perception or comprehension [36,37] [35] . The dissociations in expressive and receptive Furthermore, infants usually learn to understand speech aphasia support another view that speech perception first and then begin to learn how to produce their own and production are two distinct processes. Moreover, words [41,42] . These studies indicate that dissociations patients with lesions in Broca’s area perform well in both between speech perception and speech production word-comprehension and syllable-identification tests, but occur during development. As speech perception and patients with temporal lobule damage perform poorly in speech production do not appear at the same time during these tests[38]. These studies also negate the role of motor development, motor cortical areas may not be as important regions in speech perception, but support the view that for speech perception as the Motor Theory proposes. f</context>
</contexts>
<marker>[42]</marker>
<rawString>Tsao FM, Liu HM, Kuhl PK. Speech perception in infancy program of research for language and vision. Biolinguistics predicts language development in the second year of life: a 2010, 4: 174–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Callan DE</author>
<author>Jones JA</author>
<author>Callan AM</author>
<author>R Akahane-Yamada</author>
</authors>
<title>Phonetic perceptual identification by native-and secondlongitudinal study. Child Dev</title>
<date>2004</date>
<volume>75</volume>
<pages>1067--1084</pages>
<contexts>
<context position="10245" citStr="[24, 25, 26, 27, 28, 29, 30]" startWordPosition="1486" endWordPosition="1486">e to task goals and feedback) (Fig. 1). from a sound stream into a series of meaningful motorSo far, co-activations between auditory and motor based units and results in speech comprehension. regions in speech perception have been clearly demonstrated. In addition to the well-known fact that speech When exposed to novel speech distortions, such as timeproduction is tightly related to the motor cortex, some compressed sentences, listeners can rapidly distinguish studies have shown that the motor cortex is activated in distorted sentences from normal-speed sentences, with speech perception tasks[24, 25, 26, 27, 28, 29, 30]. For example, when listeners Fig. 1. Modified Auditory-Motor Interaction model based on the Dual-stream Processing model in combination with the Forward-inverse model. This model emphasizes that both processing load and prediction/estimation affect the auditory-motor interaction (white two-headed arrow). Red: the auditory processing system including the ventral stream pathway; blue: the motor processing system including the dorsal stream pathway; green: the systems that mediate the processing load, prediction, and estimation that modulate the auditory-motor interaction (arrows with dashed lin</context>
</contexts>
<marker>[29]</marker>
<rawString>Callan DE, Jones JA, Callan AM, Akahane-Yamada R. Phonetic perceptual identification by native-and secondlongitudinal study. Child Dev 2004, 75: 1067–1084.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Basso</author>
<author>G Casati</author>
<author>Vignolo LA</author>
</authors>
<title>Phonemic identification defect in aphasia. Cortex</title>
<date>1977</date>
<contexts>
<context position="15184" citStr="[19,43,44]" startWordPosition="2183" endWordPosition="2183"> in child development has also shown in speech perception, while clinical and developmental dissociations between speech perception and speech studies have shown significant dissociations between 494 Neurosci Bull June 1, 2014, 30(3): 490–496 speech perception and speech production. Also note that In a noisy environment (like a cocktail party), although some fMRI studies did not reveal an increased activation there are many acoustic sources from various directions, in motor cortical regions during speech perception and listeners are still able to identify and follow target speech comprehension[19,43,44]. sounds in this high perceptual-load situation. How The different results in the studies described above can listeners separate various speakers’ signals and may be due to different task demands. The Hickok understand target sentences? Although this “cocktail party” and Poeppel study showed that when the task load is problem advanced by Cherry[50] has not been fully solved, high, requiring both speech identification and speech several lines of evidence suggest that the motor system categorization, the activated frontal region extends to the plays a role in solving this problem when the percep</context>
</contexts>
<marker>[43]</marker>
<rawString>Basso A, Casati G, Vignolo LA. Phonemic identification defect in aphasia. Cortex 1977, 13: 85. language speakers differentially activates brain regions</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott SK</author>
<author>S Rosen</author>
<author>H Lang</author>
<author>Wise RJ</author>
</authors>
<title>Neural correlates involved with acoustic phonetic processing and those of intelligibility in speech investigated with noise vocoded involved with articulatory-auditory/orosensory internal speech-a positron emission tomography study. J Acoust Soc models. Neuroimage</title>
<date>2004</date>
<volume>22</volume>
<pages>1182--1194</pages>
<contexts>
<context position="15184" citStr="[19,43,44]" startWordPosition="2183" endWordPosition="2183"> in child development has also shown in speech perception, while clinical and developmental dissociations between speech perception and speech studies have shown significant dissociations between 494 Neurosci Bull June 1, 2014, 30(3): 490–496 speech perception and speech production. Also note that In a noisy environment (like a cocktail party), although some fMRI studies did not reveal an increased activation there are many acoustic sources from various directions, in motor cortical regions during speech perception and listeners are still able to identify and follow target speech comprehension[19,43,44]. sounds in this high perceptual-load situation. How The different results in the studies described above can listeners separate various speakers’ signals and may be due to different task demands. The Hickok understand target sentences? Although this “cocktail party” and Poeppel study showed that when the task load is problem advanced by Cherry[50] has not been fully solved, high, requiring both speech identification and speech several lines of evidence suggest that the motor system categorization, the activated frontal region extends to the plays a role in solving this problem when the percep</context>
</contexts>
<marker>[44]</marker>
<rawString>Scott SK, Rosen S, Lang H, Wise RJ. Neural correlates involved with acoustic phonetic processing and those of intelligibility in speech investigated with noise vocoded involved with articulatory-auditory/orosensory internal speech-a positron emission tomography study. J Acoust Soc models. Neuroimage 2004, 22: 1182–1194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hickok</author>
<author>J Houde</author>
<author>F Rong</author>
</authors>
<title>Sensorimotor integration in speech processing: computational basis and neural organization.</title>
<date>2011</date>
<journal>Neuron</journal>
<volume>69</volume>
<pages>407--422</pages>
<contexts>
<context position="10245" citStr="[24, 25, 26, 27, 28, 29, 30]" startWordPosition="1486" endWordPosition="1486">e to task goals and feedback) (Fig. 1). from a sound stream into a series of meaningful motorSo far, co-activations between auditory and motor based units and results in speech comprehension. regions in speech perception have been clearly demonstrated. In addition to the well-known fact that speech When exposed to novel speech distortions, such as timeproduction is tightly related to the motor cortex, some compressed sentences, listeners can rapidly distinguish studies have shown that the motor cortex is activated in distorted sentences from normal-speed sentences, with speech perception tasks[24, 25, 26, 27, 28, 29, 30]. For example, when listeners Fig. 1. Modified Auditory-Motor Interaction model based on the Dual-stream Processing model in combination with the Forward-inverse model. This model emphasizes that both processing load and prediction/estimation affect the auditory-motor interaction (white two-headed arrow). Red: the auditory processing system including the ventral stream pathway; blue: the motor processing system including the dorsal stream pathway; green: the systems that mediate the processing load, prediction, and estimation that modulate the auditory-motor interaction (arrows with dashed lin</context>
</contexts>
<marker>[30]</marker>
<rawString>Hickok G, Houde J, Rong F. Sensorimotor integration in speech processing: computational basis and neural organization. Neuron 2011, 69: 407–422. Am 2006, 120: 1075.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davis MH</author>
<author>Johnsrude IS</author>
</authors>
<title>Hierarchical processing in spoken language comprehension.</title>
<date>2003</date>
<journal>J Neurosci</journal>
<volume>23</volume>
<pages>3423--3431</pages>
<contexts>
<context position="15910" citStr="[45,46]" startWordPosition="2291" endWordPosition="2291"> separate various speakers’ signals and may be due to different task demands. The Hickok understand target sentences? Although this “cocktail party” and Poeppel study showed that when the task load is problem advanced by Cherry[50] has not been fully solved, high, requiring both speech identification and speech several lines of evidence suggest that the motor system categorization, the activated frontal region extends to the plays a role in solving this problem when the perceptual premotor cortex[17]. Also, when speech signals are distorted, load is high. the motor cortex is markedly activated[45,46]. Interestingly, First, observing a speaker’s articulator movements can non-verbal signals can activate motor areas [31,47], and induce better understanding of speech. Listeners perceive there is no difference in activation magnitude in the motor speech in noise-masking or speech-masking environments cortex between perceiving speech and perceiving nonmore accurately when they can see speaker’s articulatory verbal sounds[31]. Some studies have further shown that blurred speech causes even stronger activation in the bilateral premotor cortex, compared to clear speech[45] and perceiving a foreign</context>
</contexts>
<marker>[45]</marker>
<rawString>Davis MH, Johnsrude IS. Hierarchical processing in spoken language comprehension. J Neurosci 2003, 23: 3423–3431.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Uppenkamp</author>
<author>Johnsrude IS</author>
<author>D Norris</author>
<author>W Marslen-Wilson</author>
</authors>
<contexts>
<context position="15910" citStr="[45,46]" startWordPosition="2291" endWordPosition="2291"> separate various speakers’ signals and may be due to different task demands. The Hickok understand target sentences? Although this “cocktail party” and Poeppel study showed that when the task load is problem advanced by Cherry[50] has not been fully solved, high, requiring both speech identification and speech several lines of evidence suggest that the motor system categorization, the activated frontal region extends to the plays a role in solving this problem when the perceptual premotor cortex[17]. Also, when speech signals are distorted, load is high. the motor cortex is markedly activated[45,46]. Interestingly, First, observing a speaker’s articulator movements can non-verbal signals can activate motor areas [31,47], and induce better understanding of speech. Listeners perceive there is no difference in activation magnitude in the motor speech in noise-masking or speech-masking environments cortex between perceiving speech and perceiving nonmore accurately when they can see speaker’s articulatory verbal sounds[31]. Some studies have further shown that blurred speech causes even stronger activation in the bilateral premotor cortex, compared to clear speech[45] and perceiving a foreign</context>
</contexts>
<marker>[46]</marker>
<rawString>Uppenkamp S, Johnsrude IS, Norris D, Marslen-Wilson W,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Watkins KE</author>
<author>Strafella AP</author>
<author>T Paus</author>
</authors>
<title>Seeing and hearing speech Patterson RD. Locating the initial stages of speech? Sound excites the motor system involved in speech production. processing in human temporal cortex. Neuroimage</title>
<date>2006</date>
<volume>41</volume>
<pages>989--994</pages>
<contexts>
<context position="11790" citStr="[24,31]" startWordPosition="1697" endWordPosition="1697"> various verbal stimuli may cause differential automatic activations of cortical regions involved in speech production. Also, it has been suggested that the activation of the motor cortex may reflect a mediating role of the motor cortex in speech perception[28, 29, 30]. Moreover, studies using either functional magnetic resonance imaging (fMRI) or transcranial magnetic stimulation (TMS) have confirmed the role of the motor cortex in speech perception. For example, in a speech perception task, strong activation in the motor cortex can be induced only when participants perceive the target speech[24,31]. Some studies using TMS of the motor cortex have demonstrated that stimulation of speech-related regions affects speech perception[32, 33, 34]. For example, using TMS to suppress the left premotor cortex, which is activated both during speech production and speech perception (Fig. Fig. 2. Representative fMRI activation in the premotor cortex 2), Meister et al. found that participants with a suppressed (PMC) associated with discriminating voiceless stop premotor cortex were impaired in discriminating voiceless consonants in single syllables masked by white noise stop consonants under white-noi</context>
<context position="16033" citStr="[31,47]" startWordPosition="2306" endWordPosition="2306">ugh this “cocktail party” and Poeppel study showed that when the task load is problem advanced by Cherry[50] has not been fully solved, high, requiring both speech identification and speech several lines of evidence suggest that the motor system categorization, the activated frontal region extends to the plays a role in solving this problem when the perceptual premotor cortex[17]. Also, when speech signals are distorted, load is high. the motor cortex is markedly activated[45,46]. Interestingly, First, observing a speaker’s articulator movements can non-verbal signals can activate motor areas [31,47], and induce better understanding of speech. Listeners perceive there is no difference in activation magnitude in the motor speech in noise-masking or speech-masking environments cortex between perceiving speech and perceiving nonmore accurately when they can see speaker’s articulatory verbal sounds[31]. Some studies have further shown that blurred speech causes even stronger activation in the bilateral premotor cortex, compared to clear speech[45] and perceiving a foreign language causes larger activation in the motor cortex than the native language[25]. Also, lowfrequency words induce higher</context>
</contexts>
<marker>[31]</marker>
<rawString>Watkins KE, Strafella AP, Paus T. Seeing and hearing speech Patterson RD. Locating the initial stages of speech? Sound excites the motor system involved in speech production. processing in human temporal cortex. Neuroimage 2006, 31: Neuropsychologia 2003, 41: 989–994. 1284–1296.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A D’Ausilio</author>
<author>F Pulvermüller</author>
<author>P Salmas</author>
<author>I Bufalari</author>
<author>C Begliomini</author>
</authors>
<contexts>
<context position="11933" citStr="[32, 33, 34]" startWordPosition="1715" endWordPosition="1715">suggested that the activation of the motor cortex may reflect a mediating role of the motor cortex in speech perception[28, 29, 30]. Moreover, studies using either functional magnetic resonance imaging (fMRI) or transcranial magnetic stimulation (TMS) have confirmed the role of the motor cortex in speech perception. For example, in a speech perception task, strong activation in the motor cortex can be induced only when participants perceive the target speech[24,31]. Some studies using TMS of the motor cortex have demonstrated that stimulation of speech-related regions affects speech perception[32, 33, 34]. For example, using TMS to suppress the left premotor cortex, which is activated both during speech production and speech perception (Fig. Fig. 2. Representative fMRI activation in the premotor cortex 2), Meister et al. found that participants with a suppressed (PMC) associated with discriminating voiceless stop premotor cortex were impaired in discriminating voiceless consonants in single syllables masked by white noise stop consonants under white-noise masking conditions. in two representative participants. Regions selected for stimulation are shown in bright colors. Arrowheads Thus, they s</context>
</contexts>
<marker>[32]</marker>
<rawString>D’Ausilio A, Pulvermüller F, Salmas P, Bufalari I, Begliomini C,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Warren JE</author>
<author>Sauter DA</author>
<author>F Eisner</author>
<author>J Wiland</author>
<author>M Dresner</author>
<author>L Fadiga</author>
</authors>
<title>The motor somatotopy of speech perception. Curr Wise RJ, et al. Positive emotions preferentially engage an Biol</title>
<date>2009</date>
<journal>J Neurosci</journal>
<volume>26</volume>
<pages>13067</pages>
<contexts>
<context position="16033" citStr="[31,47]" startWordPosition="2306" endWordPosition="2306">ugh this “cocktail party” and Poeppel study showed that when the task load is problem advanced by Cherry[50] has not been fully solved, high, requiring both speech identification and speech several lines of evidence suggest that the motor system categorization, the activated frontal region extends to the plays a role in solving this problem when the perceptual premotor cortex[17]. Also, when speech signals are distorted, load is high. the motor cortex is markedly activated[45,46]. Interestingly, First, observing a speaker’s articulator movements can non-verbal signals can activate motor areas [31,47], and induce better understanding of speech. Listeners perceive there is no difference in activation magnitude in the motor speech in noise-masking or speech-masking environments cortex between perceiving speech and perceiving nonmore accurately when they can see speaker’s articulatory verbal sounds[31]. Some studies have further shown that blurred speech causes even stronger activation in the bilateral premotor cortex, compared to clear speech[45] and perceiving a foreign language causes larger activation in the motor cortex than the native language[25]. Also, lowfrequency words induce higher</context>
</contexts>
<marker>[47]</marker>
<rawString>Warren JE, Sauter DA, Eisner F, Wiland J, Dresner M, Fadiga L. The motor somatotopy of speech perception. Curr Wise RJ, et al. Positive emotions preferentially engage an Biol 2009, 19: 381–385. auditory–motor “mirror” system. J Neurosci 2006, 26: 13067–</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meister IG</author>
<author>Wilson SM</author>
<author>C Deblieck</author>
<author>Wu AD</author>
<author>M Iacoboni</author>
</authors>
<title>The essential role of premotor cortex in speech perception. Curr Biol</title>
<date>2007</date>
<volume>17</volume>
<pages>1692--1696</pages>
<contexts>
<context position="11933" citStr="[32, 33, 34]" startWordPosition="1715" endWordPosition="1715">suggested that the activation of the motor cortex may reflect a mediating role of the motor cortex in speech perception[28, 29, 30]. Moreover, studies using either functional magnetic resonance imaging (fMRI) or transcranial magnetic stimulation (TMS) have confirmed the role of the motor cortex in speech perception. For example, in a speech perception task, strong activation in the motor cortex can be induced only when participants perceive the target speech[24,31]. Some studies using TMS of the motor cortex have demonstrated that stimulation of speech-related regions affects speech perception[32, 33, 34]. For example, using TMS to suppress the left premotor cortex, which is activated both during speech production and speech perception (Fig. Fig. 2. Representative fMRI activation in the premotor cortex 2), Meister et al. found that participants with a suppressed (PMC) associated with discriminating voiceless stop premotor cortex were impaired in discriminating voiceless consonants in single syllables masked by white noise stop consonants under white-noise masking conditions. in two representative participants. Regions selected for stimulation are shown in bright colors. Arrowheads Thus, they s</context>
</contexts>
<marker>[33]</marker>
<rawString>Meister IG, Wilson SM, Deblieck C, Wu AD, Iacoboni M. The essential role of premotor cortex in speech perception. Curr Biol 2007, 17: 1692–1696.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Watkins</author>
<author>T Paus</author>
</authors>
<title>Modulation of motor excitability during speech perception: the role of Broca’s area.</title>
<date>2004</date>
<journal>J Cogn Neurosci</journal>
<volume>16</volume>
<pages>978--987</pages>
<contexts>
<context position="11933" citStr="[32, 33, 34]" startWordPosition="1715" endWordPosition="1715">suggested that the activation of the motor cortex may reflect a mediating role of the motor cortex in speech perception[28, 29, 30]. Moreover, studies using either functional magnetic resonance imaging (fMRI) or transcranial magnetic stimulation (TMS) have confirmed the role of the motor cortex in speech perception. For example, in a speech perception task, strong activation in the motor cortex can be induced only when participants perceive the target speech[24,31]. Some studies using TMS of the motor cortex have demonstrated that stimulation of speech-related regions affects speech perception[32, 33, 34]. For example, using TMS to suppress the left premotor cortex, which is activated both during speech production and speech perception (Fig. Fig. 2. Representative fMRI activation in the premotor cortex 2), Meister et al. found that participants with a suppressed (PMC) associated with discriminating voiceless stop premotor cortex were impaired in discriminating voiceless consonants in single syllables masked by white noise stop consonants under white-noise masking conditions. in two representative participants. Regions selected for stimulation are shown in bright colors. Arrowheads Thus, they s</context>
</contexts>
<marker>[34]</marker>
<rawString>Watkins K, Paus T. Modulation of motor excitability during speech perception: the role of Broca’s area. J Cogn Neurosci 2004, 16: 978–987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohr JP</author>
<author>Pessin MS</author>
<author>S Finkelstein</author>
<author>Funkenstein HH</author>
<author>Duncan GW</author>
<author>Davis KR</author>
</authors>
<date>1978</date>
<booktitle>Broca aphasia Pathologic and clinical. Neurology</booktitle>
<volume>28</volume>
<pages>311--311</pages>
<contexts>
<context position="13492" citStr="[35]" startWordPosition="1936" endWordPosition="1936"> Children born with hearing loss can learn speech production. For example, patients with expressive to speak if they gain enough positive somatosensory aphasia exhibit impairment of speech production but not feedback, even though the learning process is much . Also, although harder than in healthy children[39]. Also, children with severe patients with receptive aphasia exhibit impairment of dysarthria are unable to produce meaningful sentences, speech perception and comprehension, they can speak but they can accurately understand spoken content [40]. speech perception or comprehension [36,37] [35] . The dissociations in expressive and receptive Furthermore, infants usually learn to understand speech aphasia support another view that speech perception first and then begin to learn how to produce their own and production are two distinct processes. Moreover, words [41,42] . These studies indicate that dissociations patients with lesions in Broca’s area perform well in both between speech perception and speech production word-comprehension and syllable-identification tests, but occur during development. As speech perception and patients with temporal lobule damage perform poorly in speech</context>
</contexts>
<marker>[35]</marker>
<rawString>Mohr JP, Pessin MS, Finkelstein S, Funkenstein HH, Duncan GW, Davis KR. Broca aphasia Pathologic and clinical. Neurology 1978, 28: 311–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Crinion JT</author>
<author>Warburton EA</author>
<author>Lambon-Ralph MA</author>
<author>D Howard</author>
<author>Wise RJ</author>
</authors>
<title>Listening to narrative speech after aphasic stroke: the role of the left anterior temporal lobe. Cereb Cortex</title>
<date>2006</date>
<volume>16</volume>
<pages>1116--1125</pages>
<contexts>
<context position="13487" citStr="[36,37]" startWordPosition="1935" endWordPosition="1935">duction. Children born with hearing loss can learn speech production. For example, patients with expressive to speak if they gain enough positive somatosensory aphasia exhibit impairment of speech production but not feedback, even though the learning process is much . Also, although harder than in healthy children[39]. Also, children with severe patients with receptive aphasia exhibit impairment of dysarthria are unable to produce meaningful sentences, speech perception and comprehension, they can speak but they can accurately understand spoken content [40]. speech perception or comprehension [36,37] [35] . The dissociations in expressive and receptive Furthermore, infants usually learn to understand speech aphasia support another view that speech perception first and then begin to learn how to produce their own and production are two distinct processes. Moreover, words [41,42] . These studies indicate that dissociations patients with lesions in Broca’s area perform well in both between speech perception and speech production word-comprehension and syllable-identification tests, but occur during development. As speech perception and patients with temporal lobule damage perform poorly in s</context>
</contexts>
<marker>[36]</marker>
<rawString>Crinion JT, Warburton EA, Lambon-Ralph MA, Howard D, Wise RJ. Listening to narrative speech after aphasic stroke: the role of the left anterior temporal lobe. Cereb Cortex 2006, 16: 1116–1125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bogen JE</author>
<author>Bogen GM</author>
</authors>
<title>Wernick&amp;apos;s region-Where is it? Ann NY Acad Sci,</title>
<date>1976</date>
<pages>280--834</pages>
<contexts>
<context position="13487" citStr="[36,37]" startWordPosition="1935" endWordPosition="1935">duction. Children born with hearing loss can learn speech production. For example, patients with expressive to speak if they gain enough positive somatosensory aphasia exhibit impairment of speech production but not feedback, even though the learning process is much . Also, although harder than in healthy children[39]. Also, children with severe patients with receptive aphasia exhibit impairment of dysarthria are unable to produce meaningful sentences, speech perception and comprehension, they can speak but they can accurately understand spoken content [40]. speech perception or comprehension [36,37] [35] . The dissociations in expressive and receptive Furthermore, infants usually learn to understand speech aphasia support another view that speech perception first and then begin to learn how to produce their own and production are two distinct processes. Moreover, words [41,42] . These studies indicate that dissociations patients with lesions in Broca’s area perform well in both between speech perception and speech production word-comprehension and syllable-identification tests, but occur during development. As speech perception and patients with temporal lobule damage perform poorly in s</context>
</contexts>
<marker>[37]</marker>
<rawString>Bogen JE, Bogen GM. Wernick&amp;apos;s region-Where is it? Ann NY Acad Sci, 1976, 280: 834–843. 13075.</rawString>
</citation>
<citation valid="true">
<title>R o y A C , C r a i g h e r o L , F a b b r i - D e s t r o M , F a d i g a L . Phonological and lexical motor facilitation during speech listening: A transcranial magnetic stimulation study.</title>
<date>2008</date>
<journal>J Physiol</journal>
<pages>102--101</pages>
<location>Paris</location>
<contexts>
<context position="16694" citStr="[48]" startWordPosition="2397" endWordPosition="2397">rceive there is no difference in activation magnitude in the motor speech in noise-masking or speech-masking environments cortex between perceiving speech and perceiving nonmore accurately when they can see speaker’s articulatory verbal sounds[31]. Some studies have further shown that blurred speech causes even stronger activation in the bilateral premotor cortex, compared to clear speech[45] and perceiving a foreign language causes larger activation in the motor cortex than the native language[25]. Also, lowfrequency words induce higher activation in the motor cortex than high-frequency words[48]. Thus, facing unfamiliar stimuli (such as distorted speech, a foreign language, or low-frequency words), the motor cortex may play a role in facilitating the association with the auditory system to improve speech perception. More interestingly, in a mixed visual and auditory task, weaker visual stimuli evoke stronger activation in the motor cortex than clear pictures of speakers[49], suggesting that a heavy-load task, such as analyzing distorted auditory or visual signals, requires involvement of the motor cortex. These studies suggest that whether the motor system is involved in speech perce</context>
</contexts>
<marker>[48]</marker>
<rawString>R o y A C , C r a i g h e r o L , F a b b r i - D e s t r o M , F a d i g a L . Phonological and lexical motor facilitation during speech listening: A transcranial magnetic stimulation study. J Physiol Paris 2008, 102: 101–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fridriksson</author>
<author>J Moss</author>
<author>B Davis</author>
<author>Baylis GC</author>
<author>L Bonilha</author>
<author>C Rorden</author>
</authors>
<title>Motor speech perception modulates the cortical language areas. Neuroimage</title>
<date>2008</date>
<volume>41</volume>
<pages>605--613</pages>
<contexts>
<context position="17080" citStr="[49]" startWordPosition="2454" endWordPosition="2454">peech[45] and perceiving a foreign language causes larger activation in the motor cortex than the native language[25]. Also, lowfrequency words induce higher activation in the motor cortex than high-frequency words[48]. Thus, facing unfamiliar stimuli (such as distorted speech, a foreign language, or low-frequency words), the motor cortex may play a role in facilitating the association with the auditory system to improve speech perception. More interestingly, in a mixed visual and auditory task, weaker visual stimuli evoke stronger activation in the motor cortex than clear pictures of speakers[49], suggesting that a heavy-load task, such as analyzing distorted auditory or visual signals, requires involvement of the motor cortex. These studies suggest that whether the motor system is involved in speech perception is task-load dependent. In future, this assumption will be tested to confirm whether the dissociations between speech perception and speech production under either clinical or developmental conditions are task-load related. organ movement than when they cannot[7,9]. In addition, signals from the motor system help a listener to track a speaker talking over time[51,52]. It has be</context>
</contexts>
<marker>[49]</marker>
<rawString>Fridriksson J, Moss J, Davis B, Baylis GC, Bonilha L, Rorden C. Motor speech perception modulates the cortical language areas. Neuroimage 2008, 41: 605–613.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cherry EC</author>
</authors>
<title>Some experiments on the recognition of speech, with one and with two ears.</title>
<date>1953</date>
<journal>J Acoust Soc Am</journal>
<pages>25--975</pages>
<contexts>
<context position="15534" citStr="[50]" startWordPosition="2235" endWordPosition="2235">some fMRI studies did not reveal an increased activation there are many acoustic sources from various directions, in motor cortical regions during speech perception and listeners are still able to identify and follow target speech comprehension[19,43,44]. sounds in this high perceptual-load situation. How The different results in the studies described above can listeners separate various speakers’ signals and may be due to different task demands. The Hickok understand target sentences? Although this “cocktail party” and Poeppel study showed that when the task load is problem advanced by Cherry[50] has not been fully solved, high, requiring both speech identification and speech several lines of evidence suggest that the motor system categorization, the activated frontal region extends to the plays a role in solving this problem when the perceptual premotor cortex[17]. Also, when speech signals are distorted, load is high. the motor cortex is markedly activated[45,46]. Interestingly, First, observing a speaker’s articulator movements can non-verbal signals can activate motor areas [31,47], and induce better understanding of speech. Listeners perceive there is no difference in activation </context>
</contexts>
<marker>[50]</marker>
<rawString>Cherry EC. Some experiments on the recognition of speech, with one and with two ears. J Acoust Soc Am 1953, 25: 975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>McFarland DH</author>
</authors>
<title>Respiratory markers of conversational interaction.</title>
<date>2001</date>
<journal>J Speech Lang Hear Res</journal>
<pages>44--128</pages>
<contexts>
<context position="17669" citStr="[51,52]" startWordPosition="2539" endWordPosition="2539">es of speakers[49], suggesting that a heavy-load task, such as analyzing distorted auditory or visual signals, requires involvement of the motor cortex. These studies suggest that whether the motor system is involved in speech perception is task-load dependent. In future, this assumption will be tested to confirm whether the dissociations between speech perception and speech production under either clinical or developmental conditions are task-load related. organ movement than when they cannot[7,9]. In addition, signals from the motor system help a listener to track a speaker talking over time[51,52]. It has been suggested that one of the functions of motor activation is tracking the talker’s speed and rhythm over time, and provides the timing signals to the auditory cortex. Particularly in a conversation, the monitoring role of the motor system in interacting with the auditory system over time can induce fluent conversation[53]. Under “cocktail-party” conditions, listeners are able to take advantage of certain perceptual cues to facilitate their selective attention to target speech. Selective attention allocates more cognitive resources to the motor representation of speech so that a lis</context>
</contexts>
<marker>[51]</marker>
<rawString>McFarland DH. Respiratory markers of conversational interaction. J Speech Lang Hear Res 2001, 44: 128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pickering MJ</author>
<author>S Garrod</author>
</authors>
<title>Do people use language production to make predictions during comprehension? Trends Cogn Sci</title>
<date>2007</date>
<volume>11</volume>
<pages>105--110</pages>
<contexts>
<context position="17669" citStr="[51,52]" startWordPosition="2539" endWordPosition="2539">es of speakers[49], suggesting that a heavy-load task, such as analyzing distorted auditory or visual signals, requires involvement of the motor cortex. These studies suggest that whether the motor system is involved in speech perception is task-load dependent. In future, this assumption will be tested to confirm whether the dissociations between speech perception and speech production under either clinical or developmental conditions are task-load related. organ movement than when they cannot[7,9]. In addition, signals from the motor system help a listener to track a speaker talking over time[51,52]. It has been suggested that one of the functions of motor activation is tracking the talker’s speed and rhythm over time, and provides the timing signals to the auditory cortex. Particularly in a conversation, the monitoring role of the motor system in interacting with the auditory system over time can induce fluent conversation[53]. Under “cocktail-party” conditions, listeners are able to take advantage of certain perceptual cues to facilitate their selective attention to target speech. Selective attention allocates more cognitive resources to the motor representation of speech so that a lis</context>
</contexts>
<marker>[52]</marker>
<rawString>Pickering MJ, Garrod S. Do people use language production to make predictions during comprehension? Trends Cogn Sci 2007, 11: 105–110.</rawString>
</citation>
<citation valid="false">
<title>B a k e r E , B l u m s t e i n S E , G o o d g l a s s H . I n t e r a c t i o n</title>
<contexts>
<context position="14157" citStr="[38]" startWordPosition="2031" endWordPosition="2031"> infants usually learn to understand speech aphasia support another view that speech perception first and then begin to learn how to produce their own and production are two distinct processes. Moreover, words [41,42] . These studies indicate that dissociations patients with lesions in Broca’s area perform well in both between speech perception and speech production word-comprehension and syllable-identification tests, but occur during development. As speech perception and patients with temporal lobule damage perform poorly in speech production do not appear at the same time during these tests[38]. These studies also negate the role of motor development, motor cortical areas may not be as important regions in speech perception, but support the view that for speech perception as the Motor Theory proposes. fluently temporal regions rather than motor areas are important in speech perception [16,17] . As reviewed above, some fMRI and TMS studies support the view that motor cortical areas are important Research in child development has also shown in speech perception, while clinical and developmental dissociations between speech perception and speech studies have shown significant dissociat</context>
</contexts>
<marker>[38]</marker>
<rawString>B a k e r E , B l u m s t e i n S E , G o o d g l a s s H . I n t e r a c t i o n</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott SK</author>
<author>C McGettigan</author>
<author>F Eisner</author>
</authors>
<title>A little more conversation, between phonological and semantic factors in auditory a little less action-candidate roles for the motor cortex in comprehension. Neuropsychologia</title>
<date>1981</date>
<volume>19</volume>
<pages>1--15</pages>
<contexts>
<context position="18004" citStr="[53]" startWordPosition="2592" endWordPosition="2592">ns between speech perception and speech production under either clinical or developmental conditions are task-load related. organ movement than when they cannot[7,9]. In addition, signals from the motor system help a listener to track a speaker talking over time[51,52]. It has been suggested that one of the functions of motor activation is tracking the talker’s speed and rhythm over time, and provides the timing signals to the auditory cortex. Particularly in a conversation, the monitoring role of the motor system in interacting with the auditory system over time can induce fluent conversation[53]. Under “cocktail-party” conditions, listeners are able to take advantage of certain perceptual cues to facilitate their selective attention to target speech. Selective attention allocates more cognitive resources to the motor representation of speech so that a listener can capture a speaker’s intention and improve speech recognition. Under noise-masking conditions, selective attention affects both active and passive listening. As Alho et al. have reported, attention modulates the magnitude of activation of the left premotor cortex, which influences the performance of phonetic categorization[1</context>
</contexts>
<marker>[53]</marker>
<rawString>Scott SK, McGettigan C, Eisner F. A little more conversation, between phonological and semantic factors in auditory a little less action-candidate roles for the motor cortex in comprehension. Neuropsychologia 1981, 19: 1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bishop</author>
<author>K Mogford-Bevan</author>
</authors>
<title>Language Development in Exceptional Circumstances.</title>
<date>1993</date>
<publisher>Psychology Press</publisher>
<contexts>
<context position="13199" citStr="[39]" startWordPosition="1897" endWordPosition="1897">location of the central sulcus (adapted from involved in speech perception[33]. Meister et al. [33] with permission). However, the results of some clinical studies appear not to support the view that there is an association between impairment of speech perception and impairment of production. Children born with hearing loss can learn speech production. For example, patients with expressive to speak if they gain enough positive somatosensory aphasia exhibit impairment of speech production but not feedback, even though the learning process is much . Also, although harder than in healthy children[39]. Also, children with severe patients with receptive aphasia exhibit impairment of dysarthria are unable to produce meaningful sentences, speech perception and comprehension, they can speak but they can accurately understand spoken content [40]. speech perception or comprehension [36,37] [35] . The dissociations in expressive and receptive Furthermore, infants usually learn to understand speech aphasia support another view that speech perception first and then begin to learn how to produce their own and production are two distinct processes. Moreover, words [41,42] . These studies indicate tha</context>
</contexts>
<marker>[39]</marker>
<rawString>Bishop D, Mogford-Bevan K. Language Development in Exceptional Circumstances. Psychology Press 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bishop CW</author>
<author>Miller LM</author>
</authors>
<title>A multisensory cortical network for understanding speech in noise.</title>
<date>2009</date>
<journal>J Cogn Neurosci</journal>
<volume>10</volume>
<pages>295--302</pages>
<contexts>
<context position="13443" citStr="[40]" startWordPosition="1930" endWordPosition="1930">f speech perception and impairment of production. Children born with hearing loss can learn speech production. For example, patients with expressive to speak if they gain enough positive somatosensory aphasia exhibit impairment of speech production but not feedback, even though the learning process is much . Also, although harder than in healthy children[39]. Also, children with severe patients with receptive aphasia exhibit impairment of dysarthria are unable to produce meaningful sentences, speech perception and comprehension, they can speak but they can accurately understand spoken content [40]. speech perception or comprehension [36,37] [35] . The dissociations in expressive and receptive Furthermore, infants usually learn to understand speech aphasia support another view that speech perception first and then begin to learn how to produce their own and production are two distinct processes. Moreover, words [41,42] . These studies indicate that dissociations patients with lesions in Broca’s area perform well in both between speech perception and speech production word-comprehension and syllable-identification tests, but occur during development. As speech perception and patients wit</context>
<context position="19470" citStr="[40]" startWordPosition="2786" endWordPosition="2786">ts with schizophrenia Speech perception is not just for hearing speech sounds, is more vulnerable to masking stimuli, particularly speechbut more essentially, for recognizing and understanding masking stimuli, than in healthy people[54]. Thus, whether speech signals, requiring that multisensory modalities functional impairments of motor cortical regions contribute interact. In fact, speech understanding and speech hearing to the enhanced vulnerability to speech-masking stimuli in do not share the same brain network, including the motor patients with schizophrenia will be an important research [40] areas . issue in the future. Zhe-Meng Wu, et al. Interaction between auditory and motor systems in speech perception Conclusion 495 [10] Alho J, Sato M, Sams M, Schwartz JL, Tiitinen H, Jääskeläinen IP. Enhanced early-latency electromagnetic This review summarizes the studies showing that activity in the left premotor cortex is associated with interactions between the auditory system and the motor successful phonetic categorization. Neuroimage 2012, 60: system are related to speech perception. The anatomical and functional connections between the auditory and motor systems are important for i</context>
</contexts>
<marker>[40]</marker>
<rawString>Bishop CW, Miller LM. A multisensory cortical network for understanding speech in noise. J Cogn Neurosci 2009, 21: speech perception. Nat Rev Neurosci 2009, 10: 295–302.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C Wu</author>
<author>S Cao</author>
<author>F Zhou</author>
<author>C Wang</author>
<author>X Wu</author>
<author>L Li</author>
</authors>
<title>Masking of speech in people with first-episode schizophrenia and people with chronic schizophrenia.</title>
<booktitle>Schizophr Res 2012a,</booktitle>
<pages>134--33</pages>
<contexts>
<context position="19102" citStr="[54]" startWordPosition="2736" endWordPosition="2736">e of activation of the left premotor cortex, which influences the performance of phonetic categorization[10]. In patients with schizophrenia, both speech-perception Speech Perception under “Cocktail Party” Conditions deficits and increased vulnerability to masking stimuli generally occur. More specifically, speech recognition in both first-episode and chronic patients with schizophrenia Speech perception is not just for hearing speech sounds, is more vulnerable to masking stimuli, particularly speechbut more essentially, for recognizing and understanding masking stimuli, than in healthy people[54]. Thus, whether speech signals, requiring that multisensory modalities functional impairments of motor cortical regions contribute interact. In fact, speech understanding and speech hearing to the enhanced vulnerability to speech-masking stimuli in do not share the same brain network, including the motor patients with schizophrenia will be an important research [40] areas . issue in the future. Zhe-Meng Wu, et al. Interaction between auditory and motor systems in speech perception Conclusion 495 [10] Alho J, Sato M, Sams M, Schwartz JL, Tiitinen H, Jääskeläinen IP. Enhanced early-latency elect</context>
</contexts>
<marker>[54]</marker>
<rawString>Wu C, Cao S, Zhou F, Wang C, Wu X, Li L. Masking of speech in people with first-episode schizophrenia and people with chronic schizophrenia. Schizophr Res 2012a, 134: 33– 41.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>