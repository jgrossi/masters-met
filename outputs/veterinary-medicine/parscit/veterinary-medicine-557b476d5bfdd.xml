<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.753111">
Employing Relative Entropy Techniques for Assessing
Modifications in Animal Behavior
</title>
<author confidence="0.753362">
Minoru Kadota1*, Eric J. White2, Shinsuke Torisawa1, Kazuyoshi Komeyama3, Tsutomu Takagi1
</author>
<sectionHeader confidence="0.301378" genericHeader="abstract">
1 Department of Fisheries, Faculty of Agriculture, Kinki University, Naka-machi, Japan, 2 Department of Physics, University of Cincinnati, Cincinnati, Ohio, United States of
</sectionHeader>
<bodyText confidence="0.556843">
America, 3 Faculty of Fisheries, Kagoshima University, Shimoarata Kagoshima, Japan
</bodyText>
<sectionHeader confidence="0.943992" genericHeader="acknowledgments">
Abstract
</sectionHeader>
<bodyText confidence="0.972347722222222">
In order to make quantitative statements regarding behavior patterns in animals, it is important to establish whether new
observations are statistically consistent with the animal’s equilibrium behavior. For example, traumatic stress from the
presence of a telemetry transmitter may modify the baseline behavior of an animal, which in turn can lead to a bias in
results. From the perspective of information theory such a bias can be interpreted as the amount of information gained
from a new measurement, relative to an existing equilibrium distribution. One important concept in information theory is
the relative entropy, from which we develop a framework for quantifying time-dependent differences between new
observations and equilibrium. We demonstrate the utility of the relative entropy by analyzing observed speed distributions
of Pacific bluefin tuna, recorded within a 48-hour time span after capture and release. When the observed and equilibrium
distributions are Gaussian, we show that the tuna’s behavior is modified by traumatic stress, and that the resulting
modification is dominated by the difference in central tendencies of the two distributions. Within a 95% confidence level,
we find that the tuna’s behavior is significantly altered for approximately 5 hours after release. Our analysis reveals a
periodic fluctuation in speed corresponding to the moment just before sunrise on each day, a phenomenon related to the
tuna’s daily diving pattern that occurs in response to changes in ambient light.
Citation: Kadota M, White EJ, Torisawa S, Komeyama K, Takagi T (2011) Employing Relative Entropy Techniques for Assessing Modifications in Animal
Behavior. PLoS ONE 6(12): e28241. doi:10.1371/journal.pone.0028241
Editor: Enrico Scalas, Universita’ del Piemonte Orientale, Italy
Received July 6, 2011; Accepted November 4, 2011; Published December 2, 2011
Copyright: ß 2011 Kadota et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits
unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
Funding: This research was financially supported by a scientific research grant from Kinki University and the ‘‘International education and research center for
aquaculture science of bluefin tuna and other cultured fish’’, under the Global COE Program; a Grant-in-Aid for Scientific Research (no. 80399097) from the
Ministry of Education, Culture, Science, Sport and Technology, Japan; and a Grant-in-Aid for Research Activity Start-up (no. 21880047) from the Japan Society for
the Promotion of Science. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.
Competing Interests: The authors have declared that no competing interests exist.
* E-mail: kadota@cims.nyu.edu
[3]. James et al. [4] suspected some temporally short-term tagging
effects on leatherback turtles at sea, and thus excluded from their
data all results from the first week of tagging. In order to ensure
that certain species of fish have properly recovered from the effects
of anesthetics, attachment procedure, and transmitter presence,
some studies have suggested that researchers would be well advised
to exercise caution when analyzing data collected within the first
twenty-four hours of transmitter attachment [5]. However, when it
comes to quantifying the effects of external stress on animal
behavior, no sophisticated or sufficiently quantitative methods
have yet been established. Thus we turn to the question: how can
one quantitatively distinguish the difference between stressinduced, non-equilibrium distributions and those of normal
behavior ?
Often it is the case that an equilibrium (reference) distribution is
constructed from past records of observation. Upon making a new
measurement, one is generally interested in the amount of
information gained from the measured (observed) distribution.
However, when the newly observed distribution does not differ
significantly from the reference distribution, no meaningful
information is gained. In this case, the inaccessibility of new
information serves not as a statement about any inherent utility of
the newly measured distribution, but rather that the observed
distribution does not significantly differ from the reference
distribution.
Introduction
Understanding the movement patterns of animals is crucial for
their proper management and conservation. In particular, the
analysis of telemetry data provides valuable insight into the
movement, stock structure, and environmental preferences of
individually tagged animals. In order to properly understand the
relationship between an animal’s behavior and its environment, it
is essential that researchers determine the possible effects that
transmitter attachment and presence can have on equilibrium
behavior and physiology. For instance, the attachment of a
transmitter can induce stress in the animal, thereby interrupting its
normal foraging behavior. In such cases the speed of an animal
may be a good indicator of non-equilibrium behavior. Measurements of speed distributions for newly tagged animals can be
significantly different from speed distributions under normal
behavior. However, mild stress may not always be reflected by
statistically significant changes in the speed, even though an
observer confidently asserts that the animal is not behaving
normally.
The key for a successful study of an animal’s behavior is to
ensure that any data used for analysis is indicative of its natural
behavior. Distress from capture, along with the physiological
impacts due to the attachment and presence of a transmitter, can
result in stress that modifies an animal’s baseline behavior [1], [2],
</bodyText>
<equation confidence="0.832851666666667">
PLoS ONE  |www.plosone.org
1
December 2011  |Volume 6  |Issue 12  |e28241
</equation>
<bodyText confidence="0.997423781818182">
Relative Entropy on Animal Behavior
triangle inequality, these relations are satisfied to a good
approximation for P?Q.
A useful criterion in the analysis of empirical data is that the
results not be dependent on the coordinate system used to describe
a particular behavioral pattern, which often depends upon a
choice of metric. A powerful feature of the relative entropy is that
it is invariant under a change of coordinate systems. Consider a
smooth, invertible transformation from x to y described by
the function y~w(x). Since the relation D(p(x)jjq(x))~
D(pw (y)jjqw (y)) is always satisfied for such a re-parameterization,
the relative entropy remains invariant under coordinate transformations [8], [19]. Thus, we are guaranteed that the difference
between two probability distributions is always described by a
single measure, regardless of the coordinate system. We further
examine the significance of coordinate invariance as it applies
specifically to the case of telemetry data in the Discussion section.
There are currently several test statistics which are used to
quantify the similarities between two distributions, including the tand F-tests for Gaussian distributions, and the KolmogorovSmirnov test for generalized distributions [6]. Bayesian methods
often employ null hypothesis testing, an approach that has been
criticized for its inherent subjectivity and emphasis on decisionmaking statistics [7].
The purpose of this paper is to introduce the readers to the use
of relative entropy techniques as a method of quantifying timedependent differences between observed data and equilibrium.
Relative entropy techniques are robust, compelling, and can be
applied to many physical situations. Since relative entropy is
sensitive to the higher-order moments of a distribution, and not
just changes in the mean and variance, it has the major advantage
of more completely capturing probabilistic information [8]. For
example, relative entropy techniques can be used to detect a
divergence between an observed distribution and equilibrium that
may not affect low-order moments, such as a time-skewness
introduced by difficulties in detection. Although the relative
entropy is not a true metric in the mathematical sense, another
useful property is that it can be intuited as an effective distance
between two probability distributions, in the sense that 1) it is
always positive, 2) it is zero if and only if the two distributions are
identical, and 3) it increases as the distributions diverge [9], [10].
Despite the concern raised by the increased use of telemetry
techniques, very little is known about the effects of tagging devices,
and even less is known about the effects on fishes [11], [12], [13],
[14], [15]. In this paper we introduce relative entropy techniques
as a method for assessing factors that can influence and modify
animal behavior. In the Methods section we define relative
entropy, give a brief overview of its properties as applied to
generalized probability distributions, then specialize the definition
for the case of Gaussian distributions. In the Results section we
analyze the effects that transmitter attachment and presence can
have on the behavior of Pacific bluefin tuna. In the final section we
conclude with a discussion of the results.
Relative Entropy for Gaussian Distributions
An analytical expression may be obtained for the relative
entropy in the case that P(x) and Q(x) are Gaussian. Let us
assume that the first and second moments of these distributions are
denoted by mp , sp , mq , and sq , respectively. Given the standard
form of a Gaussian distribution [20], it is straightforward to show
that the relative entropy takes the form
</bodyText>
<equation confidence="0.461424666666667">
&quot;    
#
2
sq 2
sp 2
1
1 (mp {mq )
D(PjjQ)~ log
z
{1 z
2
2
2 sq
sp
sq
</equation>
<bodyText confidence="0.999565384615384">
Notice that the relative entropy can be decomposed into a set of
uncorrelated components. The term in square brackets reflects any
difference in the variances between the two distributions, and is
often referred to as the dispersion component [8], [19]. When the
variance of the observed distribution is small compared to that of
the reference distribution, the relative entropy is dominated by this
first term. In this case, the dispersion represents the reduction in
uncertainty of the random variables as a result of the observation
process.
Alternatively, when the means of the two distributions are large
relative to the variance of the reference distribution, the relative
entropy is dominated by the last term, often referred to as the
signal component. The significance of the signal term can be
better understood with the help of concrete example. Suppose that
the mean speed for an oceanic bluefin tuna is 0.8 m/s, with a
variation of 0.1 m/s. A new observation yields a measured speed
of 1.2 m/s, with a variation of 0.1 m/s. Clearly, the utility of this
new observation derives not from any improvement in the
variation, since they are both equal to 0.1 m/s, but rather from
a shift in the mean value. Assuming that both distributions are
Gaussian, we can use Equation (2) to compute the relative entropy.
Since only the signal term contributes to the relative entropy in
this case, we can plug the given speeds and variance directly into
the second term to get a value of 8 nats (logarithmic units). Thus,
the distance between the two distributions is composed entirely by
the difference in their central tendencies.
</bodyText>
<subsectionHeader confidence="0.8574365">
Methods
Relative Entropy
</subsectionHeader>
<bodyText confidence="0.978738090909091">
When performing a statistical data analysis, one often wishes to
know by how much two probability distributions differ from each
other. In information theory, the most common measure for doing
this is the relative entropy. Consider a random variable x with a
probability distribution function of Q(x). Following some
measurement, we revise our estimate from Q(x) to P(x). The
change in the probability function represents a measure of the
amount of information introduced as a result of the measurement.
The relative entropy D(PjQ) quantifies the change in information
as an effective distance between the two probability distributions,
given by
</bodyText>
<equation confidence="0.709678272727273">
?
ð
D(PjQ)~


P(x)
P(x):log
dx
Q(x)
ð1Þ
{?
</equation>
<subsectionHeader confidence="0.583184">
Ethics Statement
</subsectionHeader>
<bodyText confidence="0.998302285714286">
There exist many excellent reviews of the relative entropy in
literature [10], [16], [17], [18]. Although this parameter, also
known as the Kullback-Leibler divergence, is not a true metric, it
serves as an effective distance between the distributions P(x) and
Q(x). The relative entropy is always non-negative, and vanishes if
and only if P = Q for all x. Although the relative entropy is not
symmetric under the exchange of P and Q, nor does it satisfy the
</bodyText>
<equation confidence="0.947734">
PLoS ONE  |www.plosone.org
ð2Þ
</equation>
<bodyText confidence="0.832921">
This study (No. 2010-26) is conducted with approval by the
Faculty of Agriculture, Kinki University, located in HigashiOsaka, Japan. All experiments were conducted in accordance with
Japanese Governmental law (No. 105), as well as the guidelines
published by the Science Council of Japan concerning the
appropriate treatment of animals in life science research.
</bodyText>
<figure confidence="0.983896333333333">
2
December 2011  |Volume 6  |Issue 12  |e28241
Relative Entropy on Animal Behavior
&quot; 
#

 
2
sq 2
sp (tj ) 2
1
1 (mp (tj ){mq )
D(tj )~ log
z
{1 z
, ð3Þ
2
2
sp (tj )
s2q
sq
</figure>
<subsubsectionHeader confidence="0.797387">
Procedure
</subsubsectionHeader>
<bodyText confidence="0.99613767032967">
We use data collected from an experiment conducted in the
waters offshore of Kochi Prefecture, Japan. Three Pacific bluefin
tuna were captured within a submerged net-cage with a diameter
of 50 meters, then subsequently tagged with a data-logger package
consisting of a data-logger and recovery system. The data-logger
package is surgically attached to the right side of the body below
the anterior lobe of the dorsal fin, using two plastic attachment
wires connected to a time-release mechanism. The tags are affixed
via an attachment plate aligned along the lateral line of the tuna’s
body. After two hypodermic needles are pushed through the dorsal
musculature, a plastic wire is used to secure the attachment plate
in place. Data collection begins when the tuna are returned to the
submerged net-cage. Due to limited memory capacity and battery
life, the tuna’s speed was recorded at uniform 1-second intervals
for a continuous span of forty-eight hours.
In order to ensure that any telemetry data collected in this
experiment is indicative of the tuna’s natural behavior, it is
essential to quantify any physiological impacts incurred by the
capture and attachment of the data-logger package. Since the
speed of a bluefin tuna serves as a good criterion for discriminating
non-equilibrium behavior, we apply relative entropy techniques to
quantify the effects of stress by measuring the difference between
the speed distributions of newly tagged tuna and equilibrium. The
procedure is as follows: first, all time series corresponding to steplengths measured at 1-second intervals are collected into bins of
10-minute intervals. For each bin tj we compute the probability
density function P(xi ,tj ), where xi is the step-length and tj is the
time corresponding to the j-th interval. The reference distribution
Q(xi ) is then formed by averaging P(xi ,tj ) over all time intervals.
Assuming the data is normally distributed, we perform a chisquared goodness-of-fit test using the null hypothesis. Since the
null hypothesis cannot be rejected to a significance level of 5%, we
suppose that P(x,tj ) and Q(x) are Gaussian distributions of
dimension one.
In order to definitively state that a tuna’s behavior is no longer
affected by trauma, we must establish at which point in time an
observed data set is statistically indistinguishable from the
reference distribution. A t-test is commonly employed to determine
if the mean values of the observed and reference distributions are
statistically consistent. In Appendix S1, we discuss the connection
between the t-test and the signal component of the relative entropy
for Gaussian distributions. That such a relation exists should come
as little surprise: any difference between the mean values of an
observed and reference distribution will contribute to the overall
‘‘distance’’ between them, which in turn establishes the amount of
information provided by the observation. For the concrete
example provided above, it is clear that the signal component of
the relative entropy vanishes when the two means are equal.
where s and m are the standard deviation and mean value of the
step length, and the subscripts p and q correspond to the observed
and reference distributions, respectively. The values of D(tj ) for a
single tuna released after surgery are plotted in Figure 2. Though
the dispersion term contributes little to the overall distance D(tj )
the difference in the mean values between the distributions leads to
a dominant contribution to D(tj ) from the signal term. Since the
step-length is directly correlated with the tuna’s speed, one could
infer that behavior of a newly tagged tuna is considerably altered
due to various factors of stress.
Also evident in Figure 2 is a periodic increase in the tuna’s
speed, at times corresponding to the moment just before sunrise on
each day. Further inspection reveals that these signal spikes are
offset by about 20 minutes to the earlier side of sunrise, a result
that is consistent with a previous study [21]. In the latter study, it
was discovered that both the deepest portion of the dives and the
most rapid changes in depth are precisely timed with respect to
sunrise, with spike dives occurring at times corresponding to a sun
elevation of about 30 minutes before sunrise.
In Figure 3 we plot the signal component of the relative entropy
for three tuna fish. Large values of this term indicate that the
tuna’s behavior is considerably modified by distress from capture
and the physiological impacts of the attachments. As the tuna
recovers from the distress of capture and stress, the signal term
gradually returns to zero as the observed distribution converges
with equilibrium. In our case, the tuna’s behavior appears to
recover approximately 4–6 hours after release.
In order to demonstrate that a tuna’s behavior is no longer
affected by trauma, we must establish the point at which the
observed distribution is the ‘‘same’’ as the reference distribution.
Although the similarity of any two distributions is arbitrary, the
previously discussed t-test is often used to establish when two
distributions are indistinguishable, for example to a 95%
confidence level. The concept of distance, as defined by the
relative entropy, can also be applied as a means of distinguishing
the similarity of the two distributions. In Figure 4 we plot the
relative entropy in bins of 30-second intervals over the first seven
hours, along with a calculation of the t-test for comparison. As
discussed in Appendix S1, we solve the relative entropy for a tvalue of t~2:04, which corresponds to a confidence level of 95%
for a sample with 30 degrees of freedom. We find that the
distributions are indistinguishable when the relative entropy
reaches a value of 0.069, first obtained after 5.1 hours as shown
in Figure 4A. According to the t-test, the observed and reference
distributions become indistinguishable after 4.9 hours, shown in
Figure 4B.
</bodyText>
<subsectionHeader confidence="0.969879">
Results
</subsectionHeader>
<bodyText confidence="0.988941909090909">
We begin by comparing a newly observed speed distribution
P(xi ,tj ) for a single bluefin tuna with that of a reference
distribution Q(xi ), obtained by averaging P(xi ,tj ) over all time
intervals. In both cases, the distributions correspond to steplengths measured at 1-second intervals and collected into bins of
10-minute intervals, as described above in the Methods section. In
Figure 1 we show the observed probability distribution function for
P(xi ,tj ) calculated in each bin tj , along with the re-averaged
reference distribution Q(xi ). It is clear that the observed
distribution eventually approaches the reference distribution over
time. In this particular case the distance D(tj ) between the
observed and reference distributions takes the form
</bodyText>
<equation confidence="0.585944">
PLoS ONE  |www.plosone.org
</equation>
<subsubsectionHeader confidence="0.710886">
Discussion
</subsubsectionHeader>
<bodyText confidence="0.9994609">
We show how methods from information theory can be used to
quantify the gain in information provided by a newly measured
observation, relative to a known reference distribution. This
measure, which serves as an effective distance between the
observed and reference distributions, is called the relative entropy.
To demonstrate the utility of these methods, we have analyzed the
speed distributions of Pacific bluefin tuna over a 48-hour time span
after capture and release. The reference distribution, which serves
as a model of the tuna’s baseline behavior, was constructed by
averaging the probability density function of step-lengths over all
</bodyText>
<page confidence="0.750762">
3
</page>
<figure confidence="0.7255695">
December 2011  |Volume 6  |Issue 12  |e28241
Relative Entropy on Animal Behavior
</figure>
<figureCaption confidence="0.996807">
Figure 1. Behavior of observed step-length distributions over time. Observed (black) and reference (red) probability density functions of the
</figureCaption>
<bodyText confidence="0.966352909090909">
step length P(x,tj ) and Q(x) at various times. Observed distributions P(x,tj ) were calculated at intervals of 10 minutes, 30 minutes, 1 hour, 4 hours,
and 6 hours after release. This figure demonstrates how the observed distributions P(x,tj ) approaches the reference distribution Q(x) over time.
doi:10.1371/journal.pone.0028241.g001
recorded time intervals. The speed distributions, when measured
immediately after a tuna’s release, provide a means of observing
stress-induced fluctuations in behavior. The departure of the
observed behavior from baseline behavior was assessed by
computing the relative entropy of the distributions, from which
we discovered that the tuna’s behavior is clearly modified by the
process of tagging and release. In this case, the resulting
modification in behavior is due primarily to a difference in the
</bodyText>
<figureCaption confidence="0.987795833333333">
Figure 2. Relative entropy distribution after release. Relative
entropy D(tj ) (black), signal component (red), and dispersion component (blue), plotted versus hours after release. The periodic increases in
the signal component correspond to the moment just before sunrise on
each day.
doi:10.1371/journal.pone.0028241.g002
Figure 3. Comparison of relative entropy distributions. Signal
</figureCaption>
<bodyText confidence="0.8199185">
components of the relative entropy for three bluefin tuna. In each case
the behavior is significantly altered for approximately 4–6 hours.
</bodyText>
<figure confidence="0.9650686">
doi:10.1371/journal.pone.0028241.g003
PLoS ONE  |www.plosone.org
4
December 2011  |Volume 6  |Issue 12  |e28241
Relative Entropy on Animal Behavior
</figure>
<figureCaption confidence="0.999675">
Figure 4. Comparison of relative entropy and t-test distributions. Relative entropy (top), calculated over the first seven hours after release.
</figureCaption>
<bodyText confidence="0.992559571428572">
We determine that the distributions are indistinguishable when the relative entropy reaches a value of 0.069 (red horizontal line), first obtained after
5.1 hours (top). The t-test (bottom), can be used to determine that the observed and reference distributions are indistinguishable after 4.9 hours, to a
95% confidence level.
doi:10.1371/journal.pone.0028241.g004
As mentioned in the Methods section, the relative entropy is
invariant under a change of coordinate systems. To see why this is
important, note that the step-length distributions used in this
analysis are measured in units of distance. In order to make a
quantitative statement about the speed of the tuna, we must in
principle perform a coordinate transformation from one basis of
units to another. However, since the relative entropy is invariant
under such transformations, we are guaranteed that the results in
the new basis are identical.
Relative entropy techniques can be used to study behavior
patterns that are modified by other factors, such as water
temperature, exposure to sunlight, etc. For example, in this study
we discovered a periodic variation in the tuna’s speed corresponding to the moment before sunrise on each day. Most likely such a
fluctuation in the tuna’s diving pattern is due to changes in
ambient light during sunrise, a hypothesis supported by evidence
from other analyses. In captive bluefin tuna, it was observed a high
mortality of juveniles as a result of the fish buffeting the tank and
net-pen at sunrise [22]. In the previous study it was also found that
this phenomenon is caused by visual disorientation due to an
incompatibility of the retina to adapt to changes in ambient light
intensity. Kitagawa et al. [23] analyzed time-series data for depth,
and reported that bluefin tuna display distinct patterns in their
vertical movement at sunrise and sundown. It has also been
reported that juvenile bluefin tuna make sharp descents and
ascents, called spike dives, around sunrise and sunset each day
[21]. Willis found that these spike dives are offset by about
30 minutes on the darker side of each sunrise or sunset, which is
consistent with the results of our analysis.
There is an abundance of opportunities in which relative
entropy techniques can be applied. The relative entropy is a robust
central tendencies of the two distributions, and thus the relative
entropy is dominated by the signal term. We found that the
modified behavior regresses to the baseline behavior after
approximately 5 hours, corresponding to the first bin in which
the observed distribution becomes indistinguishable from equilibrium to a 95% confidence level.
In this analysis, the relative entropy was calculated on the
assumption that both the observed and reference distributions are
described by Gaussian functions. For the case of Gaussian
distributions, the relative entropy can be decomposed into a
dispersion and signal component, the former of which depends on
the variance of the distributions, the latter of which depends only
on their mean value. Since the relative entropy is dominated by a
difference in the mean values of each distribution, rather than a
difference in their variances, it should be noted that our t-test was
carried out explicitly for the case of two distributions with equal
variances. Since the relative entropy captures information from all
higher-order moments, it is no surprise that our result of 5.1 hours
is slightly larger than the value of 4.9 hours determined from the ttest alone. In fact, a simple example serves to illustrates why the
relative entropy is a more powerful test statistic in general: suppose
one observes a single variable with zero variance, while the
accompanying reference distribution for this variable has the exact
same mean value with unit variance. In this case, the t-value for
these two distributions is zero, and thus the signal component is
also zero. Yet the behavior of the observed variable is considerably
different from that of the reference distribution, namely because it
possesses zero variance while the latter does not. The significance
of the relative entropy is apparent: in order to accurately measure
the difference between two distributions, it is necessary to include
higher-order moments.
</bodyText>
<figure confidence="0.72704025">
PLoS ONE  |www.plosone.org
5
December 2011  |Volume 6  |Issue 12  |e28241
Relative Entropy on Animal Behavior
</figure>
<bodyText confidence="0.969020333333333">
and powerful method for quantifying time-dependent differences
between observed data and equilibrium. Although the techniques
introduced in this analysis were developed specifically for the case
of Gaussian distributions, the authors soon hope to demonstrate
the utility of relative entropy techniques in the context of
generalized non-Gaussian distributions.
</bodyText>
<figure confidence="0.8908652">
Supporting Information
Appendix S1 Relation Between t-value and Relative
Entropy.
(DOC)
Author Contributions
</figure>
<bodyText confidence="0.417717333333333">
Conceived and designed the experiments: ST TT KK. Performed the
experiments: ST KK TT. Analyzed the data: MK. Contributed reagents/
materials/analysis tools: MK EJW. Wrote the paper: MK EJW.
</bodyText>
<sectionHeader confidence="0.731772" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998501888888889">
12. U.S. National Research Council (2000) Marine mammals and low-frequency
Sound: progress since 1994. WashingtonDC: National Academies Press.
13. U.S. National Research Council (2003) Ocean Noise and Marine Mammals.
WashingtonDC: National Academies Press.
14. U.S. National Research Council (2005) Marine mammal populations and ocean
noise: determining when noise causes biologically significant effects. WashingtonDC: National Academies Press.
15. Popper AN, Fewtrell J, Smith ME, McCauley RD (2003) Anthropogenic sound:
Effects on the behavior and physiology of fishes. Marine Technology Society
Journal 37: 35–40.
16. Shannon C (1948) A mathematical theory of communication. Bell System
Technical Journal 27: 370–423, 623–656.
17. Goldman S (1953) Information Theory. New York: Prentice Hall.
18. Reza FM (1961) An introduction to information theory. New York: MacGrawHill.
19. Majda A, Kleeman R, Cai D (2002) A mathematical framework for quantifying
predictability through relative entropy. Methods of Applied Analysis 9: 425–444.
20. Gardiner CW (1990) Handbook of Stochastic Methods. Berlin: Springer.
21. Willis J, Phillips J, Muheim R, Diego-Rasilla FJ, Hobday AJ (2009) Spike dives of
juvenile southern bluefin tuna (Thunnus maccoyii): a navigational role?
Behavioral Ecology and Sociobiology 64: 57–68.
22. Masuma S, Kawamura G, Tezuka N, Koiso M, Namba K (2001) Retinomotor
responses of juvenile bluefin tuna Thunnus thynnus. Fisheries Science 67:
228–231.
23. Kitagawa T, Kimura S, Nakata H, Yamada H (2004) Diving behavior of
immature, feeding Pacific bluefin tuna (Thunnus thynnus orientalis) in relation
to season and area: the East China Sea and the Kuroshio-Oyashio transition
region. Fisheries Oceanography 13: 161–180.
1. Godfrey J, Bryant D (2003) Effects of radio transmitters: review of recent radiotracking studies. Conservation applications of measuring energy expenditure of
New Zealand birds: Assessing habitat quality and costs of carrying radio
transmitters: Department of Conservation. pp 83–95.
2. Hawkins P (2004) Bio-logging and animal welfare: practical refinements.
Memoirs of National Institute of Polar Research 58: 58–68.
3. Wilson RP, McMahon CR (2006) Measuring devices on wild animals: what
constitutes acceptable practice? Frontiers in Ecology and the Environment 4:
147–154.
4. James MC, Ottensmeyer CA, Eckert SA, Myers RA (2006) Changes in diel
diving patterns accompany shifts between northern foraging and southward
migration in leatherback turtles. Canadian Journal of Zoology 84: 754–765.
5. Bridger CJ, Booth RK (2003) The effects of biotelemetry transmitter presence
and attachment procedures on fish physiology and behavior. Reviews in Fishery
Sciences 11: 13–34.
6. Sardeshmukh PD, Compo GP, Penland C (2000) Changes of probability
associated with El Nino˜. Journal of Climate 13: 4268–4286.
7. Berger JO (1985) Statistical decision theory and Bayesian analysis. New York:
Springer.
8. Kleeman R (2002) Measuring dynamical prediction utility using relative
entropy. Journal of Atmospheric Science 59: 2057–2072.
9. Shannon CE, Weaver W (1949) The mathematical theory of communication.
Urbana: University of Illinois Press.
10. Cover TM, Thomas JA (1991) Elements of information theory. New York:
Wiley.
11. U.S. National Research Council (1994) Low-frequency sound and marine
mammals: Current knowledge and research need. WashingtonDC: National
Academies Press.
PLoS ONE  |www.plosone.org
</reference>
<page confidence="0.902447">
6
</page>
<reference confidence="0.43350125">
December 2011  |Volume 6  |Issue 12  |e28241
Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or
emailed to multiple sites or posted to a listserv without the copyright holder&amp;apos;s express written permission.
However, users may print, download, or email articles for individual use.
</reference>
</variant>
</algorithm>

<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>U S National</author>
</authors>
<title>Research Council</title>
<date>2000</date>
<publisher>National Academies Press.</publisher>
<contexts>
<context position="9026" citStr="[12]" startWordPosition="1320" endWordPosition="1320">ewness introduced by difficulties in detection. Although the relative entropy is not a true metric in the mathematical sense, another useful property is that it can be intuited as an effective distance between two probability distributions, in the sense that 1) it is always positive, 2) it is zero if and only if the two distributions are identical, and 3) it increases as the distributions diverge [9], [10]. Despite the concern raised by the increased use of telemetry techniques, very little is known about the effects of tagging devices, and even less is known about the effects on fishes [11], [12], [13], [14], [15]. In this paper we introduce relative entropy techniques as a method for assessing factors that can influence and modify animal behavior. In the Methods section we define relative entropy, give a brief overview of its properties as applied to generalized probability distributions, then specialize the definition for the case of Gaussian distributions. In the Results section we analyze the effects that transmitter attachment and presence can have on the behavior of Pacific bluefin tuna. In the final section we conclude with a discussion of the results. Relative Entropy for Gaus</context>
</contexts>
<marker>12.</marker>
<rawString>U.S. National Research Council (2000) Marine mammals and low-frequency Sound: progress since 1994. WashingtonDC: National Academies Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U S National</author>
</authors>
<title>Research Council</title>
<date>2003</date>
<publisher>National Academies Press.</publisher>
<contexts>
<context position="9032" citStr="[13]" startWordPosition="1321" endWordPosition="1321"> introduced by difficulties in detection. Although the relative entropy is not a true metric in the mathematical sense, another useful property is that it can be intuited as an effective distance between two probability distributions, in the sense that 1) it is always positive, 2) it is zero if and only if the two distributions are identical, and 3) it increases as the distributions diverge [9], [10]. Despite the concern raised by the increased use of telemetry techniques, very little is known about the effects of tagging devices, and even less is known about the effects on fishes [11], [12], [13], [14], [15]. In this paper we introduce relative entropy techniques as a method for assessing factors that can influence and modify animal behavior. In the Methods section we define relative entropy, give a brief overview of its properties as applied to generalized probability distributions, then specialize the definition for the case of Gaussian distributions. In the Results section we analyze the effects that transmitter attachment and presence can have on the behavior of Pacific bluefin tuna. In the final section we conclude with a discussion of the results. Relative Entropy for Gaussian D</context>
</contexts>
<marker>13.</marker>
<rawString>U.S. National Research Council (2003) Ocean Noise and Marine Mammals. WashingtonDC: National Academies Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U S National</author>
</authors>
<title>Research Council</title>
<date>2005</date>
<publisher>National Academies Press.</publisher>
<contexts>
<context position="9038" citStr="[14]" startWordPosition="1322" endWordPosition="1322">duced by difficulties in detection. Although the relative entropy is not a true metric in the mathematical sense, another useful property is that it can be intuited as an effective distance between two probability distributions, in the sense that 1) it is always positive, 2) it is zero if and only if the two distributions are identical, and 3) it increases as the distributions diverge [9], [10]. Despite the concern raised by the increased use of telemetry techniques, very little is known about the effects of tagging devices, and even less is known about the effects on fishes [11], [12], [13], [14], [15]. In this paper we introduce relative entropy techniques as a method for assessing factors that can influence and modify animal behavior. In the Methods section we define relative entropy, give a brief overview of its properties as applied to generalized probability distributions, then specialize the definition for the case of Gaussian distributions. In the Results section we analyze the effects that transmitter attachment and presence can have on the behavior of Pacific bluefin tuna. In the final section we conclude with a discussion of the results. Relative Entropy for Gaussian Distrib</context>
</contexts>
<marker>14.</marker>
<rawString>U.S. National Research Council (2005) Marine mammal populations and ocean noise: determining when noise causes biologically significant effects. WashingtonDC: National Academies Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Popper AN</author>
<author>J Fewtrell</author>
<author>Smith ME</author>
<author>McCauley RD</author>
</authors>
<title>Anthropogenic sound: Effects on the behavior and physiology of fishes.</title>
<date>2003</date>
<journal>Marine Technology Society Journal</journal>
<volume>37</volume>
<pages>35--40</pages>
<contexts>
<context position="9044" citStr="[15]" startWordPosition="1323" endWordPosition="1323">by difficulties in detection. Although the relative entropy is not a true metric in the mathematical sense, another useful property is that it can be intuited as an effective distance between two probability distributions, in the sense that 1) it is always positive, 2) it is zero if and only if the two distributions are identical, and 3) it increases as the distributions diverge [9], [10]. Despite the concern raised by the increased use of telemetry techniques, very little is known about the effects of tagging devices, and even less is known about the effects on fishes [11], [12], [13], [14], [15]. In this paper we introduce relative entropy techniques as a method for assessing factors that can influence and modify animal behavior. In the Methods section we define relative entropy, give a brief overview of its properties as applied to generalized probability distributions, then specialize the definition for the case of Gaussian distributions. In the Results section we analyze the effects that transmitter attachment and presence can have on the behavior of Pacific bluefin tuna. In the final section we conclude with a discussion of the results. Relative Entropy for Gaussian Distributions</context>
</contexts>
<marker>15.</marker>
<rawString>Popper AN, Fewtrell J, Smith ME, McCauley RD (2003) Anthropogenic sound: Effects on the behavior and physiology of fishes. Marine Technology Society Journal 37: 35–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Shannon</author>
</authors>
<title>A mathematical theory of communication.</title>
<date>1948</date>
<journal>Bell System Technical Journal</journal>
<volume>27</volume>
<pages>370--423</pages>
<contexts>
<context position="12527" citStr="[16]" startWordPosition="1905" endWordPosition="1905">doing this is the relative entropy. Consider a random variable x with a probability distribution function of Q(x). Following some measurement, we revise our estimate from Q(x) to P(x). The change in the probability function represents a measure of the amount of information introduced as a result of the measurement. The relative entropy D(PjQ) quantifies the change in information as an effective distance between the two probability distributions, given by ? ð D(PjQ)~   P(x) P(x):log dx Q(x) ð1Þ {? Ethics Statement There exist many excellent reviews of the relative entropy in literature [10], [16], [17], [18]. Although this parameter, also known as the Kullback-Leibler divergence, is not a true metric, it serves as an effective distance between the distributions P(x) and Q(x). The relative entropy is always non-negative, and vanishes if and only if P = Q for all x. Although the relative entropy is not symmetric under the exchange of P and Q, nor does it satisfy the PLoS ONE |www.plosone.org ð2Þ This study (No. 2010-26) is conducted with approval by the Faculty of Agriculture, Kinki University, located in HigashiOsaka, Japan. All experiments were conducted in accordance with Japanese Go</context>
</contexts>
<marker>16.</marker>
<rawString>Shannon C (1948) A mathematical theory of communication. Bell System Technical Journal 27: 370–423, 623–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldman</author>
</authors>
<title>Information Theory.</title>
<date>1953</date>
<publisher>Prentice Hall.</publisher>
<location>New York:</location>
<contexts>
<context position="12533" citStr="[17]" startWordPosition="1906" endWordPosition="1906">this is the relative entropy. Consider a random variable x with a probability distribution function of Q(x). Following some measurement, we revise our estimate from Q(x) to P(x). The change in the probability function represents a measure of the amount of information introduced as a result of the measurement. The relative entropy D(PjQ) quantifies the change in information as an effective distance between the two probability distributions, given by ? ð D(PjQ)~   P(x) P(x):log dx Q(x) ð1Þ {? Ethics Statement There exist many excellent reviews of the relative entropy in literature [10], [16], [17], [18]. Although this parameter, also known as the Kullback-Leibler divergence, is not a true metric, it serves as an effective distance between the distributions P(x) and Q(x). The relative entropy is always non-negative, and vanishes if and only if P = Q for all x. Although the relative entropy is not symmetric under the exchange of P and Q, nor does it satisfy the PLoS ONE |www.plosone.org ð2Þ This study (No. 2010-26) is conducted with approval by the Faculty of Agriculture, Kinki University, located in HigashiOsaka, Japan. All experiments were conducted in accordance with Japanese Governme</context>
</contexts>
<marker>17.</marker>
<rawString>Goldman S (1953) Information Theory. New York: Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reza FM</author>
</authors>
<title>An introduction to information theory.</title>
<date>1961</date>
<publisher>MacGrawHill.</publisher>
<location>New York:</location>
<contexts>
<context position="12539" citStr="[18]" startWordPosition="1907" endWordPosition="1907">s the relative entropy. Consider a random variable x with a probability distribution function of Q(x). Following some measurement, we revise our estimate from Q(x) to P(x). The change in the probability function represents a measure of the amount of information introduced as a result of the measurement. The relative entropy D(PjQ) quantifies the change in information as an effective distance between the two probability distributions, given by ? ð D(PjQ)~   P(x) P(x):log dx Q(x) ð1Þ {? Ethics Statement There exist many excellent reviews of the relative entropy in literature [10], [16], [17], [18]. Although this parameter, also known as the Kullback-Leibler divergence, is not a true metric, it serves as an effective distance between the distributions P(x) and Q(x). The relative entropy is always non-negative, and vanishes if and only if P = Q for all x. Although the relative entropy is not symmetric under the exchange of P and Q, nor does it satisfy the PLoS ONE |www.plosone.org ð2Þ This study (No. 2010-26) is conducted with approval by the Faculty of Agriculture, Kinki University, located in HigashiOsaka, Japan. All experiments were conducted in accordance with Japanese Governmental l</context>
</contexts>
<marker>18.</marker>
<rawString>Reza FM (1961) An introduction to information theory. New York: MacGrawHill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Majda</author>
<author>R Kleeman</author>
<author>D Cai</author>
</authors>
<title>A mathematical framework for quantifying predictability through relative entropy.</title>
<date>2002</date>
<journal>Methods of Applied Analysis</journal>
<volume>9</volume>
<pages>425--444</pages>
<contexts>
<context position="7024" citStr="[19]" startWordPosition="1017" endWordPosition="1017">ion for P?Q. A useful criterion in the analysis of empirical data is that the results not be dependent on the coordinate system used to describe a particular behavioral pattern, which often depends upon a choice of metric. A powerful feature of the relative entropy is that it is invariant under a change of coordinate systems. Consider a smooth, invertible transformation from x to y described by the function y~w(x). Since the relation D(p(x)jjq(x))~ D(pw (y)jjqw (y)) is always satisfied for such a re-parameterization, the relative entropy remains invariant under coordinate transformations [8], [19]. Thus, we are guaranteed that the difference between two probability distributions is always described by a single measure, regardless of the coordinate system. We further examine the significance of coordinate invariance as it applies specifically to the case of telemetry data in the Discussion section. There are currently several test statistics which are used to quantify the similarities between two distributions, including the tand F-tests for Gaussian distributions, and the KolmogorovSmirnov test for generalized distributions [6]. Bayesian methods often employ null hypothesis testing, an</context>
<context position="10333" citStr="[19]" startWordPosition="1546" endWordPosition="1546">(x) and Q(x) are Gaussian. Let us assume that the first and second moments of these distributions are denoted by mp , sp , mq , and sq , respectively. Given the standard form of a Gaussian distribution [20], it is straightforward to show that the relative entropy takes the form &quot;     # 2 sq 2 sp 2 1 1 (mp {mq ) D(PjjQ)~ log z {1 z 2 2 2 sq sp sq Notice that the relative entropy can be decomposed into a set of uncorrelated components. The term in square brackets reflects any difference in the variances between the two distributions, and is often referred to as the dispersion component [8], [19]. When the variance of the observed distribution is small compared to that of the reference distribution, the relative entropy is dominated by this first term. In this case, the dispersion represents the reduction in uncertainty of the random variables as a result of the observation process. Alternatively, when the means of the two distributions are large relative to the variance of the reference distribution, the relative entropy is dominated by the last term, often referred to as the signal component. The significance of the signal term can be better understood with the help of concrete exam</context>
</contexts>
<marker>19.</marker>
<rawString>Majda A, Kleeman R, Cai D (2002) A mathematical framework for quantifying predictability through relative entropy. Methods of Applied Analysis 9: 425–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gardiner CW</author>
</authors>
<title>Handbook of Stochastic Methods.</title>
<date>1990</date>
<publisher>Springer.</publisher>
<location>Berlin:</location>
<contexts>
<context position="9935" citStr="[20]" startWordPosition="1467" endWordPosition="1467">specialize the definition for the case of Gaussian distributions. In the Results section we analyze the effects that transmitter attachment and presence can have on the behavior of Pacific bluefin tuna. In the final section we conclude with a discussion of the results. Relative Entropy for Gaussian Distributions An analytical expression may be obtained for the relative entropy in the case that P(x) and Q(x) are Gaussian. Let us assume that the first and second moments of these distributions are denoted by mp , sp , mq , and sq , respectively. Given the standard form of a Gaussian distribution [20], it is straightforward to show that the relative entropy takes the form &quot;     # 2 sq 2 sp 2 1 1 (mp {mq ) D(PjjQ)~ log z {1 z 2 2 2 sq sp sq Notice that the relative entropy can be decomposed into a set of uncorrelated components. The term in square brackets reflects any difference in the variances between the two distributions, and is often referred to as the dispersion component [8], [19]. When the variance of the observed distribution is small compared to that of the reference distribution, the relative entropy is dominated by this first term. In this case, the dispersion represents th</context>
</contexts>
<marker>20.</marker>
<rawString>Gardiner CW (1990) Handbook of Stochastic Methods. Berlin: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Willis</author>
<author>J Phillips</author>
<author>R Muheim</author>
<author>Diego-Rasilla FJ</author>
<author>Hobday AJ</author>
</authors>
<title>Spike dives of juvenile southern bluefin tuna (Thunnus maccoyii): a navigational role?</title>
<date>2009</date>
<journal>Behavioral Ecology and Sociobiology</journal>
<volume>64</volume>
<pages>57--68</pages>
<contexts>
<context position="17496" citStr="[21]" startWordPosition="2718" endWordPosition="2718">erence in the mean values between the distributions leads to a dominant contribution to D(tj ) from the signal term. Since the step-length is directly correlated with the tuna’s speed, one could infer that behavior of a newly tagged tuna is considerably altered due to various factors of stress. Also evident in Figure 2 is a periodic increase in the tuna’s speed, at times corresponding to the moment just before sunrise on each day. Further inspection reveals that these signal spikes are offset by about 20 minutes to the earlier side of sunrise, a result that is consistent with a previous study [21]. In the latter study, it was discovered that both the deepest portion of the dives and the most rapid changes in depth are precisely timed with respect to sunrise, with spike dives occurring at times corresponding to a sun elevation of about 30 minutes before sunrise. In Figure 3 we plot the signal component of the relative entropy for three tuna fish. Large values of this term indicate that the tuna’s behavior is considerably modified by distress from capture and the physiological impacts of the attachments. As the tuna recovers from the distress of capture and stress, the signal term gradua</context>
<context position="24761" citStr="[21]" startWordPosition="3835" endWordPosition="3835">served a high mortality of juveniles as a result of the fish buffeting the tank and net-pen at sunrise [22]. In the previous study it was also found that this phenomenon is caused by visual disorientation due to an incompatibility of the retina to adapt to changes in ambient light intensity. Kitagawa et al. [23] analyzed time-series data for depth, and reported that bluefin tuna display distinct patterns in their vertical movement at sunrise and sundown. It has also been reported that juvenile bluefin tuna make sharp descents and ascents, called spike dives, around sunrise and sunset each day [21]. Willis found that these spike dives are offset by about 30 minutes on the darker side of each sunrise or sunset, which is consistent with the results of our analysis. There is an abundance of opportunities in which relative entropy techniques can be applied. The relative entropy is a robust central tendencies of the two distributions, and thus the relative entropy is dominated by the signal term. We found that the modified behavior regresses to the baseline behavior after approximately 5 hours, corresponding to the first bin in which the observed distribution becomes indistinguishable from e</context>
</contexts>
<marker>21.</marker>
<rawString>Willis J, Phillips J, Muheim R, Diego-Rasilla FJ, Hobday AJ (2009) Spike dives of juvenile southern bluefin tuna (Thunnus maccoyii): a navigational role? Behavioral Ecology and Sociobiology 64: 57–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Masuma</author>
<author>G Kawamura</author>
<author>N Tezuka</author>
<author>M Koiso</author>
<author>K Namba</author>
</authors>
<title>Retinomotor responses of juvenile bluefin tuna Thunnus thynnus.</title>
<date>2001</date>
<journal>Fisheries Science</journal>
<volume>67</volume>
<pages>228--231</pages>
<contexts>
<context position="24264" citStr="[22]" startWordPosition="3755" endWordPosition="3755"> Relative entropy techniques can be used to study behavior patterns that are modified by other factors, such as water temperature, exposure to sunlight, etc. For example, in this study we discovered a periodic variation in the tuna’s speed corresponding to the moment before sunrise on each day. Most likely such a fluctuation in the tuna’s diving pattern is due to changes in ambient light during sunrise, a hypothesis supported by evidence from other analyses. In captive bluefin tuna, it was observed a high mortality of juveniles as a result of the fish buffeting the tank and net-pen at sunrise [22]. In the previous study it was also found that this phenomenon is caused by visual disorientation due to an incompatibility of the retina to adapt to changes in ambient light intensity. Kitagawa et al. [23] analyzed time-series data for depth, and reported that bluefin tuna display distinct patterns in their vertical movement at sunrise and sundown. It has also been reported that juvenile bluefin tuna make sharp descents and ascents, called spike dives, around sunrise and sunset each day [21]. Willis found that these spike dives are offset by about 30 minutes on the darker side of each sunrise</context>
</contexts>
<marker>22.</marker>
<rawString>Masuma S, Kawamura G, Tezuka N, Koiso M, Namba K (2001) Retinomotor responses of juvenile bluefin tuna Thunnus thynnus. Fisheries Science 67: 228–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kitagawa</author>
<author>S Kimura</author>
<author>H Nakata</author>
<author>H Yamada</author>
</authors>
<title>Diving behavior of immature, feeding Pacific bluefin tuna (Thunnus thynnus orientalis) in relation to season and area: the East China Sea and the Kuroshio-Oyashio transition region.</title>
<date>2004</date>
<journal>Fisheries Oceanography</journal>
<volume>13</volume>
<pages>161--180</pages>
<contexts>
<context position="24470" citStr="[23]" startWordPosition="3790" endWordPosition="3790">dic variation in the tuna’s speed corresponding to the moment before sunrise on each day. Most likely such a fluctuation in the tuna’s diving pattern is due to changes in ambient light during sunrise, a hypothesis supported by evidence from other analyses. In captive bluefin tuna, it was observed a high mortality of juveniles as a result of the fish buffeting the tank and net-pen at sunrise [22]. In the previous study it was also found that this phenomenon is caused by visual disorientation due to an incompatibility of the retina to adapt to changes in ambient light intensity. Kitagawa et al. [23] analyzed time-series data for depth, and reported that bluefin tuna display distinct patterns in their vertical movement at sunrise and sundown. It has also been reported that juvenile bluefin tuna make sharp descents and ascents, called spike dives, around sunrise and sunset each day [21]. Willis found that these spike dives are offset by about 30 minutes on the darker side of each sunrise or sunset, which is consistent with the results of our analysis. There is an abundance of opportunities in which relative entropy techniques can be applied. The relative entropy is a robust central tendenc</context>
</contexts>
<marker>23.</marker>
<rawString>Kitagawa T, Kimura S, Nakata H, Yamada H (2004) Diving behavior of immature, feeding Pacific bluefin tuna (Thunnus thynnus orientalis) in relation to season and area: the East China Sea and the Kuroshio-Oyashio transition region. Fisheries Oceanography 13: 161–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Godfrey</author>
<author>D Bryant</author>
</authors>
<title>Effects of radio transmitters: review of recent radiotracking studies. Conservation applications of measuring energy expenditure of New Zealand birds: Assessing habitat quality and costs of carrying radio transmitters: Department of Conservation.</title>
<date>2003</date>
<pages>83--95</pages>
<contexts>
<context position="6236" citStr="[1]" startWordPosition="897" endWordPosition="897">ly tagged animals can be significantly different from speed distributions under normal behavior. However, mild stress may not always be reflected by statistically significant changes in the speed, even though an observer confidently asserts that the animal is not behaving normally. The key for a successful study of an animal’s behavior is to ensure that any data used for analysis is indicative of its natural behavior. Distress from capture, along with the physiological impacts due to the attachment and presence of a transmitter, can result in stress that modifies an animal’s baseline behavior [1], [2], PLoS ONE |www.plosone.org 1 December 2011 |Volume 6 |Issue 12 |e28241 Relative Entropy on Animal Behavior triangle inequality, these relations are satisfied to a good approximation for P?Q. A useful criterion in the analysis of empirical data is that the results not be dependent on the coordinate system used to describe a particular behavioral pattern, which often depends upon a choice of metric. A powerful feature of the relative entropy is that it is invariant under a change of coordinate systems. Consider a smooth, invertible transformation from x to y described by the function y~w(x</context>
</contexts>
<marker>1.</marker>
<rawString>Godfrey J, Bryant D (2003) Effects of radio transmitters: review of recent radiotracking studies. Conservation applications of measuring energy expenditure of New Zealand birds: Assessing habitat quality and costs of carrying radio transmitters: Department of Conservation. pp 83–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hawkins</author>
</authors>
<title>Bio-logging and animal welfare: practical refinements.</title>
<date>2004</date>
<journal>Memoirs of National Institute of Polar Research</journal>
<volume>58</volume>
<pages>58--68</pages>
<contexts>
<context position="6241" citStr="[2]" startWordPosition="898" endWordPosition="898">gged animals can be significantly different from speed distributions under normal behavior. However, mild stress may not always be reflected by statistically significant changes in the speed, even though an observer confidently asserts that the animal is not behaving normally. The key for a successful study of an animal’s behavior is to ensure that any data used for analysis is indicative of its natural behavior. Distress from capture, along with the physiological impacts due to the attachment and presence of a transmitter, can result in stress that modifies an animal’s baseline behavior [1], [2], PLoS ONE |www.plosone.org 1 December 2011 |Volume 6 |Issue 12 |e28241 Relative Entropy on Animal Behavior triangle inequality, these relations are satisfied to a good approximation for P?Q. A useful criterion in the analysis of empirical data is that the results not be dependent on the coordinate system used to describe a particular behavioral pattern, which often depends upon a choice of metric. A powerful feature of the relative entropy is that it is invariant under a change of coordinate systems. Consider a smooth, invertible transformation from x to y described by the function y~w(x). Si</context>
</contexts>
<marker>2.</marker>
<rawString>Hawkins P (2004) Bio-logging and animal welfare: practical refinements. Memoirs of National Institute of Polar Research 58: 58–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilson RP</author>
<author>McMahon CR</author>
</authors>
<title>Measuring devices on wild animals: what constitutes acceptable practice?</title>
<date>2006</date>
<booktitle>Frontiers in Ecology and the Environment</booktitle>
<volume>4</volume>
<pages>147--154</pages>
<contexts>
<context position="3329" citStr="[3]" startWordPosition="475" endWordPosition="475">search center for aquaculture science of bluefin tuna and other cultured fish’’, under the Global COE Program; a Grant-in-Aid for Scientific Research (no. 80399097) from the Ministry of Education, Culture, Science, Sport and Technology, Japan; and a Grant-in-Aid for Research Activity Start-up (no. 21880047) from the Japan Society for the Promotion of Science. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. Competing Interests: The authors have declared that no competing interests exist. * E-mail: kadota@cims.nyu.edu [3]. James et al. [4] suspected some temporally short-term tagging effects on leatherback turtles at sea, and thus excluded from their data all results from the first week of tagging. In order to ensure that certain species of fish have properly recovered from the effects of anesthetics, attachment procedure, and transmitter presence, some studies have suggested that researchers would be well advised to exercise caution when analyzing data collected within the first twenty-four hours of transmitter attachment [5]. However, when it comes to quantifying the effects of external stress on animal beha</context>
</contexts>
<marker>3.</marker>
<rawString>Wilson RP, McMahon CR (2006) Measuring devices on wild animals: what constitutes acceptable practice? Frontiers in Ecology and the Environment 4: 147–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James MC</author>
<author>Ottensmeyer CA</author>
<author>Eckert SA</author>
<author>Myers RA</author>
</authors>
<title>Changes in diel diving patterns accompany shifts between northern foraging and southward migration in leatherback turtles.</title>
<date>2006</date>
<journal>Canadian Journal of Zoology</journal>
<volume>84</volume>
<pages>754--765</pages>
<contexts>
<context position="3347" citStr="[4]" startWordPosition="479" endWordPosition="479">aquaculture science of bluefin tuna and other cultured fish’’, under the Global COE Program; a Grant-in-Aid for Scientific Research (no. 80399097) from the Ministry of Education, Culture, Science, Sport and Technology, Japan; and a Grant-in-Aid for Research Activity Start-up (no. 21880047) from the Japan Society for the Promotion of Science. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. Competing Interests: The authors have declared that no competing interests exist. * E-mail: kadota@cims.nyu.edu [3]. James et al. [4] suspected some temporally short-term tagging effects on leatherback turtles at sea, and thus excluded from their data all results from the first week of tagging. In order to ensure that certain species of fish have properly recovered from the effects of anesthetics, attachment procedure, and transmitter presence, some studies have suggested that researchers would be well advised to exercise caution when analyzing data collected within the first twenty-four hours of transmitter attachment [5]. However, when it comes to quantifying the effects of external stress on animal behavior, no sophistic</context>
</contexts>
<marker>4.</marker>
<rawString>James MC, Ottensmeyer CA, Eckert SA, Myers RA (2006) Changes in diel diving patterns accompany shifts between northern foraging and southward migration in leatherback turtles. Canadian Journal of Zoology 84: 754–765.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bridger CJ</author>
<author>Booth RK</author>
</authors>
<title>The effects of biotelemetry transmitter presence and attachment procedures on fish physiology and behavior.</title>
<date>2003</date>
<journal>Reviews in Fishery Sciences</journal>
<volume>11</volume>
<pages>13--34</pages>
<contexts>
<context position="3844" citStr="[5]" startWordPosition="552" endWordPosition="552">thors have declared that no competing interests exist. * E-mail: kadota@cims.nyu.edu [3]. James et al. [4] suspected some temporally short-term tagging effects on leatherback turtles at sea, and thus excluded from their data all results from the first week of tagging. In order to ensure that certain species of fish have properly recovered from the effects of anesthetics, attachment procedure, and transmitter presence, some studies have suggested that researchers would be well advised to exercise caution when analyzing data collected within the first twenty-four hours of transmitter attachment [5]. However, when it comes to quantifying the effects of external stress on animal behavior, no sophisticated or sufficiently quantitative methods have yet been established. Thus we turn to the question: how can one quantitatively distinguish the difference between stressinduced, non-equilibrium distributions and those of normal behavior ? Often it is the case that an equilibrium (reference) distribution is constructed from past records of observation. Upon making a new measurement, one is generally interested in the amount of information gained from the measured (observed) distribution. However</context>
</contexts>
<marker>5.</marker>
<rawString>Bridger CJ, Booth RK (2003) The effects of biotelemetry transmitter presence and attachment procedures on fish physiology and behavior. Reviews in Fishery Sciences 11: 13–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sardeshmukh PD</author>
<author>Compo GP</author>
<author>C Penland</author>
</authors>
<title>Changes of probability associated with El Nino˜.</title>
<date>2000</date>
<journal>Journal of Climate</journal>
<volume>13</volume>
<pages>4268--4286</pages>
<contexts>
<context position="7565" citStr="[6]" startWordPosition="1093" endWordPosition="1093">y remains invariant under coordinate transformations [8], [19]. Thus, we are guaranteed that the difference between two probability distributions is always described by a single measure, regardless of the coordinate system. We further examine the significance of coordinate invariance as it applies specifically to the case of telemetry data in the Discussion section. There are currently several test statistics which are used to quantify the similarities between two distributions, including the tand F-tests for Gaussian distributions, and the KolmogorovSmirnov test for generalized distributions [6]. Bayesian methods often employ null hypothesis testing, an approach that has been criticized for its inherent subjectivity and emphasis on decisionmaking statistics [7]. The purpose of this paper is to introduce the readers to the use of relative entropy techniques as a method of quantifying timedependent differences between observed data and equilibrium. Relative entropy techniques are robust, compelling, and can be applied to many physical situations. Since relative entropy is sensitive to the higher-order moments of a distribution, and not just changes in the mean and variance, it has the </context>
</contexts>
<marker>6.</marker>
<rawString>Sardeshmukh PD, Compo GP, Penland C (2000) Changes of probability associated with El Nino˜. Journal of Climate 13: 4268–4286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Berger JO</author>
</authors>
<title>Statistical decision theory and Bayesian analysis.</title>
<date>1985</date>
<publisher>Springer.</publisher>
<location>New York:</location>
<contexts>
<context position="7734" citStr="[7]" startWordPosition="1116" endWordPosition="1116">a single measure, regardless of the coordinate system. We further examine the significance of coordinate invariance as it applies specifically to the case of telemetry data in the Discussion section. There are currently several test statistics which are used to quantify the similarities between two distributions, including the tand F-tests for Gaussian distributions, and the KolmogorovSmirnov test for generalized distributions [6]. Bayesian methods often employ null hypothesis testing, an approach that has been criticized for its inherent subjectivity and emphasis on decisionmaking statistics [7]. The purpose of this paper is to introduce the readers to the use of relative entropy techniques as a method of quantifying timedependent differences between observed data and equilibrium. Relative entropy techniques are robust, compelling, and can be applied to many physical situations. Since relative entropy is sensitive to the higher-order moments of a distribution, and not just changes in the mean and variance, it has the major advantage of more completely capturing probabilistic information [8]. For example, relative entropy techniques can be used to detect a divergence between an observ</context>
</contexts>
<marker>7.</marker>
<rawString>Berger JO (1985) Statistical decision theory and Bayesian analysis. New York: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kleeman</author>
</authors>
<title>Measuring dynamical prediction utility using relative entropy.</title>
<date>2002</date>
<journal>Journal of Atmospheric Science</journal>
<volume>59</volume>
<pages>2057--2072</pages>
<contexts>
<context position="7018" citStr="[8]" startWordPosition="1016" endWordPosition="1016">ximation for P?Q. A useful criterion in the analysis of empirical data is that the results not be dependent on the coordinate system used to describe a particular behavioral pattern, which often depends upon a choice of metric. A powerful feature of the relative entropy is that it is invariant under a change of coordinate systems. Consider a smooth, invertible transformation from x to y described by the function y~w(x). Since the relation D(p(x)jjq(x))~ D(pw (y)jjqw (y)) is always satisfied for such a re-parameterization, the relative entropy remains invariant under coordinate transformations [8], [19]. Thus, we are guaranteed that the difference between two probability distributions is always described by a single measure, regardless of the coordinate system. We further examine the significance of coordinate invariance as it applies specifically to the case of telemetry data in the Discussion section. There are currently several test statistics which are used to quantify the similarities between two distributions, including the tand F-tests for Gaussian distributions, and the KolmogorovSmirnov test for generalized distributions [6]. Bayesian methods often employ null hypothesis testi</context>
<context position="8239" citStr="[8]" startWordPosition="1192" endWordPosition="1192">at has been criticized for its inherent subjectivity and emphasis on decisionmaking statistics [7]. The purpose of this paper is to introduce the readers to the use of relative entropy techniques as a method of quantifying timedependent differences between observed data and equilibrium. Relative entropy techniques are robust, compelling, and can be applied to many physical situations. Since relative entropy is sensitive to the higher-order moments of a distribution, and not just changes in the mean and variance, it has the major advantage of more completely capturing probabilistic information [8]. For example, relative entropy techniques can be used to detect a divergence between an observed distribution and equilibrium that may not affect low-order moments, such as a time-skewness introduced by difficulties in detection. Although the relative entropy is not a true metric in the mathematical sense, another useful property is that it can be intuited as an effective distance between two probability distributions, in the sense that 1) it is always positive, 2) it is zero if and only if the two distributions are identical, and 3) it increases as the distributions diverge [9], [10]. Despit</context>
<context position="10327" citStr="[8]" startWordPosition="1545" endWordPosition="1545">hat P(x) and Q(x) are Gaussian. Let us assume that the first and second moments of these distributions are denoted by mp , sp , mq , and sq , respectively. Given the standard form of a Gaussian distribution [20], it is straightforward to show that the relative entropy takes the form &quot;     # 2 sq 2 sp 2 1 1 (mp {mq ) D(PjjQ)~ log z {1 z 2 2 2 sq sp sq Notice that the relative entropy can be decomposed into a set of uncorrelated components. The term in square brackets reflects any difference in the variances between the two distributions, and is often referred to as the dispersion component [8], [19]. When the variance of the observed distribution is small compared to that of the reference distribution, the relative entropy is dominated by this first term. In this case, the dispersion represents the reduction in uncertainty of the random variables as a result of the observation process. Alternatively, when the means of the two distributions are large relative to the variance of the reference distribution, the relative entropy is dominated by the last term, often referred to as the signal component. The significance of the signal term can be better understood with the help of concret</context>
</contexts>
<marker>8.</marker>
<rawString>Kleeman R (2002) Measuring dynamical prediction utility using relative entropy. Journal of Atmospheric Science 59: 2057–2072.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shannon CE</author>
<author>W Weaver</author>
</authors>
<title>The mathematical theory of communication.</title>
<date>1949</date>
<publisher>University of Illinois Press.</publisher>
<location>Urbana:</location>
<contexts>
<context position="8825" citStr="[9]" startWordPosition="1286" endWordPosition="1286">c information [8]. For example, relative entropy techniques can be used to detect a divergence between an observed distribution and equilibrium that may not affect low-order moments, such as a time-skewness introduced by difficulties in detection. Although the relative entropy is not a true metric in the mathematical sense, another useful property is that it can be intuited as an effective distance between two probability distributions, in the sense that 1) it is always positive, 2) it is zero if and only if the two distributions are identical, and 3) it increases as the distributions diverge [9], [10]. Despite the concern raised by the increased use of telemetry techniques, very little is known about the effects of tagging devices, and even less is known about the effects on fishes [11], [12], [13], [14], [15]. In this paper we introduce relative entropy techniques as a method for assessing factors that can influence and modify animal behavior. In the Methods section we define relative entropy, give a brief overview of its properties as applied to generalized probability distributions, then specialize the definition for the case of Gaussian distributions. In the Results section we an</context>
</contexts>
<marker>9.</marker>
<rawString>Shannon CE, Weaver W (1949) The mathematical theory of communication. Urbana: University of Illinois Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cover TM</author>
<author>Thomas JA</author>
</authors>
<title>Elements of information theory.</title>
<date>1991</date>
<publisher>Wiley.</publisher>
<location>New York:</location>
<contexts>
<context position="8831" citStr="[10]" startWordPosition="1287" endWordPosition="1287">ormation [8]. For example, relative entropy techniques can be used to detect a divergence between an observed distribution and equilibrium that may not affect low-order moments, such as a time-skewness introduced by difficulties in detection. Although the relative entropy is not a true metric in the mathematical sense, another useful property is that it can be intuited as an effective distance between two probability distributions, in the sense that 1) it is always positive, 2) it is zero if and only if the two distributions are identical, and 3) it increases as the distributions diverge [9], [10]. Despite the concern raised by the increased use of telemetry techniques, very little is known about the effects of tagging devices, and even less is known about the effects on fishes [11], [12], [13], [14], [15]. In this paper we introduce relative entropy techniques as a method for assessing factors that can influence and modify animal behavior. In the Methods section we define relative entropy, give a brief overview of its properties as applied to generalized probability distributions, then specialize the definition for the case of Gaussian distributions. In the Results section we analyze </context>
<context position="12521" citStr="[10]" startWordPosition="1904" endWordPosition="1904">e for doing this is the relative entropy. Consider a random variable x with a probability distribution function of Q(x). Following some measurement, we revise our estimate from Q(x) to P(x). The change in the probability function represents a measure of the amount of information introduced as a result of the measurement. The relative entropy D(PjQ) quantifies the change in information as an effective distance between the two probability distributions, given by ? ð D(PjQ)~   P(x) P(x):log dx Q(x) ð1Þ {? Ethics Statement There exist many excellent reviews of the relative entropy in literature [10], [16], [17], [18]. Although this parameter, also known as the Kullback-Leibler divergence, is not a true metric, it serves as an effective distance between the distributions P(x) and Q(x). The relative entropy is always non-negative, and vanishes if and only if P = Q for all x. Although the relative entropy is not symmetric under the exchange of P and Q, nor does it satisfy the PLoS ONE |www.plosone.org ð2Þ This study (No. 2010-26) is conducted with approval by the Faculty of Agriculture, Kinki University, located in HigashiOsaka, Japan. All experiments were conducted in accordance with Japan</context>
</contexts>
<marker>10.</marker>
<rawString>Cover TM, Thomas JA (1991) Elements of information theory. New York: Wiley.</rawString>
</citation>
<citation valid="false">
<authors>
<author>U S National Research Council</author>
</authors>
<title>Low-frequency sound and marine mammals: Current knowledge and research need. WashingtonDC: National Academies Press.</title>
<date>1994</date>
<booktitle>Volume 6 |Issue 12 |e28241 Copyright of PLoS ONE is the property of Public Library of Science and its content</booktitle>
<tech>PLoS ONE |www.plosone.org</tech>
<contexts>
<context position="9020" citStr="[11]" startWordPosition="1319" endWordPosition="1319">ime-skewness introduced by difficulties in detection. Although the relative entropy is not a true metric in the mathematical sense, another useful property is that it can be intuited as an effective distance between two probability distributions, in the sense that 1) it is always positive, 2) it is zero if and only if the two distributions are identical, and 3) it increases as the distributions diverge [9], [10]. Despite the concern raised by the increased use of telemetry techniques, very little is known about the effects of tagging devices, and even less is known about the effects on fishes [11], [12], [13], [14], [15]. In this paper we introduce relative entropy techniques as a method for assessing factors that can influence and modify animal behavior. In the Methods section we define relative entropy, give a brief overview of its properties as applied to generalized probability distributions, then specialize the definition for the case of Gaussian distributions. In the Results section we analyze the effects that transmitter attachment and presence can have on the behavior of Pacific bluefin tuna. In the final section we conclude with a discussion of the results. Relative Entropy fo</context>
</contexts>
<marker>11.</marker>
<rawString>U.S. National Research Council (1994) Low-frequency sound and marine mammals: Current knowledge and research need. WashingtonDC: National Academies Press. PLoS ONE  |www.plosone.org December 2011  |Volume 6  |Issue 12  |e28241 Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder&amp;apos;s express written permission. However, users may print, download, or email articles for individual use.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>